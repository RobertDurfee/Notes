{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant([[-0.23390341,  1.18151883, -2.46493986,  1.55322202,  1.27621763,\n",
    "                   2.39710997, -1.34403040, -0.46903436, -0.64673502, -1.44029872,\n",
    "                  -1.37537243,  1.05994811, -0.93311512,  1.02735575, -0.84138778,\n",
    "                  -2.22585412, -0.42591102,  1.03561105,  0.91125595, -2.26550369],\n",
    "                 [-0.92254932, -1.10309630, -2.41956036, -1.15509002, -1.04805327,\n",
    "                   0.08717325,  0.81847250, -0.75171045,  0.60664705,  0.80410947,\n",
    "                  -0.11600488,  1.03747218, -0.67210575,  0.99944446, -0.65559838,\n",
    "                  -0.40744784, -0.58367642,  1.05972780, -0.95991874, -1.41720255]], dtype=tf.float32)\n",
    "\n",
    "Y = tf.constant([[0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
    "                 [1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(2,)),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.005), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBatchLogger(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, X, Y, n):\n",
    "        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.seen = 0\n",
    "        self.n = n\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        \n",
    "        self.seen += 1\n",
    "        \n",
    "        if self.seen % self.n == 1:\n",
    "            \n",
    "            metrics = self.model.evaluate(tf.transpose(self.X), tf.transpose(self.Y), verbose=0)\n",
    "            print('Iteration =', self.seen, '\\tAcc =', metrics[1], '\\tLoss =', metrics[0], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.75 \tLoss = 0.6277180910110474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0329 19:32:13.454325 140592853210944 callbacks.py:236] Method (on_train_batch_end) is slow compared to the batch update (0.132159). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 251 \tAcc = 0.9 \tLoss = 0.43712979555130005\n",
      "Iteration = 501 \tAcc = 0.9 \tLoss = 0.33396413922309875\n",
      "Iteration = 751 \tAcc = 0.9 \tLoss = 0.28002071380615234\n",
      "Iteration = 1001 \tAcc = 0.9 \tLoss = 0.24655354022979736\n",
      "Iteration = 1251 \tAcc = 0.9 \tLoss = 0.2267615795135498\n",
      "Iteration = 1501 \tAcc = 0.9 \tLoss = 0.21295323967933655\n",
      "Iteration = 1751 \tAcc = 0.9 \tLoss = 0.20483747124671936\n",
      "Iteration = 2001 \tAcc = 0.9 \tLoss = 0.19869253039360046\n",
      "Iteration = 2251 \tAcc = 0.9 \tLoss = 0.19416102766990662\n",
      "Iteration = 2501 \tAcc = 0.95 \tLoss = 0.18667593598365784\n",
      "Iteration = 2751 \tAcc = 0.95 \tLoss = 0.18277189135551453\n",
      "Iteration = 3001 \tAcc = 0.95 \tLoss = 0.17989571392536163\n",
      "Iteration = 3251 \tAcc = 0.95 \tLoss = 0.17740772664546967\n",
      "Iteration = 3501 \tAcc = 0.95 \tLoss = 0.17523236572742462\n",
      "Iteration = 3751 \tAcc = 0.95 \tLoss = 0.1734848916530609\n",
      "Iteration = 4001 \tAcc = 0.95 \tLoss = 0.17148038744926453\n",
      "Iteration = 4251 \tAcc = 0.95 \tLoss = 0.17044398188591003\n",
      "Iteration = 4501 \tAcc = 0.95 \tLoss = 0.16830778121948242\n",
      "Iteration = 4751 \tAcc = 0.95 \tLoss = 0.1669234335422516\n",
      "Iteration = 5001 \tAcc = 0.95 \tLoss = 0.16545113921165466\n",
      "Iteration = 5251 \tAcc = 0.95 \tLoss = 0.16439388692378998\n",
      "Iteration = 5501 \tAcc = 0.95 \tLoss = 0.16264291107654572\n",
      "Iteration = 5751 \tAcc = 0.95 \tLoss = 0.16149751842021942\n",
      "Iteration = 6001 \tAcc = 0.95 \tLoss = 0.160512313246727\n",
      "Iteration = 6251 \tAcc = 0.95 \tLoss = 0.15895798802375793\n",
      "Iteration = 6501 \tAcc = 0.95 \tLoss = 0.15786027908325195\n",
      "Iteration = 6751 \tAcc = 0.95 \tLoss = 0.1571057140827179\n",
      "Iteration = 7001 \tAcc = 0.95 \tLoss = 0.15568843483924866\n",
      "Iteration = 7251 \tAcc = 0.95 \tLoss = 0.15440624952316284\n",
      "Iteration = 7501 \tAcc = 0.95 \tLoss = 0.15315411984920502\n",
      "Iteration = 7751 \tAcc = 0.95 \tLoss = 0.15190139412879944\n",
      "Iteration = 8001 \tAcc = 0.95 \tLoss = 0.15083973109722137\n",
      "Iteration = 8251 \tAcc = 0.95 \tLoss = 0.14985476434230804\n",
      "Iteration = 8501 \tAcc = 0.95 \tLoss = 0.1481720358133316\n",
      "Iteration = 8751 \tAcc = 0.95 \tLoss = 0.1472860723733902\n",
      "Iteration = 9001 \tAcc = 0.95 \tLoss = 0.14607569575309753\n",
      "Iteration = 9251 \tAcc = 0.95 \tLoss = 0.144962340593338\n",
      "Iteration = 9501 \tAcc = 0.95 \tLoss = 0.14412958920001984\n",
      "Iteration = 9751 \tAcc = 0.95 \tLoss = 0.14331254363059998\n",
      "Iteration = 10001 \tAcc = 0.95 \tLoss = 0.1423138678073883\n",
      "Iteration = 10251 \tAcc = 0.95 \tLoss = 0.14141210913658142\n",
      "Iteration = 10501 \tAcc = 0.95 \tLoss = 0.14083115756511688\n",
      "Iteration = 10751 \tAcc = 0.95 \tLoss = 0.1397312730550766\n",
      "Iteration = 11001 \tAcc = 0.95 \tLoss = 0.13840976357460022\n",
      "Iteration = 11251 \tAcc = 0.95 \tLoss = 0.13759645819664001\n",
      "Iteration = 11501 \tAcc = 0.95 \tLoss = 0.13686542212963104\n",
      "Iteration = 11751 \tAcc = 0.95 \tLoss = 0.13590295612812042\n",
      "Iteration = 12001 \tAcc = 0.95 \tLoss = 0.13502880930900574\n",
      "Iteration = 12251 \tAcc = 0.95 \tLoss = 0.1345708668231964\n",
      "Iteration = 12501 \tAcc = 0.95 \tLoss = 0.13305047154426575\n",
      "Iteration = 12751 \tAcc = 0.95 \tLoss = 0.13198335468769073\n",
      "Iteration = 13001 \tAcc = 0.95 \tLoss = 0.13091573119163513\n",
      "Iteration = 13251 \tAcc = 0.95 \tLoss = 0.13018055260181427\n",
      "Iteration = 13501 \tAcc = 0.95 \tLoss = 0.12921945750713348\n",
      "Iteration = 13751 \tAcc = 0.95 \tLoss = 0.12836098670959473\n",
      "Iteration = 14001 \tAcc = 0.95 \tLoss = 0.12770679593086243\n",
      "Iteration = 14251 \tAcc = 0.95 \tLoss = 0.1267588883638382\n",
      "Iteration = 14501 \tAcc = 0.95 \tLoss = 0.12546759843826294\n",
      "Iteration = 14751 \tAcc = 0.95 \tLoss = 0.12467291206121445\n",
      "Iteration = 15001 \tAcc = 0.95 \tLoss = 0.12350694835186005\n",
      "Iteration = 15251 \tAcc = 0.95 \tLoss = 0.12276871502399445\n",
      "Iteration = 15501 \tAcc = 0.95 \tLoss = 0.1215774193406105\n",
      "Iteration = 15751 \tAcc = 0.95 \tLoss = 0.12066193670034409\n",
      "Iteration = 16001 \tAcc = 0.95 \tLoss = 0.11924378573894501\n",
      "Iteration = 16251 \tAcc = 0.95 \tLoss = 0.11865899711847305\n",
      "Iteration = 16501 \tAcc = 0.95 \tLoss = 0.11723633855581284\n",
      "Iteration = 16751 \tAcc = 0.95 \tLoss = 0.11636338382959366\n",
      "Iteration = 17001 \tAcc = 0.95 \tLoss = 0.115337073802948\n",
      "Iteration = 17251 \tAcc = 0.95 \tLoss = 0.11411931365728378\n",
      "Iteration = 17501 \tAcc = 0.95 \tLoss = 0.11325707286596298\n",
      "Iteration = 17751 \tAcc = 0.95 \tLoss = 0.11256502568721771\n",
      "Iteration = 18001 \tAcc = 0.95 \tLoss = 0.1118428111076355\n",
      "Iteration = 18251 \tAcc = 0.95 \tLoss = 0.11051474511623383\n",
      "Iteration = 18501 \tAcc = 0.95 \tLoss = 0.10903706401586533\n",
      "Iteration = 18751 \tAcc = 0.95 \tLoss = 0.10817767679691315\n",
      "Iteration = 19001 \tAcc = 0.95 \tLoss = 0.10693006217479706\n",
      "Iteration = 19251 \tAcc = 0.95 \tLoss = 0.10676498711109161\n",
      "Iteration = 19501 \tAcc = 0.95 \tLoss = 0.10506415367126465\n",
      "Iteration = 19751 \tAcc = 0.95 \tLoss = 0.10388197749853134\n",
      "Iteration = 20001 \tAcc = 0.95 \tLoss = 0.10266467183828354\n",
      "Iteration = 20251 \tAcc = 0.95 \tLoss = 0.10204504430294037\n",
      "Iteration = 20501 \tAcc = 0.95 \tLoss = 0.10048327594995499\n",
      "Iteration = 20751 \tAcc = 0.95 \tLoss = 0.09941981732845306\n",
      "Iteration = 21001 \tAcc = 0.95 \tLoss = 0.09842902421951294\n",
      "Iteration = 21251 \tAcc = 0.95 \tLoss = 0.09787394851446152\n",
      "Iteration = 21501 \tAcc = 0.95 \tLoss = 0.09628994017839432\n",
      "Iteration = 21751 \tAcc = 0.95 \tLoss = 0.09575728327035904\n",
      "Iteration = 22001 \tAcc = 0.95 \tLoss = 0.09396009147167206\n",
      "Iteration = 22251 \tAcc = 0.95 \tLoss = 0.09296922385692596\n",
      "Iteration = 22501 \tAcc = 0.95 \tLoss = 0.09218927472829819\n",
      "Iteration = 22751 \tAcc = 0.95 \tLoss = 0.09137247502803802\n",
      "Iteration = 23001 \tAcc = 0.95 \tLoss = 0.09013770520687103\n",
      "Iteration = 23251 \tAcc = 0.95 \tLoss = 0.0898808166384697\n",
      "Iteration = 23501 \tAcc = 0.95 \tLoss = 0.0880303829908371\n",
      "Iteration = 23751 \tAcc = 0.95 \tLoss = 0.08753466606140137\n",
      "Iteration = 24001 \tAcc = 0.95 \tLoss = 0.08638755977153778\n",
      "Iteration = 24251 \tAcc = 0.95 \tLoss = 0.08591748028993607\n",
      "Iteration = 24501 \tAcc = 0.95 \tLoss = 0.08480934053659439\n",
      "Iteration = 24751 \tAcc = 0.95 \tLoss = 0.08408814668655396\n",
      "Iteration = 25001 \tAcc = 0.95 \tLoss = 0.08385922759771347\n",
      "Iteration = 25251 \tAcc = 0.95 \tLoss = 0.08228562772274017\n",
      "Iteration = 25501 \tAcc = 0.95 \tLoss = 0.0806596428155899\n",
      "Iteration = 25751 \tAcc = 0.95 \tLoss = 0.08161824196577072\n",
      "Iteration = 26001 \tAcc = 0.95 \tLoss = 0.07930054515600204\n",
      "Iteration = 26251 \tAcc = 0.95 \tLoss = 0.07985051721334457\n",
      "Iteration = 26501 \tAcc = 0.95 \tLoss = 0.07720005512237549\n",
      "Iteration = 26751 \tAcc = 0.95 \tLoss = 0.07672734558582306\n",
      "Iteration = 27001 \tAcc = 0.95 \tLoss = 0.0765196904540062\n",
      "Iteration = 27251 \tAcc = 0.95 \tLoss = 0.07636746764183044\n",
      "Iteration = 27501 \tAcc = 0.95 \tLoss = 0.07449022680521011\n",
      "Iteration = 27751 \tAcc = 0.95 \tLoss = 0.07437348365783691\n",
      "Iteration = 28001 \tAcc = 0.95 \tLoss = 0.07306231558322906\n",
      "Iteration = 28251 \tAcc = 0.95 \tLoss = 0.07199863344430923\n",
      "Iteration = 28501 \tAcc = 0.95 \tLoss = 0.07116611301898956\n",
      "Iteration = 28751 \tAcc = 0.95 \tLoss = 0.07116255909204483\n",
      "Iteration = 29001 \tAcc = 1.0 \tLoss = 0.06991247832775116\n",
      "Iteration = 29251 \tAcc = 0.95 \tLoss = 0.06936633586883545\n",
      "Iteration = 29501 \tAcc = 0.95 \tLoss = 0.06844372302293777\n",
      "Iteration = 29751 \tAcc = 1.0 \tLoss = 0.06827622652053833\n",
      "Iteration = 30001 \tAcc = 1.0 \tLoss = 0.06712716072797775\n",
      "Iteration = 30251 \tAcc = 1.0 \tLoss = 0.06634913384914398\n",
      "Iteration = 30501 \tAcc = 1.0 \tLoss = 0.06602689623832703\n",
      "Iteration = 30751 \tAcc = 1.0 \tLoss = 0.06778459250926971\n",
      "Iteration = 31001 \tAcc = 1.0 \tLoss = 0.06461936235427856\n",
      "Iteration = 31251 \tAcc = 1.0 \tLoss = 0.0642801970243454\n",
      "Iteration = 31501 \tAcc = 1.0 \tLoss = 0.06356557458639145\n",
      "Iteration = 31751 \tAcc = 0.95 \tLoss = 0.06443922221660614\n",
      "Iteration = 32001 \tAcc = 1.0 \tLoss = 0.06250079721212387\n",
      "Iteration = 32251 \tAcc = 0.95 \tLoss = 0.06284403055906296\n",
      "Iteration = 32501 \tAcc = 1.0 \tLoss = 0.061446815729141235\n",
      "Iteration = 32751 \tAcc = 0.95 \tLoss = 0.0617603175342083\n",
      "Iteration = 33001 \tAcc = 1.0 \tLoss = 0.061101943254470825\n",
      "Iteration = 33251 \tAcc = 1.0 \tLoss = 0.06019916385412216\n",
      "Iteration = 33501 \tAcc = 1.0 \tLoss = 0.060164839029312134\n",
      "Iteration = 33751 \tAcc = 1.0 \tLoss = 0.05943547561764717\n",
      "Iteration = 34001 \tAcc = 1.0 \tLoss = 0.059040408581495285\n",
      "Iteration = 34251 \tAcc = 0.95 \tLoss = 0.06049657613039017\n",
      "Iteration = 34501 \tAcc = 1.0 \tLoss = 0.05826492980122566\n",
      "Iteration = 34751 \tAcc = 1.0 \tLoss = 0.0577395036816597\n",
      "Iteration = 35001 \tAcc = 1.0 \tLoss = 0.05964620038866997\n",
      "Iteration = 35251 \tAcc = 1.0 \tLoss = 0.05727870389819145\n",
      "Iteration = 35501 \tAcc = 1.0 \tLoss = 0.05654231458902359\n",
      "Iteration = 35751 \tAcc = 0.95 \tLoss = 0.05938795208930969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 36001 \tAcc = 1.0 \tLoss = 0.056122563779354095\n",
      "Iteration = 36251 \tAcc = 1.0 \tLoss = 0.05555842071771622\n",
      "Iteration = 36501 \tAcc = 1.0 \tLoss = 0.05498484522104263\n",
      "Iteration = 36751 \tAcc = 0.95 \tLoss = 0.05669151619076729\n",
      "Iteration = 37001 \tAcc = 1.0 \tLoss = 0.054381608963012695\n",
      "Iteration = 37251 \tAcc = 1.0 \tLoss = 0.055469732731580734\n",
      "Iteration = 37501 \tAcc = 1.0 \tLoss = 0.05364380404353142\n",
      "Iteration = 37751 \tAcc = 1.0 \tLoss = 0.05354698747396469\n",
      "Iteration = 38001 \tAcc = 1.0 \tLoss = 0.05340628698468208\n",
      "Iteration = 38251 \tAcc = 1.0 \tLoss = 0.05313458293676376\n",
      "Iteration = 38501 \tAcc = 1.0 \tLoss = 0.05374236777424812\n",
      "Iteration = 38751 \tAcc = 1.0 \tLoss = 0.05424834415316582\n",
      "Iteration = 39001 \tAcc = 1.0 \tLoss = 0.0514812171459198\n",
      "Iteration = 39251 \tAcc = 0.95 \tLoss = 0.05467892438173294\n",
      "Iteration = 39501 \tAcc = 1.0 \tLoss = 0.052997857332229614\n",
      "Iteration = 39751 \tAcc = 1.0 \tLoss = 0.051489491015672684\n",
      "Iteration = 40001 \tAcc = 1.0 \tLoss = 0.049923673272132874\n",
      "Iteration = 40251 \tAcc = 1.0 \tLoss = 0.05365534499287605\n",
      "Iteration = 40501 \tAcc = 1.0 \tLoss = 0.049840047955513\n",
      "Iteration = 40751 \tAcc = 1.0 \tLoss = 0.05523751303553581\n",
      "Iteration = 41001 \tAcc = 1.0 \tLoss = 0.04855462163686752\n",
      "Iteration = 41251 \tAcc = 1.0 \tLoss = 0.04872242361307144\n",
      "Iteration = 41501 \tAcc = 1.0 \tLoss = 0.047723062336444855\n",
      "Iteration = 41751 \tAcc = 1.0 \tLoss = 0.05257849022746086\n",
      "Iteration = 42001 \tAcc = 1.0 \tLoss = 0.04737918823957443\n",
      "Iteration = 42251 \tAcc = 1.0 \tLoss = 0.04964066296815872\n",
      "Iteration = 42501 \tAcc = 1.0 \tLoss = 0.04656431823968887\n",
      "Iteration = 42751 \tAcc = 1.0 \tLoss = 0.047057971358299255\n",
      "Iteration = 43001 \tAcc = 1.0 \tLoss = 0.04572071135044098\n",
      "Iteration = 43251 \tAcc = 1.0 \tLoss = 0.045231737196445465\n",
      "Iteration = 43501 \tAcc = 1.0 \tLoss = 0.04552599415183067\n",
      "Iteration = 43751 \tAcc = 1.0 \tLoss = 0.047737546265125275\n",
      "Iteration = 44001 \tAcc = 1.0 \tLoss = 0.04467490315437317\n",
      "Iteration = 44251 \tAcc = 1.0 \tLoss = 0.04643414169549942\n",
      "Iteration = 44501 \tAcc = 1.0 \tLoss = 0.04331891983747482\n",
      "Iteration = 44751 \tAcc = 1.0 \tLoss = 0.04478759691119194\n",
      "Iteration = 45001 \tAcc = 1.0 \tLoss = 0.04255353659391403\n",
      "Iteration = 45251 \tAcc = 1.0 \tLoss = 0.04317984730005264\n",
      "Iteration = 45501 \tAcc = 1.0 \tLoss = 0.0418195016682148\n",
      "Iteration = 45751 \tAcc = 1.0 \tLoss = 0.04816405847668648\n",
      "Iteration = 46001 \tAcc = 1.0 \tLoss = 0.041872669011354446\n",
      "Iteration = 46251 \tAcc = 1.0 \tLoss = 0.04842755198478699\n",
      "Iteration = 46501 \tAcc = 1.0 \tLoss = 0.04021179676055908\n",
      "Iteration = 46751 \tAcc = 1.0 \tLoss = 0.039817214012145996\n",
      "Iteration = 47001 \tAcc = 1.0 \tLoss = 0.039388783276081085\n",
      "Iteration = 47251 \tAcc = 1.0 \tLoss = 0.03961848467588425\n",
      "Iteration = 47501 \tAcc = 1.0 \tLoss = 0.04758736491203308\n",
      "Iteration = 47751 \tAcc = 1.0 \tLoss = 0.03868523985147476\n",
      "Iteration = 48001 \tAcc = 1.0 \tLoss = 0.03806150704622269\n",
      "Iteration = 48251 \tAcc = 1.0 \tLoss = 0.03845217823982239\n",
      "Iteration = 48501 \tAcc = 1.0 \tLoss = 0.0379771962761879\n",
      "Iteration = 48751 \tAcc = 1.0 \tLoss = 0.037568334490060806\n",
      "Iteration = 49001 \tAcc = 1.0 \tLoss = 0.036354996263980865\n",
      "Iteration = 49251 \tAcc = 1.0 \tLoss = 0.03954383730888367\n",
      "Iteration = 49501 \tAcc = 1.0 \tLoss = 0.03618533909320831\n",
      "Iteration = 49751 \tAcc = 1.0 \tLoss = 0.036095473915338516\n",
      "Iteration = 50001 \tAcc = 1.0 \tLoss = 0.03619980067014694\n",
      "Iteration = 50251 \tAcc = 1.0 \tLoss = 0.03438713029026985\n",
      "Iteration = 50501 \tAcc = 1.0 \tLoss = 0.04098810628056526\n",
      "Iteration = 50751 \tAcc = 1.0 \tLoss = 0.034356266260147095\n",
      "Iteration = 51001 \tAcc = 1.0 \tLoss = 0.03330677002668381\n",
      "Iteration = 51251 \tAcc = 1.0 \tLoss = 0.036315612494945526\n",
      "Iteration = 51501 \tAcc = 1.0 \tLoss = 0.03311606124043465\n",
      "Iteration = 51751 \tAcc = 1.0 \tLoss = 0.03283892944455147\n",
      "Iteration = 52001 \tAcc = 1.0 \tLoss = 0.03404540196061134\n",
      "Iteration = 52251 \tAcc = 1.0 \tLoss = 0.03126717731356621\n",
      "Iteration = 52501 \tAcc = 1.0 \tLoss = 0.031726546585559845\n",
      "Iteration = 52751 \tAcc = 1.0 \tLoss = 0.0328257717192173\n",
      "Iteration = 53001 \tAcc = 1.0 \tLoss = 0.03026142343878746\n",
      "Iteration = 53251 \tAcc = 1.0 \tLoss = 0.029974138364195824\n",
      "Iteration = 53501 \tAcc = 1.0 \tLoss = 0.030047032982110977\n",
      "Iteration = 53751 \tAcc = 1.0 \tLoss = 0.030036741867661476\n",
      "Iteration = 54001 \tAcc = 1.0 \tLoss = 0.02953106164932251\n",
      "Iteration = 54251 \tAcc = 1.0 \tLoss = 0.02854124829173088\n",
      "Iteration = 54501 \tAcc = 1.0 \tLoss = 0.028871705755591393\n",
      "Iteration = 54751 \tAcc = 1.0 \tLoss = 0.028230169788002968\n",
      "Iteration = 55001 \tAcc = 1.0 \tLoss = 0.027859926223754883\n",
      "Iteration = 55251 \tAcc = 1.0 \tLoss = 0.02807914838194847\n",
      "Iteration = 55501 \tAcc = 1.0 \tLoss = 0.02687663957476616\n",
      "Iteration = 55751 \tAcc = 1.0 \tLoss = 0.02731805108487606\n",
      "Iteration = 56001 \tAcc = 1.0 \tLoss = 0.02701154723763466\n",
      "Iteration = 56251 \tAcc = 1.0 \tLoss = 0.03029271587729454\n",
      "Iteration = 56501 \tAcc = 1.0 \tLoss = 0.025494378060102463\n",
      "Iteration = 56751 \tAcc = 1.0 \tLoss = 0.02543758973479271\n",
      "Iteration = 57001 \tAcc = 1.0 \tLoss = 0.02595394477248192\n",
      "Iteration = 57251 \tAcc = 1.0 \tLoss = 0.025214359164237976\n",
      "Iteration = 57501 \tAcc = 1.0 \tLoss = 0.02425420470535755\n",
      "Iteration = 57751 \tAcc = 1.0 \tLoss = 0.02705993875861168\n",
      "Iteration = 58001 \tAcc = 1.0 \tLoss = 0.024656128138303757\n",
      "Iteration = 58251 \tAcc = 1.0 \tLoss = 0.023308040574193\n",
      "Iteration = 58501 \tAcc = 1.0 \tLoss = 0.02384401485323906\n",
      "Iteration = 58751 \tAcc = 1.0 \tLoss = 0.02327755279839039\n",
      "Iteration = 59001 \tAcc = 1.0 \tLoss = 0.026019930839538574\n",
      "Iteration = 59251 \tAcc = 1.0 \tLoss = 0.02320653572678566\n",
      "Iteration = 59501 \tAcc = 1.0 \tLoss = 0.023241225630044937\n",
      "Iteration = 59751 \tAcc = 1.0 \tLoss = 0.02250090055167675\n",
      "Iteration = 60001 \tAcc = 1.0 \tLoss = 0.02223956771194935\n",
      "Iteration = 60251 \tAcc = 1.0 \tLoss = 0.02092105895280838\n",
      "Iteration = 60501 \tAcc = 1.0 \tLoss = 0.021180231124162674\n",
      "Iteration = 60751 \tAcc = 1.0 \tLoss = 0.021057449281215668\n",
      "Iteration = 61001 \tAcc = 1.0 \tLoss = 0.022170180454850197\n",
      "Iteration = 61251 \tAcc = 1.0 \tLoss = 0.020490795373916626\n",
      "Iteration = 61501 \tAcc = 1.0 \tLoss = 0.020081521943211555\n",
      "Iteration = 61751 \tAcc = 1.0 \tLoss = 0.01996452547609806\n",
      "Iteration = 62001 \tAcc = 1.0 \tLoss = 0.019039731472730637\n",
      "Iteration = 62251 \tAcc = 1.0 \tLoss = 0.018716465681791306\n",
      "Iteration = 62501 \tAcc = 1.0 \tLoss = 0.018882766366004944\n",
      "Iteration = 62751 \tAcc = 1.0 \tLoss = 0.021330703049898148\n",
      "Iteration = 63001 \tAcc = 1.0 \tLoss = 0.018666550517082214\n",
      "Iteration = 63251 \tAcc = 1.0 \tLoss = 0.01806570217013359\n",
      "Iteration = 63501 \tAcc = 1.0 \tLoss = 0.019869765266776085\n",
      "Iteration = 63751 \tAcc = 1.0 \tLoss = 0.017646240070462227\n",
      "Iteration = 64001 \tAcc = 1.0 \tLoss = 0.01697383262217045\n",
      "Iteration = 64251 \tAcc = 1.0 \tLoss = 0.017084117978811264\n",
      "Iteration = 64501 \tAcc = 1.0 \tLoss = 0.01645505055785179\n",
      "Iteration = 64751 \tAcc = 1.0 \tLoss = 0.016622768715023994\n",
      "Iteration = 65001 \tAcc = 1.0 \tLoss = 0.016351349651813507\n",
      "Iteration = 65251 \tAcc = 1.0 \tLoss = 0.016155969351530075\n",
      "Iteration = 65501 \tAcc = 1.0 \tLoss = 0.01668545790016651\n",
      "Iteration = 65751 \tAcc = 1.0 \tLoss = 0.01665646582841873\n",
      "Iteration = 66001 \tAcc = 1.0 \tLoss = 0.015099985525012016\n",
      "Iteration = 66251 \tAcc = 1.0 \tLoss = 0.015337628312408924\n",
      "Iteration = 66501 \tAcc = 1.0 \tLoss = 0.01597750559449196\n",
      "Iteration = 66751 \tAcc = 1.0 \tLoss = 0.014524844475090504\n",
      "Iteration = 67001 \tAcc = 1.0 \tLoss = 0.014330027624964714\n",
      "Iteration = 67251 \tAcc = 1.0 \tLoss = 0.01441693864762783\n",
      "Iteration = 67501 \tAcc = 1.0 \tLoss = 0.014963900670409203\n",
      "Iteration = 67751 \tAcc = 1.0 \tLoss = 0.014112314209342003\n",
      "Iteration = 68001 \tAcc = 1.0 \tLoss = 0.014628020115196705\n",
      "Iteration = 68251 \tAcc = 1.0 \tLoss = 0.013513481244444847\n",
      "Iteration = 68501 \tAcc = 1.0 \tLoss = 0.013705490157008171\n",
      "Iteration = 68751 \tAcc = 1.0 \tLoss = 0.013054887764155865\n",
      "Iteration = 69001 \tAcc = 1.0 \tLoss = 0.013139253482222557\n",
      "Iteration = 69251 \tAcc = 1.0 \tLoss = 0.015488332137465477\n",
      "Iteration = 69501 \tAcc = 1.0 \tLoss = 0.012512085027992725\n",
      "Iteration = 69751 \tAcc = 1.0 \tLoss = 0.013088436797261238\n",
      "Iteration = 70001 \tAcc = 1.0 \tLoss = 0.012394057586789131\n",
      "Iteration = 70251 \tAcc = 1.0 \tLoss = 0.012765544466674328\n",
      "Iteration = 70501 \tAcc = 1.0 \tLoss = 0.012160908430814743\n",
      "Iteration = 70751 \tAcc = 1.0 \tLoss = 0.013451410457491875\n",
      "Iteration = 71001 \tAcc = 1.0 \tLoss = 0.01250089518725872\n",
      "Iteration = 71251 \tAcc = 1.0 \tLoss = 0.014070103876292706\n",
      "Iteration = 71501 \tAcc = 1.0 \tLoss = 0.012939071282744408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 71751 \tAcc = 1.0 \tLoss = 0.011588847264647484\n",
      "Iteration = 72001 \tAcc = 1.0 \tLoss = 0.011157139204442501\n",
      "Iteration = 72251 \tAcc = 1.0 \tLoss = 0.011990009807050228\n",
      "Iteration = 72501 \tAcc = 1.0 \tLoss = 0.0113800885155797\n",
      "Iteration = 72751 \tAcc = 1.0 \tLoss = 0.011142624542117119\n",
      "Iteration = 73001 \tAcc = 1.0 \tLoss = 0.011003024876117706\n",
      "Iteration = 73251 \tAcc = 1.0 \tLoss = 0.010388929396867752\n",
      "Iteration = 73501 \tAcc = 1.0 \tLoss = 0.010440802201628685\n",
      "Iteration = 73751 \tAcc = 1.0 \tLoss = 0.010260826908051968\n",
      "Iteration = 74001 \tAcc = 1.0 \tLoss = 0.01036514900624752\n",
      "Iteration = 74251 \tAcc = 1.0 \tLoss = 0.009827881120145321\n",
      "Iteration = 74501 \tAcc = 1.0 \tLoss = 0.009797845967113972\n",
      "Iteration = 74751 \tAcc = 1.0 \tLoss = 0.009825482964515686\n",
      "Iteration = 75001 \tAcc = 1.0 \tLoss = 0.009376363828778267\n",
      "Iteration = 75251 \tAcc = 1.0 \tLoss = 0.010808432474732399\n",
      "Iteration = 75501 \tAcc = 1.0 \tLoss = 0.009256887249648571\n",
      "Iteration = 75751 \tAcc = 1.0 \tLoss = 0.00916074588894844\n",
      "Iteration = 76001 \tAcc = 1.0 \tLoss = 0.009187815710902214\n",
      "Iteration = 76251 \tAcc = 1.0 \tLoss = 0.00937715359032154\n",
      "Iteration = 76501 \tAcc = 1.0 \tLoss = 0.008731631562113762\n",
      "Iteration = 76751 \tAcc = 1.0 \tLoss = 0.009285134263336658\n",
      "Iteration = 77001 \tAcc = 1.0 \tLoss = 0.009616144932806492\n",
      "Iteration = 77251 \tAcc = 1.0 \tLoss = 0.00934860110282898\n",
      "Iteration = 77501 \tAcc = 1.0 \tLoss = 0.008359774947166443\n",
      "Iteration = 77751 \tAcc = 1.0 \tLoss = 0.008806535974144936\n",
      "Iteration = 78001 \tAcc = 1.0 \tLoss = 0.00834621675312519\n",
      "Iteration = 78251 \tAcc = 1.0 \tLoss = 0.008587831631302834\n",
      "Iteration = 78501 \tAcc = 1.0 \tLoss = 0.007988426834344864\n",
      "Iteration = 78751 \tAcc = 1.0 \tLoss = 0.007908265106379986\n",
      "Iteration = 79001 \tAcc = 1.0 \tLoss = 0.007734640501439571\n",
      "Iteration = 79251 \tAcc = 1.0 \tLoss = 0.007992995902895927\n",
      "Iteration = 79501 \tAcc = 1.0 \tLoss = 0.00854155421257019\n",
      "Iteration = 79751 \tAcc = 1.0 \tLoss = 0.007586132735013962\n",
      "Iteration = 80001 \tAcc = 1.0 \tLoss = 0.00752480560913682\n",
      "Iteration = 80251 \tAcc = 1.0 \tLoss = 0.0074264900758862495\n",
      "Iteration = 80501 \tAcc = 1.0 \tLoss = 0.007804735563695431\n",
      "Iteration = 80751 \tAcc = 1.0 \tLoss = 0.007531826850026846\n",
      "Iteration = 81001 \tAcc = 1.0 \tLoss = 0.0075014447793364525\n",
      "Iteration = 81251 \tAcc = 1.0 \tLoss = 0.007497142069041729\n",
      "Iteration = 81501 \tAcc = 1.0 \tLoss = 0.006903816945850849\n",
      "Iteration = 81751 \tAcc = 1.0 \tLoss = 0.007566704414784908\n",
      "Iteration = 82001 \tAcc = 1.0 \tLoss = 0.0067474269308149815\n",
      "Iteration = 82251 \tAcc = 1.0 \tLoss = 0.00713600218296051\n",
      "Iteration = 82501 \tAcc = 1.0 \tLoss = 0.006600216031074524\n",
      "Iteration = 82751 \tAcc = 1.0 \tLoss = 0.006942585110664368\n",
      "Iteration = 83001 \tAcc = 1.0 \tLoss = 0.006495446898043156\n",
      "Iteration = 83251 \tAcc = 1.0 \tLoss = 0.006584151182323694\n",
      "Iteration = 83501 \tAcc = 1.0 \tLoss = 0.006361722014844418\n",
      "Iteration = 83751 \tAcc = 1.0 \tLoss = 0.006363299675285816\n",
      "Iteration = 84001 \tAcc = 1.0 \tLoss = 0.006198937073349953\n",
      "Iteration = 84251 \tAcc = 1.0 \tLoss = 0.006249000318348408\n",
      "Iteration = 84501 \tAcc = 1.0 \tLoss = 0.0060908254235982895\n",
      "Iteration = 84751 \tAcc = 1.0 \tLoss = 0.006181343458592892\n",
      "Iteration = 85001 \tAcc = 1.0 \tLoss = 0.00598862674087286\n",
      "Iteration = 85251 \tAcc = 1.0 \tLoss = 0.006232171785086393\n",
      "Iteration = 85501 \tAcc = 1.0 \tLoss = 0.0059524583630263805\n",
      "Iteration = 85751 \tAcc = 1.0 \tLoss = 0.005819620098918676\n",
      "Iteration = 86001 \tAcc = 1.0 \tLoss = 0.005760538391768932\n",
      "Iteration = 86251 \tAcc = 1.0 \tLoss = 0.005758980754762888\n",
      "Iteration = 86501 \tAcc = 1.0 \tLoss = 0.0056208716705441475\n",
      "Iteration = 86751 \tAcc = 1.0 \tLoss = 0.005872116424143314\n",
      "Iteration = 87001 \tAcc = 1.0 \tLoss = 0.005628461949527264\n",
      "Iteration = 87251 \tAcc = 1.0 \tLoss = 0.005724671296775341\n",
      "Iteration = 87501 \tAcc = 1.0 \tLoss = 0.005414043553173542\n",
      "Iteration = 87751 \tAcc = 1.0 \tLoss = 0.005468399729579687\n",
      "Iteration = 88001 \tAcc = 1.0 \tLoss = 0.005527365952730179\n",
      "Iteration = 88251 \tAcc = 1.0 \tLoss = 0.005392633844166994\n",
      "Iteration = 88501 \tAcc = 1.0 \tLoss = 0.005598003976047039\n",
      "Iteration = 88751 \tAcc = 1.0 \tLoss = 0.0054705822840332985\n",
      "Iteration = 89001 \tAcc = 1.0 \tLoss = 0.005306079983711243\n",
      "Iteration = 89251 \tAcc = 1.0 \tLoss = 0.00522062461823225\n",
      "Iteration = 89501 \tAcc = 1.0 \tLoss = 0.005126879550516605\n",
      "Iteration = 89751 \tAcc = 1.0 \tLoss = 0.005078212358057499\n",
      "Iteration = 90001 \tAcc = 1.0 \tLoss = 0.004943751730024815\n",
      "Iteration = 90251 \tAcc = 1.0 \tLoss = 0.005028091836720705\n",
      "Iteration = 90501 \tAcc = 1.0 \tLoss = 0.004911486525088549\n",
      "Iteration = 90751 \tAcc = 1.0 \tLoss = 0.004901544656604528\n",
      "Iteration = 91001 \tAcc = 1.0 \tLoss = 0.004840620793402195\n",
      "Iteration = 91251 \tAcc = 1.0 \tLoss = 0.004741162993013859\n",
      "Iteration = 91501 \tAcc = 1.0 \tLoss = 0.004804753698408604\n",
      "Iteration = 91751 \tAcc = 1.0 \tLoss = 0.004774649627506733\n",
      "Iteration = 92001 \tAcc = 1.0 \tLoss = 0.0046091629192233086\n",
      "Iteration = 92251 \tAcc = 1.0 \tLoss = 0.0049886154010891914\n",
      "Iteration = 92501 \tAcc = 1.0 \tLoss = 0.004575931467115879\n",
      "Iteration = 92751 \tAcc = 1.0 \tLoss = 0.004503603093326092\n",
      "Iteration = 93001 \tAcc = 1.0 \tLoss = 0.004545987118035555\n",
      "Iteration = 93251 \tAcc = 1.0 \tLoss = 0.004621756263077259\n",
      "Iteration = 93501 \tAcc = 1.0 \tLoss = 0.004695328883826733\n",
      "Iteration = 93751 \tAcc = 1.0 \tLoss = 0.004408972803503275\n",
      "Iteration = 94001 \tAcc = 1.0 \tLoss = 0.004363260697573423\n",
      "Iteration = 94251 \tAcc = 1.0 \tLoss = 0.004300800152122974\n",
      "Iteration = 94501 \tAcc = 1.0 \tLoss = 0.004273936618119478\n",
      "Iteration = 94751 \tAcc = 1.0 \tLoss = 0.004770745523273945\n",
      "Iteration = 95001 \tAcc = 1.0 \tLoss = 0.004211102612316608\n",
      "Iteration = 95251 \tAcc = 1.0 \tLoss = 0.004434371367096901\n",
      "Iteration = 95501 \tAcc = 1.0 \tLoss = 0.0042335400357842445\n",
      "Iteration = 95751 \tAcc = 1.0 \tLoss = 0.0042205266654491425\n",
      "Iteration = 96001 \tAcc = 1.0 \tLoss = 0.004040914122015238\n",
      "Iteration = 96251 \tAcc = 1.0 \tLoss = 0.00402618246152997\n",
      "Iteration = 96501 \tAcc = 1.0 \tLoss = 0.0040224213153123856\n",
      "Iteration = 96751 \tAcc = 1.0 \tLoss = 0.004014286212623119\n",
      "Iteration = 97001 \tAcc = 1.0 \tLoss = 0.003978601191192865\n",
      "Iteration = 97251 \tAcc = 1.0 \tLoss = 0.004024564288556576\n",
      "Iteration = 97501 \tAcc = 1.0 \tLoss = 0.003989626653492451\n",
      "Iteration = 97751 \tAcc = 1.0 \tLoss = 0.003957034088671207\n",
      "Iteration = 98001 \tAcc = 1.0 \tLoss = 0.004006005823612213\n",
      "Iteration = 98251 \tAcc = 1.0 \tLoss = 0.003828375367447734\n",
      "Iteration = 98501 \tAcc = 1.0 \tLoss = 0.0037558202166110277\n",
      "Iteration = 98751 \tAcc = 1.0 \tLoss = 0.003725782735273242\n",
      "Iteration = 99001 \tAcc = 1.0 \tLoss = 0.003744053188711405\n",
      "Iteration = 99251 \tAcc = 1.0 \tLoss = 0.003938044887036085\n",
      "Iteration = 99501 \tAcc = 1.0 \tLoss = 0.0036368630826473236\n",
      "Iteration = 99751 \tAcc = 1.0 \tLoss = 0.0036465295124799013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fddf8709a58>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf.transpose(X), tf.transpose(Y), batch_size=1, epochs=(100000 // 20), verbose=0, callbacks=[NBatchLogger(X, Y, 250)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a grid of points to classify\n",
    "xx1, xx2 = np.meshgrid(np.arange(-3, 3, 0.005), np.arange(-3, 3, 0.005))\n",
    "\n",
    "# Flatten the grid to pass into model\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()].T\n",
    "\n",
    "# Predict classification at every point on the grid\n",
    "Z = (model.predict(grid.T).T)[1,:].reshape(xx1.shape)\n",
    "\n",
    "# Plot the prediction regions.\n",
    "plt.imshow(Z, interpolation='bicubic', origin='lower', extent=[-3, 3, -3, 3], \n",
    "           cmap=ListedColormap(['#1f77b4', '#ff7f0e']), alpha=0.55, aspect='auto')\n",
    "\n",
    "# Plot the original points.\n",
    "_ = plt.scatter(X[0,:], X[1,:], c=Y[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
