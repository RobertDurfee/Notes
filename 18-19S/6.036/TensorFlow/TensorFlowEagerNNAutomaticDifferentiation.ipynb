{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Eager NN Automatic Differentiation\n",
    "\n",
    "This notebook walks through the process of constructing a feed-forward neural network for multi-class classification using TensorFlow's eager execution. This notebook should look most nearly identical to the NumPy representation, but with automatic gradient calculations. Eager execution is easiest to use from a Python background, but it is painfully slow.\n",
    "\n",
    "## Layers\n",
    "\n",
    "For our neural network, we want to abstract away from individual neurons and focus on layers. Each element of the network will be defined by a certain layer.\n",
    "\n",
    "### Base Layer\n",
    "\n",
    "This layer provides the virtual methods that each layer has to implement. If the layer doesn't implement the method, we default to one of these empty methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Abstract base layer for our neural network.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__init__` method sets up constant information about the layer that is necessary to build the graph later. Typically, only dimensions of input/output data is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self):\n",
    "        \"\"\"Initializes layer constants necessary to construct the graph\n",
    "            for training. Likely: just dimension information or nothing\n",
    "            at all.\"\"\"\n",
    "\n",
    "Layer.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward method computes the forward pass from the network. All layers should implement this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, X):\n",
    "        \"\"\"Executes the forward pass through the layer\n",
    "            \n",
    "        Args:\n",
    "            X (ndarray): A matrix representing the inputs to the layer.\n",
    "                Likely: A or Z depending on the layer.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: A tensor representing the outputs of the layer. \n",
    "                Likely: Z or A depending on the layer.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "Layer.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward method computes the backward pass step for a single layer. This should be implemented for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dL):\n",
    "        \"\"\"Computes the backward pass for the layer.\n",
    "            \n",
    "        Args:\n",
    "            dL (ndarray): A matrix representing the gradient of the loss\n",
    "                of the network with respect to the outputs of the current\n",
    "                layer. Likely: dLdA or dLdZ depending on the layer.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: A matrix representing the gradient of the loss of the\n",
    "                network will respect to the inputs of the current layer.\n",
    "                Likely: dLdZ or dLdA depending on the layer.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "Layer.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any layer that has any variables needs to update these variables during stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, eta):\n",
    "        \"\"\"Updates trainable variables based off the results from the\n",
    "            backward pass.\n",
    "            \n",
    "        Args:\n",
    "            eta (float): The learning rate to use for the stochastic\n",
    "                gradient descent update step.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "Layer.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "This is the simplest possible layer where all inputs are connected to all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"Simple layer fully-connecting inputs to outputs linearly.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up this layer, we need to know the input and output dimensions ahead of time. Using this information, we randomly initialize the weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, m, n):\n",
    "        \"\"\"Initializes the layer based on input and output dimensions. \n",
    "\n",
    "        Note: Kernel is initialized using normal distribution with mean 0 \n",
    "        and variance 1 / m. All biases are initialized to zero.\n",
    "\n",
    "        Args:\n",
    "            m (int): Number of input features to the layer.\n",
    "            n (int): Number of output features of the layer.\n",
    "\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "\n",
    "        self.W0 = tf.Variable(initial_value=tf.zeros(shape=(self.n, 1)))\n",
    "        self.W = tf.Variable(initial_value=tf.random.normal(shape=(self.m, self.n), mean=0.0, stddev=tf.sqrt(1 / self.m)))\n",
    "                \n",
    "Linear.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our variables defined, we can execute the forward pass. We take the activation $A$ from the previous layer and produce the current $Z$ pre-activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, A):\n",
    "        \"\"\"Computes the forward pass of the layer.\n",
    "            \n",
    "        Args:\n",
    "            A (ndarray): An m by b matrix representing the activations from\n",
    "                the previous layer with a batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix, Z, representing the pre-activations\n",
    "                as the output from this linear layer.\n",
    "        \n",
    "        \"\"\"\n",
    "        # We need these later when computing the backward path.\n",
    "        self.A = A\n",
    "        self.Z = tf.transpose(self.W) @ self.A + self.W0\n",
    "        \n",
    "        return self.Z\n",
    "\n",
    "Linear.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will compute the gradients by hand using back-propogation. We take the gradient of the loss with respect to the pre-activations of the layer $\\partial \\mathrm{Loss} / \\partial Z$ and compute the gradient of the loss with respect to the activations of the previous layer $\\partial \\mathrm{Loss} / \\partial A$. \n",
    "\n",
    "In addition, we save gradients of the loss with respect to the weights ($\\partial \\mathrm{Loss} / \\partial W$ and $\\partial \\mathrm{Loss} / \\partial W_0$) for the stochastic gradient descent update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, gradient_tape, dLdZ):\n",
    "        \"\"\"Computes the backward pass for the layer. Also records gradients\n",
    "            of the loss with respect to weights for later stochastic \n",
    "            gradient descent updates.\n",
    "        \n",
    "        Args:\n",
    "            dLdZ (ndarray): An n by b matrix representing the gradient of\n",
    "                the loss with respect to the current layer's \n",
    "                pre-activations for a batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            ndarray: An m by b matrix, dLdA, representing the gradient of \n",
    "                the loss with respect to the previous layer's activations.\n",
    "        \n",
    "        \"\"\"\n",
    "        # We store these gradients for use later in the sgd_step\n",
    "        self.dLdW, self.dLdW0 = gradient_tape.gradient(self.Z, [self.W, self.W0], dLdZ)\n",
    "\n",
    "        return gradient_tape.gradient(self.Z, self.A, dLdZ)\n",
    "\n",
    "Linear.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear layer has some trainable parameters to update. Using the specified learning rate $\\eta$, we can re-assign our variable values using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, eta):\n",
    "        \"\"\"Updates the layer's variables using stochastic gradient\n",
    "            descent.\n",
    "            \n",
    "        Args:\n",
    "            eta (float): The learning rate to use for the stochastic\n",
    "                gradient descent update step.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.W.assign_sub(eta * self.dLdW)\n",
    "        self.W0.assign_sub(eta * self.dLdW0)\n",
    "        \n",
    "Linear.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit Activation Layer\n",
    "\n",
    "This layer applies the relu activation function to each of the inputs element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"Applies relu activation function to all inputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have no variables to consider, we just need to construct the forward and backward passes. With the forward pass we compute the activation $A$ using the previous layer's pre-activation $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Compute the forward pass output for the layer. \n",
    "            \n",
    "        Args:\n",
    "            Z (ndarray): An m by b matrix representing the pre-activations\n",
    "                from the previous layer for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix, A, representing the activations from\n",
    "                the current layer for a batch of size b. (Note: n and m \n",
    "                are equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        # We need these when computing the backward step later\n",
    "        self.Z = Z\n",
    "        self.A = tf.maximum(0.0, self.Z)\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "ReLU.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backward pass, we use the gradient of the loss with respect to the layer's activations $\\partial \\mathrm{Loss} / \\partial A$ to compute the gradient of the loss with respect to the previous layer's pre-activations $\\partial \\mathrm{Loss} / \\partial Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, gradient_tape, dLdA):\n",
    "        \"\"\"Computes the backward pass for the layer.\n",
    "        \n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of\n",
    "                the loss with respect to the current layer's activations\n",
    "                for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An m by b matrix, dLdZ, representing the gradient of\n",
    "                the loss with respect to the previous layer's activations\n",
    "                for a batch of size b. (Note: n and m are equal.)\n",
    "        \n",
    "        \"\"\"\n",
    "        return gradient_tape.gradient(self.A, self.Z, dLdA)\n",
    "    \n",
    "ReLU.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent Activation Layer\n",
    "\n",
    "This layer applies the hyperbolic tangent activation function to each input element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    \"\"\"Applies hyperbolic tangent activation function to all inputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no variables in this layer, thus we only need to worry about forward and backward passes. The forward pass takes the previous layer's pre-activation $Z$ and produces the activation $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the forward pass activation for the layer.\n",
    "        \n",
    "        Args:\n",
    "            Z (ndarray): An m by b matrix representing the previous layer's\n",
    "                pre-activation for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix, A, representing the layer's \n",
    "                activation for a batch of size b. (Note: m and n are\n",
    "                equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        # We need these when computing the backward step later\n",
    "        self.Z = Z\n",
    "        self.A = tf.tanh(self.Z)\n",
    "        \n",
    "        return self.A\n",
    "\n",
    "Tanh.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward pass uses the gradient of the loss with respect to the layer's activation $\\partial \\mathrm{Loss} / \\partial A$ to compute the gradient of the loss with respect to the previous layer's pre-activation $\\partial \\mathrm{Loss} / \\partial Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, gradient_tape, dLdA):\n",
    "        \"\"\"Computes the backward pass for the layer.\n",
    "        \n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of\n",
    "                the loss with respect to this layer's activation for a \n",
    "                batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            ndarray: An m by b matrix, dLdZ, representing the gradient of \n",
    "                the loss with respect to the previous layer's \n",
    "                pre-activation for a batch of size b. (Note: n and m are \n",
    "                equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        return gradient_tape.gradient(self.A, self.Z, dLdA)\n",
    "Tanh.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Layer\n",
    "\n",
    "This layer applies the softmax activation function to the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Layer):\n",
    "    \"\"\"Applies the softmax activation function to layer inputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the other activation layers, there are no variables to consider. The forward pass takes the previous layer's pre-activations $Z$ and computes the current layer's activation $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the forward pass activations for the layer.\n",
    "            \n",
    "        Args:\n",
    "            Z (ndarray): An m by b matrix representing the previous layer's\n",
    "                pre-activation for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix, A, representing the current layer's\n",
    "                activation for a batch of size b. (Note: m and n are\n",
    "                equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        # We need this activation when computing the backward step later\n",
    "        self.Z = Z\n",
    "        self.A = tf.exp(self.Z) / tf.reduce_sum(tf.exp(self.Z), axis=0, keepdims=True)\n",
    "        \n",
    "        return self.A\n",
    "\n",
    "SoftMax.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backward pass, we use the gradient of the loss with respect to the current layer's activations $\\partial \\mathrm{Loss} / \\partial A$ to compute the gradient of the loss with respect to the previous layer's pre-activations $\\partial \\mathrm{Loss} / \\partial Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, gradient_tape, dLdA):\n",
    "        \"\"\"Computes the backward pass for the layer.\n",
    "            \n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of\n",
    "                the loss with respect to the current layer's activation.\n",
    "                \n",
    "        Returns:\n",
    "            ndarray: An m by b tensor, dLdZ, representing the gradient of \n",
    "                the loss with respect to the previous layer's \n",
    "                pre-activation. (Note: n and m are equal.)\n",
    "            \n",
    "        \"\"\"\n",
    "        return gradient_tape.gradient(self.A, self.Z, dLdA)\n",
    "        \n",
    "SoftMax.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log-Likelihood Multi-Class Loss Layer\n",
    "\n",
    "This layer computes the loss of the output of the network compared with the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLM(Layer):\n",
    "    \"\"\"Computes the negative log-likelihood multi-class loss for neural\n",
    "        network outputs and expected outputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the activation layers, there are no variables to consider. The forward pass takes the neural network's final activations $A$ and the expected outputs\n",
    "$Y$ and computes the loss scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, A, Y):\n",
    "        \"\"\"Computes the loss of predictions vs expected outputs.\n",
    "            \n",
    "        Args:\n",
    "            A (ndarray): An n by b matrix representing the neural network's\n",
    "                outputs for a batch of size b.\n",
    "            Y (ndarray): An n by b matrix representing the expected outputs\n",
    "                from the neural network for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            float: A scalar, L, which represents the loss of the neural\n",
    "                network for a batch of size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        # We will need both of these later to compute the backward pass.\n",
    "        self.A = A\n",
    "        self.Y = Y\n",
    "        self.L = -tf.reduce_sum(self.Y * tf.math.log(self.A))\n",
    "        \n",
    "        return self.L\n",
    "\n",
    "NLLM.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward pass computes the gradient of the loss with respect to the neural network's final activations $\\partial \\mathrm{Loss} / \\partial A$. Note, this is not immediately computing $\\partial \\mathrm{Loss} / \\partial Z$ by assuming softmax activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, gradient_tape):\n",
    "        \"\"\"Computes the backward step for the loss.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: An n by b tensor, dLdA, representing the gradient of\n",
    "                the loss with respect to the neural network's outputs.\n",
    "                \n",
    "        \"\"\"\n",
    "        return gradient_tape.gradient(self.L, self.A)\n",
    "\n",
    "NLLM.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now we have all the components to construct our neural network, but we need a model to connect them together.\n",
    "\n",
    "### Sequential Model\n",
    "\n",
    "The sequential model simply connects all the layer linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"A standard neural network model with linearly stacked layers.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to construct the model is to provide a list of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, layers, loss):\n",
    "        \"\"\"Initialize the layers and the loss for the network.\n",
    "        \n",
    "        Args:\n",
    "            layers (list of Layer): A list of layers in sequential order\n",
    "                to construct the model from.\n",
    "            loss (Layer): A layer used to construct the objective for\n",
    "                stochastic gradient descent.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "\n",
    "Sequential.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions with the network, we use the `forward` method. This passes the data through every layer and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, X):\n",
    "        \"\"\"Predicts the output for a training input batch.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): A d by b matrix of points to predict with \n",
    "                dimension d and batch size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: A c by b matrix representing the predicted outputs \n",
    "                with c features of the neural network for a batch size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "            \n",
    "        return X\n",
    "\n",
    "Sequential.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we will use stochastic gradient descent. Before we define the stochastic gradient descent training loop, we have to back-propogate the error throughout the layers of the network. To do this, we use the `backward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, gradient_tape, dL):\n",
    "        \"\"\"Computes the gradients of the loss with respect to each weight\n",
    "        in the neural network to prepare for stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            dL (ndarray): An n by b tensor representing the gradient of the\n",
    "                loss with respect to the output of the neural network.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers[::-1]:\n",
    "            dL = layer.backward(gradient_tape, dL)\n",
    "\n",
    "Sequential.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the error is propogated through all the layers, each layer can update their weight matrices. For a single step, this is achieved through the `sgd_step` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, eta):\n",
    "        \"\"\"Runs a single update step on the weight matrices throughout the\n",
    "        neural network using stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            eta (float): A learning rate for stochastic gradient descent.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.sgd_step(eta)\n",
    "\n",
    "Sequential.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop over the data applying many stochastic gradient descent update steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd(self, X_train, Y_train, epochs, eta):\n",
    "        \"\"\"Trains the neural network by running stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): A d by n NumPy array representing n input\n",
    "                training points each with d features.\n",
    "            Y_train (ndarray): A c by n NumPy array representing n output\n",
    "                training points each with c features.\n",
    "            epochs (int): Number of iterations to run stochastic gradient\n",
    "                descent.\n",
    "            eta (float): A learning rate for stochastic gradient descent.\n",
    "        \n",
    "        \"\"\"\n",
    "        _, n = X.shape\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            t = np.random.randint(n)\n",
    "            \n",
    "            Xt = tf.convert_to_tensor(X[:, t:t + 1])\n",
    "            Yt = tf.convert_to_tensor(Y[:, t:t + 1])\n",
    "            \n",
    "            with tf.GradientTape(persistent=True) as gradient_tape:\n",
    "                loss = self.loss.forward(self.forward(Xt), Yt)\n",
    "            self.backward(gradient_tape, self.loss.backward(gradient_tape))            \n",
    "            self.sgd_step(eta)\n",
    "            \n",
    "            if epoch % 250 == 1:\n",
    "                \n",
    "                acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.forward(X_train), axis=0), tf.argmax(Y_train, axis=0)), tf.float32))\n",
    "                print('Iteration =', epoch, '\\tAcc =', acc.numpy(), '\\tLoss =', loss.numpy(), flush=True)\n",
    "\n",
    "Sequential.sgd = sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Our model is complete! Let's train it on some data and see how will it can classify. We will use the standard 'hard' data set used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[-0.23390341,  1.18151883, -2.46493986,  1.55322202,  1.27621763,\n",
    "                2.39710997, -1.34403040, -0.46903436, -0.64673502, -1.44029872,\n",
    "               -1.37537243,  1.05994811, -0.93311512,  1.02735575, -0.84138778,\n",
    "               -2.22585412, -0.42591102,  1.03561105,  0.91125595, -2.26550369],\n",
    "              [-0.92254932, -1.10309630, -2.41956036, -1.15509002, -1.04805327,\n",
    "                0.08717325,  0.81847250, -0.75171045,  0.60664705,  0.80410947,\n",
    "               -0.11600488,  1.03747218, -0.67210575,  0.99944446, -0.65559838,\n",
    "               -0.40744784, -0.58367642,  1.05972780, -0.95991874, -1.41720255]], dtype=np.float32)\n",
    "\n",
    "Y = np.array([[0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
    "              [1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.scatter(X[0,:], X[1,:], c=Y[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct a neural network we think might be able to classify these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Linear(2, 10), ReLU(), \n",
    "                    Linear(10, 10), ReLU(), \n",
    "                    Linear(10, 2), SoftMax()], NLLM())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training the model on the data for a few thousand iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.25 \tLoss = 0.9410402\n",
      "Iteration = 251 \tAcc = 0.9 \tLoss = 0.57954\n",
      "Iteration = 501 \tAcc = 0.9 \tLoss = 1.447955\n",
      "Iteration = 751 \tAcc = 0.9 \tLoss = 0.075937405\n",
      "Iteration = 1001 \tAcc = 0.95 \tLoss = 0.12638488\n",
      "Iteration = 1251 \tAcc = 0.95 \tLoss = 0.0027685903\n",
      "Iteration = 1501 \tAcc = 0.95 \tLoss = 0.003746665\n",
      "Iteration = 1751 \tAcc = 0.95 \tLoss = 0.0021606167\n",
      "Iteration = 2001 \tAcc = 0.95 \tLoss = 0.008674887\n",
      "Iteration = 2251 \tAcc = 0.95 \tLoss = 0.011993518\n",
      "Iteration = 2501 \tAcc = 0.95 \tLoss = 0.0012499153\n",
      "Iteration = 2751 \tAcc = 0.95 \tLoss = 0.00086517795\n",
      "Iteration = 3001 \tAcc = 0.95 \tLoss = 0.01638822\n",
      "Iteration = 3251 \tAcc = 0.95 \tLoss = 0.24862334\n",
      "Iteration = 3501 \tAcc = 0.95 \tLoss = 0.02234308\n",
      "Iteration = 3751 \tAcc = 0.95 \tLoss = 0.03711169\n",
      "Iteration = 4001 \tAcc = 0.95 \tLoss = 0.5959203\n",
      "Iteration = 4251 \tAcc = 0.95 \tLoss = 0.0004961529\n",
      "Iteration = 4501 \tAcc = 0.95 \tLoss = 0.17424287\n",
      "Iteration = 4751 \tAcc = 0.95 \tLoss = 0.0023092448\n",
      "Iteration = 5001 \tAcc = 0.95 \tLoss = 0.015638584\n",
      "Iteration = 5251 \tAcc = 0.95 \tLoss = 0.02407902\n",
      "Iteration = 5501 \tAcc = 0.95 \tLoss = 0.0065343017\n",
      "Iteration = 5751 \tAcc = 0.95 \tLoss = 2.2590415e-05\n",
      "Iteration = 6001 \tAcc = 0.95 \tLoss = 0.00045965743\n",
      "Iteration = 6251 \tAcc = 0.95 \tLoss = 0.026334146\n",
      "Iteration = 6501 \tAcc = 0.95 \tLoss = 0.010450179\n",
      "Iteration = 6751 \tAcc = 0.95 \tLoss = 0.0010749513\n",
      "Iteration = 7001 \tAcc = 0.95 \tLoss = 0.042042546\n",
      "Iteration = 7251 \tAcc = 0.95 \tLoss = 0.0003249577\n",
      "Iteration = 7501 \tAcc = 0.95 \tLoss = 0.21679209\n",
      "Iteration = 7751 \tAcc = 0.95 \tLoss = 0.00020690913\n",
      "Iteration = 8001 \tAcc = 0.95 \tLoss = 0.0019858514\n",
      "Iteration = 8251 \tAcc = 0.95 \tLoss = 0.0019486446\n",
      "Iteration = 8501 \tAcc = 0.95 \tLoss = 0.00016005128\n",
      "Iteration = 8751 \tAcc = 0.95 \tLoss = 0.046355885\n",
      "Iteration = 9001 \tAcc = 0.95 \tLoss = 0.002253447\n",
      "Iteration = 9251 \tAcc = 0.95 \tLoss = 0.0017566604\n",
      "Iteration = 9501 \tAcc = 0.95 \tLoss = 0.039080843\n",
      "Iteration = 9751 \tAcc = 0.95 \tLoss = 0.0041916477\n",
      "Iteration = 10001 \tAcc = 0.95 \tLoss = 0.042589303\n",
      "Iteration = 10251 \tAcc = 0.95 \tLoss = 2.121948e-05\n",
      "Iteration = 10501 \tAcc = 0.95 \tLoss = 0.0014438916\n",
      "Iteration = 10751 \tAcc = 0.95 \tLoss = 0.03165469\n",
      "Iteration = 11001 \tAcc = 0.95 \tLoss = 0.0011363523\n",
      "Iteration = 11251 \tAcc = 0.95 \tLoss = 0.03830639\n",
      "Iteration = 11501 \tAcc = 0.95 \tLoss = 0.15148096\n",
      "Iteration = 11751 \tAcc = 0.95 \tLoss = 0.29057533\n",
      "Iteration = 12001 \tAcc = 0.95 \tLoss = 0.035812583\n",
      "Iteration = 12251 \tAcc = 0.95 \tLoss = 0.0008466847\n",
      "Iteration = 12501 \tAcc = 0.95 \tLoss = 0.34263012\n",
      "Iteration = 12751 \tAcc = 0.95 \tLoss = 0.005605105\n",
      "Iteration = 13001 \tAcc = 0.95 \tLoss = 0.00023922205\n",
      "Iteration = 13251 \tAcc = 0.95 \tLoss = 0.028443623\n",
      "Iteration = 13501 \tAcc = 0.95 \tLoss = 0.026204849\n",
      "Iteration = 13751 \tAcc = 0.95 \tLoss = 0.00022556941\n",
      "Iteration = 14001 \tAcc = 0.95 \tLoss = 0.000600399\n",
      "Iteration = 14251 \tAcc = 0.95 \tLoss = 0.0001422268\n",
      "Iteration = 14501 \tAcc = 0.95 \tLoss = 1.3577771\n",
      "Iteration = 14751 \tAcc = 0.95 \tLoss = 1.180179e-05\n",
      "Iteration = 15001 \tAcc = 0.95 \tLoss = 0.97184116\n",
      "Iteration = 15251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 15501 \tAcc = 0.95 \tLoss = 0.10397384\n",
      "Iteration = 15751 \tAcc = 0.95 \tLoss = 0.27302834\n",
      "Iteration = 16001 \tAcc = 0.95 \tLoss = 0.242866\n",
      "Iteration = 16251 \tAcc = 0.95 \tLoss = 0.11970729\n",
      "Iteration = 16501 \tAcc = 0.95 \tLoss = 6.491157e-05\n",
      "Iteration = 16751 \tAcc = 0.95 \tLoss = 1.001363e-05\n",
      "Iteration = 17001 \tAcc = 0.95 \tLoss = 0.3105701\n",
      "Iteration = 17251 \tAcc = 0.95 \tLoss = 0.015503157\n",
      "Iteration = 17501 \tAcc = 0.95 \tLoss = 0.00032161878\n",
      "Iteration = 17751 \tAcc = 0.95 \tLoss = 0.0153681105\n",
      "Iteration = 18001 \tAcc = 0.95 \tLoss = 0.08559563\n",
      "Iteration = 18251 \tAcc = 0.95 \tLoss = 1.6093383e-05\n",
      "Iteration = 18501 \tAcc = 0.95 \tLoss = 0.00028071768\n",
      "Iteration = 18751 \tAcc = 0.95 \tLoss = 0.0001961185\n",
      "Iteration = 19001 \tAcc = 0.95 \tLoss = 0.0031077226\n",
      "Iteration = 19251 \tAcc = 0.95 \tLoss = 0.39996508\n",
      "Iteration = 19501 \tAcc = 0.95 \tLoss = 8.9407007e-07\n",
      "Iteration = 19751 \tAcc = 0.95 \tLoss = 0.00029818687\n",
      "Iteration = 20001 \tAcc = 0.95 \tLoss = 0.00049746485\n",
      "Iteration = 20251 \tAcc = 0.95 \tLoss = 5.9604645e-08\n",
      "Iteration = 20501 \tAcc = 0.95 \tLoss = 0.0001688146\n",
      "Iteration = 20751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 21001 \tAcc = 0.95 \tLoss = 0.1762613\n",
      "Iteration = 21251 \tAcc = 0.95 \tLoss = 0.0138772745\n",
      "Iteration = 21501 \tAcc = 0.95 \tLoss = 0.01031238\n",
      "Iteration = 21751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 22001 \tAcc = 1.0 \tLoss = 0.014023907\n",
      "Iteration = 22251 \tAcc = 0.95 \tLoss = 0.006876334\n",
      "Iteration = 22501 \tAcc = 0.95 \tLoss = 0.008808612\n",
      "Iteration = 22751 \tAcc = 0.95 \tLoss = 0.0003557242\n",
      "Iteration = 23001 \tAcc = 0.95 \tLoss = 0.009913305\n",
      "Iteration = 23251 \tAcc = 0.95 \tLoss = 0.000177578\n",
      "Iteration = 23501 \tAcc = 1.0 \tLoss = 0.16835465\n",
      "Iteration = 23751 \tAcc = 0.95 \tLoss = 2.6822127e-06\n",
      "Iteration = 24001 \tAcc = 1.0 \tLoss = 7.927732e-05\n",
      "Iteration = 24251 \tAcc = 0.95 \tLoss = 0.3609516\n",
      "Iteration = 24501 \tAcc = 1.0 \tLoss = 7.19454e-05\n",
      "Iteration = 24751 \tAcc = 0.95 \tLoss = 0.11154726\n",
      "Iteration = 25001 \tAcc = 0.95 \tLoss = 1.6093267e-06\n",
      "Iteration = 25251 \tAcc = 0.95 \tLoss = 0.07632217\n",
      "Iteration = 25501 \tAcc = 1.0 \tLoss = 0.04095179\n",
      "Iteration = 25751 \tAcc = 0.95 \tLoss = 0.0057512484\n",
      "Iteration = 26001 \tAcc = 0.95 \tLoss = 2.8133789e-05\n",
      "Iteration = 26251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 26501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 26751 \tAcc = 0.95 \tLoss = 0.091375604\n",
      "Iteration = 27001 \tAcc = 1.0 \tLoss = 0.6465946\n",
      "Iteration = 27251 \tAcc = 0.95 \tLoss = 0.0033845296\n",
      "Iteration = 27501 \tAcc = 0.95 \tLoss = 1.4245612e-05\n",
      "Iteration = 27751 \tAcc = 1.0 \tLoss = 0.05943871\n",
      "Iteration = 28001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 28251 \tAcc = 0.95 \tLoss = 0.034618635\n",
      "Iteration = 28501 \tAcc = 1.0 \tLoss = 1.9312092e-05\n",
      "Iteration = 28751 \tAcc = 1.0 \tLoss = 0.009108354\n",
      "Iteration = 29001 \tAcc = 0.95 \tLoss = 0.0067830733\n",
      "Iteration = 29251 \tAcc = 1.0 \tLoss = 1.83584e-05\n",
      "Iteration = 29501 \tAcc = 1.0 \tLoss = 0.031324495\n",
      "Iteration = 29751 \tAcc = 0.95 \tLoss = 0.0021592428\n",
      "Iteration = 30001 \tAcc = 1.0 \tLoss = 0.039256945\n",
      "Iteration = 30251 \tAcc = 0.95 \tLoss = 2.384186e-07\n",
      "Iteration = 30501 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 30751 \tAcc = 0.95 \tLoss = 2.4438203e-05\n",
      "Iteration = 31001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 31251 \tAcc = 1.0 \tLoss = 0.018048054\n",
      "Iteration = 31501 \tAcc = 0.95 \tLoss = 0.0011707242\n",
      "Iteration = 31751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 32001 \tAcc = 0.95 \tLoss = 3.737281e-05\n",
      "Iteration = 32251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 32501 \tAcc = 1.0 \tLoss = 1.5020483e-05\n",
      "Iteration = 32751 \tAcc = 1.0 \tLoss = 3.5644214e-05\n",
      "Iteration = 33001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 33251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 33501 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 33751 \tAcc = 1.0 \tLoss = 0.0401144\n",
      "Iteration = 34001 \tAcc = 0.95 \tLoss = 0.0008515764\n",
      "Iteration = 34251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 34501 \tAcc = 0.95 \tLoss = 0.00464629\n",
      "Iteration = 34751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 35001 \tAcc = 1.0 \tLoss = 0.49517247\n",
      "Iteration = 35251 \tAcc = 0.95 \tLoss = 5.2452224e-06\n",
      "Iteration = 35501 \tAcc = 1.0 \tLoss = 9.179157e-06\n",
      "Iteration = 35751 \tAcc = 1.0 \tLoss = 1.4782061e-05\n",
      "Iteration = 36001 \tAcc = 1.0 \tLoss = 0.025823114\n",
      "Iteration = 36251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 36501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 36751 \tAcc = 0.95 \tLoss = 7.92745e-06\n",
      "Iteration = 37001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 37251 \tAcc = 1.0 \tLoss = 4.3511486e-06\n",
      "Iteration = 37501 \tAcc = 1.0 \tLoss = 1.47224555e-05\n",
      "Iteration = 37751 \tAcc = 1.0 \tLoss = 0.41290173\n",
      "Iteration = 38001 \tAcc = 1.0 \tLoss = 0.41291937\n",
      "Iteration = 38251 \tAcc = 0.95 \tLoss = 0.00091749785\n",
      "Iteration = 38501 \tAcc = 0.95 \tLoss = 0.011751408\n",
      "Iteration = 38751 \tAcc = 1.0 \tLoss = 0.0048949523\n",
      "Iteration = 39001 \tAcc = 1.0 \tLoss = 0.4571853\n",
      "Iteration = 39251 \tAcc = 0.95 \tLoss = 1.1324889e-06\n",
      "Iteration = 39501 \tAcc = 0.95 \tLoss = 0.0058344\n",
      "Iteration = 39751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 40001 \tAcc = 1.0 \tLoss = 0.0031450323\n",
      "Iteration = 40251 \tAcc = 1.0 \tLoss = 2.1457695e-06\n",
      "Iteration = 40501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 40751 \tAcc = 0.95 \tLoss = 1.4901173e-06\n",
      "Iteration = 41001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 41251 \tAcc = 1.0 \tLoss = 0.0014288495\n",
      "Iteration = 41501 \tAcc = 1.0 \tLoss = 2.02656e-06\n",
      "Iteration = 41751 \tAcc = 1.0 \tLoss = 7.7486067e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 42001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 42251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 42501 \tAcc = 1.0 \tLoss = 0.0059868135\n",
      "Iteration = 42751 \tAcc = 0.95 \tLoss = 1.1324889e-06\n",
      "Iteration = 43001 \tAcc = 1.0 \tLoss = 0.50469184\n",
      "Iteration = 43251 \tAcc = 1.0 \tLoss = 1.4305125e-06\n",
      "Iteration = 43501 \tAcc = 1.0 \tLoss = 2.0861648e-06\n",
      "Iteration = 43751 \tAcc = 1.0 \tLoss = 0.0024366835\n",
      "Iteration = 44001 \tAcc = 1.0 \tLoss = 0.003281189\n",
      "Iteration = 44251 \tAcc = 1.0 \tLoss = 0.003804042\n",
      "Iteration = 44501 \tAcc = 0.95 \tLoss = 0.00284211\n",
      "Iteration = 44751 \tAcc = 1.0 \tLoss = 0.638136\n",
      "Iteration = 45001 \tAcc = 1.0 \tLoss = 4.768373e-07\n",
      "Iteration = 45251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 45501 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 45751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 46001 \tAcc = 0.95 \tLoss = 0.00013590783\n",
      "Iteration = 46251 \tAcc = 1.0 \tLoss = 1.549722e-06\n",
      "Iteration = 46501 \tAcc = 0.95 \tLoss = 5.36442e-07\n",
      "Iteration = 46751 \tAcc = 1.0 \tLoss = 6.556513e-07\n",
      "Iteration = 47001 \tAcc = 1.0 \tLoss = 0.0007501077\n",
      "Iteration = 47251 \tAcc = 1.0 \tLoss = 5.960466e-07\n",
      "Iteration = 47501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 47751 \tAcc = 1.0 \tLoss = 0.004360034\n",
      "Iteration = 48001 \tAcc = 1.0 \tLoss = 0.0005168462\n",
      "Iteration = 48251 \tAcc = 1.0 \tLoss = 0.33301243\n",
      "Iteration = 48501 \tAcc = 1.0 \tLoss = 0.011927647\n",
      "Iteration = 48751 \tAcc = 1.0 \tLoss = 0.0026369859\n",
      "Iteration = 49001 \tAcc = 1.0 \tLoss = 4.172326e-07\n",
      "Iteration = 49251 \tAcc = 1.0 \tLoss = 4.172326e-07\n",
      "Iteration = 49501 \tAcc = 1.0 \tLoss = 0.0013282176\n",
      "Iteration = 49751 \tAcc = 1.0 \tLoss = 0.00021972686\n",
      "Iteration = 50001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 50251 \tAcc = 0.95 \tLoss = 0.14446288\n",
      "Iteration = 50501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 50751 \tAcc = 1.0 \tLoss = 0.0071708756\n",
      "Iteration = 51001 \tAcc = 1.0 \tLoss = 2.9802328e-07\n",
      "Iteration = 51251 \tAcc = 1.0 \tLoss = 2.1457695e-06\n",
      "Iteration = 51501 \tAcc = 1.0 \tLoss = 5.960466e-07\n",
      "Iteration = 51751 \tAcc = 0.95 \tLoss = 3.5762793e-07\n",
      "Iteration = 52001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 52251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 52501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 52751 \tAcc = 1.0 \tLoss = 0.009469139\n",
      "Iteration = 53001 \tAcc = 1.0 \tLoss = 0.002347182\n",
      "Iteration = 53251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 53501 \tAcc = 1.0 \tLoss = 3.5762793e-07\n",
      "Iteration = 53751 \tAcc = 0.95 \tLoss = 0.001526686\n",
      "Iteration = 54001 \tAcc = 1.0 \tLoss = 0.39179826\n",
      "Iteration = 54251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 54501 \tAcc = 1.0 \tLoss = 4.768373e-07\n",
      "Iteration = 54751 \tAcc = 1.0 \tLoss = 0.00022431744\n",
      "Iteration = 55001 \tAcc = 1.0 \tLoss = 2.9802328e-07\n",
      "Iteration = 55251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 55501 \tAcc = 1.0 \tLoss = 0.0030856004\n",
      "Iteration = 55751 \tAcc = 1.0 \tLoss = 3.5762793e-07\n",
      "Iteration = 56001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 56251 \tAcc = 1.0 \tLoss = 0.1501425\n",
      "Iteration = 56501 \tAcc = 1.0 \tLoss = 0.002331051\n",
      "Iteration = 56751 \tAcc = 1.0 \tLoss = 0.0022847508\n",
      "Iteration = 57001 \tAcc = 1.0 \tLoss = 3.5762793e-07\n",
      "Iteration = 57251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 57501 \tAcc = 1.0 \tLoss = 2.384186e-07\n",
      "Iteration = 57751 \tAcc = 1.0 \tLoss = 0.0025154373\n",
      "Iteration = 58001 \tAcc = 1.0 \tLoss = 4.768373e-07\n",
      "Iteration = 58251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 58501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 58751 \tAcc = 1.0 \tLoss = 1.1920929e-07\n",
      "Iteration = 59001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 59251 \tAcc = 1.0 \tLoss = 0.003505466\n",
      "Iteration = 59501 \tAcc = 1.0 \tLoss = 0.0039097704\n",
      "Iteration = 59751 \tAcc = 1.0 \tLoss = 0.0016960572\n",
      "Iteration = 60001 \tAcc = 1.0 \tLoss = 0.010993066\n",
      "Iteration = 60251 \tAcc = 1.0 \tLoss = 1.1920929e-07\n",
      "Iteration = 60501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 60751 \tAcc = 1.0 \tLoss = 1.7881395e-07\n",
      "Iteration = 61001 \tAcc = 1.0 \tLoss = 0.009215247\n",
      "Iteration = 61251 \tAcc = 1.0 \tLoss = 0.19573471\n",
      "Iteration = 61501 \tAcc = 1.0 \tLoss = 0.22364725\n",
      "Iteration = 61751 \tAcc = 1.0 \tLoss = 0.007464903\n",
      "Iteration = 62001 \tAcc = 1.0 \tLoss = 0.0022113915\n",
      "Iteration = 62251 \tAcc = 1.0 \tLoss = 0.1069175\n",
      "Iteration = 62501 \tAcc = 1.0 \tLoss = 5.960466e-07\n",
      "Iteration = 62751 \tAcc = 1.0 \tLoss = 0.00928473\n",
      "Iteration = 63001 \tAcc = 1.0 \tLoss = 0.003457616\n",
      "Iteration = 63251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 63501 \tAcc = 1.0 \tLoss = 2.9802328e-07\n",
      "Iteration = 63751 \tAcc = 1.0 \tLoss = 0.0073621008\n",
      "Iteration = 64001 \tAcc = 1.0 \tLoss = 0.0026130814\n",
      "Iteration = 64251 \tAcc = 1.0 \tLoss = 0.0002475091\n",
      "Iteration = 64501 \tAcc = 1.0 \tLoss = 3.5762793e-07\n",
      "Iteration = 64751 \tAcc = 1.0 \tLoss = 1.1920929e-07\n",
      "Iteration = 65001 \tAcc = 1.0 \tLoss = 0.29036406\n",
      "Iteration = 65251 \tAcc = 1.0 \tLoss = 0.010700228\n",
      "Iteration = 65501 \tAcc = 1.0 \tLoss = 4.172326e-07\n",
      "Iteration = 65751 \tAcc = 1.0 \tLoss = 0.0020276585\n",
      "Iteration = 66001 \tAcc = 1.0 \tLoss = 1.1920929e-07\n",
      "Iteration = 66251 \tAcc = 1.0 \tLoss = 0.0019162765\n",
      "Iteration = 66501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 66751 \tAcc = 1.0 \tLoss = 5.9604645e-08\n",
      "Iteration = 67001 \tAcc = 1.0 \tLoss = 0.002092463\n",
      "Iteration = 67251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 67501 \tAcc = 1.0 \tLoss = 4.768373e-07\n",
      "Iteration = 67751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 68001 \tAcc = 1.0 \tLoss = 1.1920929e-07\n",
      "Iteration = 68251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 68501 \tAcc = 1.0 \tLoss = 4.768373e-07\n",
      "Iteration = 68751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 69001 \tAcc = 1.0 \tLoss = 0.0020402607\n",
      "Iteration = 69251 \tAcc = 1.0 \tLoss = 0.007996919\n",
      "Iteration = 69501 \tAcc = 1.0 \tLoss = 0.00026927044\n",
      "Iteration = 69751 \tAcc = 1.0 \tLoss = 0.0007677044\n",
      "Iteration = 70001 \tAcc = 1.0 \tLoss = 5.9604645e-08\n",
      "Iteration = 70251 \tAcc = 1.0 \tLoss = 0.00084048056\n",
      "Iteration = 70501 \tAcc = 1.0 \tLoss = 0.0056306394\n",
      "Iteration = 70751 \tAcc = 1.0 \tLoss = 5.960466e-07\n",
      "Iteration = 71001 \tAcc = 1.0 \tLoss = 0.09441408\n",
      "Iteration = 71251 \tAcc = 1.0 \tLoss = 5.9604645e-08\n",
      "Iteration = 71501 \tAcc = 1.0 \tLoss = 0.47725642\n",
      "Iteration = 71751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 72001 \tAcc = 1.0 \tLoss = 7.1525596e-07\n",
      "Iteration = 72251 \tAcc = 1.0 \tLoss = 0.0056880647\n",
      "Iteration = 72501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 72751 \tAcc = 1.0 \tLoss = 0.001041776\n",
      "Iteration = 73001 \tAcc = 1.0 \tLoss = 5.9604645e-08\n",
      "Iteration = 73251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 73501 \tAcc = 1.0 \tLoss = 0.18307403\n",
      "Iteration = 73751 \tAcc = 1.0 \tLoss = 0.0050565675\n",
      "Iteration = 74001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 74251 \tAcc = 1.0 \tLoss = 0.21676211\n",
      "Iteration = 74501 \tAcc = 1.0 \tLoss = 0.00022443668\n",
      "Iteration = 74751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 75001 \tAcc = 1.0 \tLoss = 0.0006830044\n",
      "Iteration = 75251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 75501 \tAcc = 1.0 \tLoss = 0.0015767721\n",
      "Iteration = 75751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 76001 \tAcc = 1.0 \tLoss = 0.0006045738\n",
      "Iteration = 76251 \tAcc = 1.0 \tLoss = 0.00062485185\n",
      "Iteration = 76501 \tAcc = 1.0 \tLoss = 8.3446537e-07\n",
      "Iteration = 76751 \tAcc = 1.0 \tLoss = 1.0132795e-06\n",
      "Iteration = 77001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 77251 \tAcc = 1.0 \tLoss = 0.00059342105\n",
      "Iteration = 77501 \tAcc = 1.0 \tLoss = 0.0048193047\n",
      "Iteration = 77751 \tAcc = 1.0 \tLoss = 0.0019222484\n",
      "Iteration = 78001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 78251 \tAcc = 1.0 \tLoss = 0.00017644532\n",
      "Iteration = 78501 \tAcc = 1.0 \tLoss = 0.0005081395\n",
      "Iteration = 78751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 79001 \tAcc = 1.0 \tLoss = 0.0032171446\n",
      "Iteration = 79251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 79501 \tAcc = 1.0 \tLoss = 0.0009413619\n",
      "Iteration = 79751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 80001 \tAcc = 1.0 \tLoss = 1.7881395e-07\n",
      "Iteration = 80251 \tAcc = 1.0 \tLoss = 0.08954944\n",
      "Iteration = 80501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 80751 \tAcc = 1.0 \tLoss = 2.384186e-07\n",
      "Iteration = 81001 \tAcc = 1.0 \tLoss = 0.005746333\n",
      "Iteration = 81251 \tAcc = 1.0 \tLoss = 0.001350659\n",
      "Iteration = 81501 \tAcc = 1.0 \tLoss = 0.0013602086\n",
      "Iteration = 81751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 82001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 82251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 82501 \tAcc = 1.0 \tLoss = 0.006531662\n",
      "Iteration = 82751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 83001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 83251 \tAcc = 1.0 \tLoss = 1.3113031e-06\n",
      "Iteration = 83501 \tAcc = 1.0 \tLoss = 0.0022085242\n",
      "Iteration = 83751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 84001 \tAcc = 1.0 \tLoss = 0.002443316\n",
      "Iteration = 84251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 84501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 84751 \tAcc = 1.0 \tLoss = 0.00012237583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 85001 \tAcc = 1.0 \tLoss = 2.9802328e-07\n",
      "Iteration = 85251 \tAcc = 1.0 \tLoss = 0.00034439532\n",
      "Iteration = 85501 \tAcc = 1.0 \tLoss = 0.00031023068\n",
      "Iteration = 85751 \tAcc = 1.0 \tLoss = 0.00042322293\n",
      "Iteration = 86001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 86251 \tAcc = 1.0 \tLoss = 0.004303643\n",
      "Iteration = 86501 \tAcc = 1.0 \tLoss = 1.4901173e-06\n",
      "Iteration = 86751 \tAcc = 1.0 \tLoss = 0.0012643578\n",
      "Iteration = 87001 \tAcc = 1.0 \tLoss = 2.9802328e-07\n",
      "Iteration = 87251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 87501 \tAcc = 1.0 \tLoss = 0.08409069\n",
      "Iteration = 87751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 88001 \tAcc = 1.0 \tLoss = 0.0009994732\n",
      "Iteration = 88251 \tAcc = 1.0 \tLoss = 9.6444965e-05\n",
      "Iteration = 88501 \tAcc = 1.0 \tLoss = 0.0009721473\n",
      "Iteration = 88751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 89001 \tAcc = 1.0 \tLoss = 1.4901173e-06\n",
      "Iteration = 89251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 89501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 89751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 90001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 90251 \tAcc = 1.0 \tLoss = 0.00022425782\n",
      "Iteration = 90501 \tAcc = 1.0 \tLoss = 0.0053349934\n",
      "Iteration = 90751 \tAcc = 1.0 \tLoss = 0.0041249716\n",
      "Iteration = 91001 \tAcc = 1.0 \tLoss = 8.672852e-05\n",
      "Iteration = 91251 \tAcc = 1.0 \tLoss = 0.0020218054\n",
      "Iteration = 91501 \tAcc = 1.0 \tLoss = 8.368842e-05\n",
      "Iteration = 91751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 92001 \tAcc = 1.0 \tLoss = 1.7881409e-06\n",
      "Iteration = 92251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 92501 \tAcc = 1.0 \tLoss = 1.3709077e-06\n",
      "Iteration = 92751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 93001 \tAcc = 1.0 \tLoss = 9.638535e-05\n",
      "Iteration = 93251 \tAcc = 1.0 \tLoss = 0.09131976\n",
      "Iteration = 93501 \tAcc = 1.0 \tLoss = 0.0001856261\n",
      "Iteration = 93751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 94001 \tAcc = 1.0 \tLoss = 0.0016765335\n",
      "Iteration = 94251 \tAcc = 1.0 \tLoss = 0.00367266\n",
      "Iteration = 94501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 94751 \tAcc = 1.0 \tLoss = 0.0802272\n",
      "Iteration = 95001 \tAcc = 1.0 \tLoss = 0.00019331655\n",
      "Iteration = 95251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 95501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 95751 \tAcc = 1.0 \tLoss = 0.0031895787\n",
      "Iteration = 96001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 96251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 96501 \tAcc = 1.0 \tLoss = 0.05759798\n",
      "Iteration = 96751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 97001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 97251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 97501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 97751 \tAcc = 1.0 \tLoss = 0.04246852\n",
      "Iteration = 98001 \tAcc = 1.0 \tLoss = 0.00017340494\n",
      "Iteration = 98251 \tAcc = 1.0 \tLoss = 0.0029980734\n",
      "Iteration = 98501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 98751 \tAcc = 1.0 \tLoss = 4.172326e-07\n",
      "Iteration = 99001 \tAcc = 1.0 \tLoss = 1.0132795e-06\n",
      "Iteration = 99251 \tAcc = 1.0 \tLoss = 0.0015238804\n",
      "Iteration = 99501 \tAcc = 1.0 \tLoss = 0.05625838\n",
      "Iteration = 99751 \tAcc = 1.0 \tLoss = -0.0\n"
     ]
    }
   ],
   "source": [
    "model.sgd(X, Y, 100000, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like it has fitted the data 100\\%. Let's see what the decision boundary looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a grid of points to classify\n",
    "xx1, xx2 = np.meshgrid(np.arange(-3, 3, 0.005), np.arange(-3, 3, 0.005))\n",
    "\n",
    "# Flatten the grid to pass into model\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()].T\n",
    "\n",
    "# Predict classification at every point on the grid\n",
    "Z = model.forward(grid)[1,:].numpy().reshape(xx1.shape)\n",
    "\n",
    "# Plot the prediction regions.\n",
    "plt.imshow(Z, interpolation='bicubic', origin='lower', extent=[-3, 3, -3, 3], \n",
    "           cmap=ListedColormap(['#1f77b4', '#ff7f0e']), alpha=0.55, aspect='auto')\n",
    "\n",
    "# Plot the original points.\n",
    "_ = plt.scatter(X[0,:], X[1,:], c=Y[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
