{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT! I am using TensorFlow 2.0 Alpha release, but this guide is\n",
    "# for TensorFlow 1.X so I use the backwards compatible API \n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "# IMPORTANT! If you are using TensorFlow 1.X (which is probably most\n",
    "# likely), you should use this import statement instead:\n",
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow NN Detailed\n",
    "\n",
    "This notebook goes through the training process for a specific neural network architecture (no explicit abstraction into layers) in very low-level, layer-by-layer detail.\n",
    "\n",
    "## Model\n",
    "\n",
    "The first step is to define the model we wish to use for prediction.\n",
    "\n",
    "### Input Layer\n",
    "\n",
    "- Input shape: $2 \\times 20$. \n",
    "- Output shape: $2 \\times 20$.\n",
    "\n",
    "Define tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(2, None), name='X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer 1\n",
    "\n",
    "- Input shape: $2 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "Define tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0329 15:27:22.425384 140265422280512 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py:883: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "W0_1 = tf.get_variable('W0_1', shape=(10, 1), initializer=tf.zeros_initializer)\n",
    "W_1 = tf.get_variable('W_1', shape=(2, 10), initializer=tf.random_normal_initializer(0.0, tf.sqrt(1 / 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Layer 1\n",
    "\n",
    "- Input shape: $10 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "No variables need to be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer 2\n",
    "\n",
    "- Input shape: $10 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "Define tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0_2 = tf.get_variable('W0_2', shape=(10, 1), initializer=tf.zeros_initializer)\n",
    "W_2 = tf.get_variable('W_2', shape=(10, 10), initializer=tf.random_normal_initializer(0.0, tf.sqrt(1 / 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Layer 2\n",
    "\n",
    "- Input shape: $10 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "No variables need to be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer 3\n",
    "\n",
    "- Input shape: $10 \\times 20 $\n",
    "- Output shape: $2 \\times 20 $\n",
    "\n",
    "Define tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0_3 = tf.get_variable('W0_3', shape=(2, 1), initializer=tf.zeros_initializer)\n",
    "W_3 = tf.get_variable('W_3', shape=(10, 2), initializer=tf.random_normal_initializer(0.0, tf.sqrt(1 / 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Layer\n",
    "\n",
    "- Input shape: $2 \\times 20$\n",
    "- Output shape: $2 \\times 20$\n",
    "\n",
    "No variables need to be defined.\n",
    "\n",
    "## Forward Pass\n",
    "\n",
    "Now we have the individual nodes of a neural network graph, but they aren't connected in a way to do anything useful. Let's connect them to run the forward pass to get ready for prediction.\n",
    "\n",
    "### Input Layer\n",
    "\n",
    "- Input shape: $ 2 \\times 20 $\n",
    "- Output shape: $ 2\\times 20 $\n",
    "\n",
    "Define forward result $A^{(0)}$ from placeholder values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_0 = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer 1\n",
    "\n",
    "- Input shape: $2 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "Define forward pre-activation result $Z^{(1)}$ from previous activation $A^{(0)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_1 = tf.add(tf.matmul(tf.transpose(W_1), A_0), W0_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Layer 1\n",
    "\n",
    "- Input shape: $10 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "Define forward activation result $A^{(1)}$ from previous pre-activation $Z^{(1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_1 = tf.maximum(tf.constant(0.0), Z_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer 2\n",
    "\n",
    "- Input shape: $10 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "Define forward pre-activation result $Z^{(2)}$ from previous activation $A^{(1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_2 = tf.add(tf.matmul(tf.transpose(W_2), A_1), W0_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Layer 2\n",
    "\n",
    "- Input shape: $10 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "Define forward activation result $A^{(2)}$ from previous pre-activation $Z^{(2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_2 = tf.maximum(tf.constant(0.0), Z_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer 3\n",
    "\n",
    "- Input shape: $10 \\times 20 $\n",
    "- Output shape: $2 \\times 20 $\n",
    "\n",
    "Define forward pre-activation result $Z^{(3)}$ from previous activation $A^{(2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_3 = tf.add(tf.matmul(tf.transpose(W_3), A_2), W0_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Layer\n",
    "\n",
    "- Input shape: $2 \\times 20$\n",
    "- Output shape: $2 \\times 20$\n",
    "\n",
    "Define forward activation result $A^{(3)}$ from previous pre-activation $Z^{(3)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_3 = tf.add(1.e-8, tf.divide(tf.exp(tf.subtract(Z_3, tf.reduce_max(Z_3))), tf.reduce_sum(tf.exp(tf.subtract(Z_3, tf.reduce_max(Z_3))), axis=0, keepdims=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The end result of the forward pass will be stored in $A^{(3)}$ as it is the last activation result computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "The graph has now been constructed to provide the forward pass result. Using some data, we can observe this prediction.\n",
    "\n",
    "### Data\n",
    "\n",
    "I am going to use the standard 'hard' data classification data set. (I only use NumPy arrays here because tensors cannot be iterated over without eager execution enable, which I do not want to introduce in this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[-0.23390341,  1.18151883, -2.46493986,  1.55322202,  1.27621763,\n",
    "                      2.39710997, -1.34403040, -0.46903436, -0.64673502, -1.44029872,\n",
    "                     -1.37537243,  1.05994811, -0.93311512,  1.02735575, -0.84138778,\n",
    "                     -2.22585412, -0.42591102,  1.03561105,  0.91125595, -2.26550369],\n",
    "                    [-0.92254932, -1.10309630, -2.41956036, -1.15509002, -1.04805327,\n",
    "                      0.08717325,  0.81847250, -0.75171045,  0.60664705,  0.80410947,\n",
    "                     -0.11600488,  1.03747218, -0.67210575,  0.99944446, -0.65559838,\n",
    "                     -0.40744784, -0.58367642,  1.05972780, -0.95991874, -1.41720255]])\n",
    "\n",
    "Y_train = np.array([[0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
    "                    [1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.scatter(X_train[0,:], X_train[1,:], c=Y_train[1,:], \n",
    "                cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "Now that we have some data, we can feed it through the network using the placeholders and forward pass (whose result is contained in `A`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    A_3_eval = sess.run(A_3, feed_dict={ X: X_train })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the evaluated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4217925 , 0.41070566, 0.32826102, 0.42412972, 0.42362353,\n",
       "        0.54641986, 0.4730857 , 0.44186425, 0.50301903, 0.47088966,\n",
       "        0.45274308, 0.54611164, 0.4581545 , 0.54436684, 0.45364237,\n",
       "        0.41914773, 0.455563  , 0.5475737 , 0.41558966, 0.43606028],\n",
       "       [0.57820743, 0.5892944 , 0.67173904, 0.5758703 , 0.5763765 ,\n",
       "        0.45358008, 0.5269143 , 0.55813575, 0.49698102, 0.5291103 ,\n",
       "        0.5472569 , 0.45388833, 0.54184556, 0.45563316, 0.5463577 ,\n",
       "        0.58085227, 0.544437  , 0.4524263 , 0.5844103 , 0.5639397 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_3_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Next step for training is to evaluate the loss and accuracy of the prediction with respect to the actual values.\n",
    "\n",
    "### Input\n",
    "\n",
    "In the graph environment, we need a placeholder for the actual labels.\n",
    "\n",
    "- Input shape: $2 \\times 20$\n",
    "- Output shape: $2 \\times 20$\n",
    "\n",
    "Define tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.placeholder(tf.float32, shape=(2, None), name='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log-Likelihood Multi-Class Loss\n",
    "\n",
    "- Input shape: $2 \\times 20$\n",
    "- Output shape: Scalar\n",
    "\n",
    "Define loss result `L` in terms of placeholder `Y` and predictions `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = tf.negative(tf.reduce_sum(tf.multiply(Y, tf.log(A_3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "- Input shape: $2 \\times 20$\n",
    "- Output shape: Scalar\n",
    "\n",
    "Define accuracy `accuracy` in terms of placeholder `Y` and predictions `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y, axis=0), tf.argmax(A_3, axis=0)), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "Now that we have our loss defined in our graph, we can feed the actual labels and get a loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    L_eval, accuracy_eval = sess.run([L, accuracy], feed_dict={ X: X_train, Y: Y_train })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the evaluated loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.542858, 0.5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_eval, accuracy_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "Now we have our prediction and the loss of that prediction, we can compute the gradient of the loss with respect to every `A`, `Z`, and weight matrix.\n",
    "\n",
    "### Negative Log-Likelihood Multi-Class Loss\n",
    "\n",
    "- Input shape: $2 \\times 20$\n",
    "- Output shape: $2 \\times 20$\n",
    "\n",
    "Define gradient of loss with respect to activations $\\partial L / \\partial A^{(3)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdA_3 = -Y / A_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Layer\n",
    "\n",
    "- Input shape: $2 \\times 20$\n",
    "- Output shape: $2 \\times 20$\n",
    "\n",
    "First, we calculate the gradient of softmax outputs with respect to softmax inputs $\\partial A^{(3)} / \\partial Z^{(3)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA_3dZ_3 = tf.einsum('jk,jk,ji->ijk', A_3, tf.subtract(tf.constant(1.0), A_3), tf.eye(2)) \\\n",
    "         + tf.einsum('jk,ik,ji->ijk', tf.negative(A_3), A_3, tf.subtract(tf.constant(1.0), tf.eye(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can multiply this result with $\\partial L / \\partial A^{(3)}$ to get $\\partial L / \\partial Z^{(3)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdZ_3 = tf.einsum('ikj,kj->ij', dA_3dZ_3, dLdA_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer 3\n",
    "\n",
    "- Input shape: $2 \\times 20$\n",
    "- Output shape: $ 10 \\times 20$\n",
    "\n",
    "First, we construct the gradients of the loss with respect to the weight matrices $\\partial L / \\partial W^{(3)}$ and $ \\partial L / \\partial W_0^{(3)} $ using the previous $\\partial L / \\partial Z^{(3)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdW_3 = tf.matmul(A_2, tf.transpose(dLdZ_3))\n",
    "dLdW0_3 = tf.reduce_sum(dLdZ_3, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can also compute the loss with respect to the layer activations $\\partial L / \\partial A^{(2)}$ using $\\partial L / \\partial Z^{(3)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdA_2 = tf.matmul(W_3, dLdZ_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Layer 2\n",
    "\n",
    "- Input shape: $10 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "Compute the gradient of loss with respect to layer pre-activations $\\partial L / \\partial Z^{(2)} $ using $\\partial L / \\partial A^{(2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdZ_2 = tf.multiply(tf.sign(A_2), dLdA_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer 2\n",
    "\n",
    "- Input shape: $10 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "First, we construct the gradients of the loss with respect to the weight matrices $\\partial L / \\partial W^{(2)}$ and $ \\partial L / \\partial W_0^{(2)} $ using the previous $\\partial L / \\partial Z^{(2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdW_2 = tf.matmul(A_1, tf.transpose(dLdZ_2))\n",
    "dLdW0_2 = tf.reduce_sum(dLdZ_2, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can also compute the loss with respect to the layer activations $\\partial L / \\partial A^{(1)}$ using $\\partial L / \\partial Z^{(2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdA_1 = tf.matmul(W_2, dLdZ_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Layer 1\n",
    "\n",
    "- Input shape: $10 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "Compute the gradient of loss with respect to layer pre-activations $\\partial L / \\partial Z^{(1)} $ using $\\partial L / \\partial A^{(1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdZ_1 = tf.multiply(tf.sign(A_1), dLdA_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer 1\n",
    "\n",
    "- Input shape: $10 \\times 20$\n",
    "- Output shape: $2 \\times 20$\n",
    "\n",
    "First, we construct the gradients of the loss with respect to the weight matrices $\\partial L / \\partial W^{(1)}$ and $ \\partial L / \\partial W_0^{(1)} $ using the previous $\\partial L / \\partial Z^{(1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdW_1 = tf.matmul(A_0, tf.transpose(dLdZ_1))\n",
    "dLdW0_1 = tf.reduce_sum(dLdZ_1, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can also compute the loss with respect to the layer activations $\\partial L / \\partial A^{(0)}$ using $\\partial L / \\partial Z^{(1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdA_0 = tf.matmul(W_1, dLdZ_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last gradient isn't really important and neither is $\\partial L / \\partial Z^{(0)}$ especially (why we don't continue calculating gradients once more into the input layer). But, to keep the calculation uniform, we include it (in the layer abstraction, this will be computed no matter what).\n",
    "\n",
    "## Parameter Update\n",
    "\n",
    "Now that we have all the necessary gradients, we can update the model parameters.\n",
    "\n",
    "### Input Layer\n",
    "\n",
    "- Input shape: $ 2 \\times 20 $\n",
    "- Output shape: $ 2\\times 20 $\n",
    "\n",
    "There are no parameters to update.\n",
    "\n",
    "### Linear Layer 1\n",
    "\n",
    "- Input shape: $2 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "We update both the $W^{(1)}$ and $W_0^{(1)}$ matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W_1_sgd_step = W_1.assign_sub(tf.scalar_mul(0.005, dLdW_1))\n",
    "W0_1_sgd_step = W0_1.assign_sub(tf.scalar_mul(0.005, dLdW0_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Layer 1\n",
    "\n",
    "- Input shape: $10 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "There are no parameters to update.\n",
    "\n",
    "### Linear Layer 2\n",
    "\n",
    "- Input shape: $10 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "We update both the $W^{(2)}$ and $W_0^{(2)}$ matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_2_sgd_step = W_2.assign_sub(tf.scalar_mul(0.005, dLdW_2))\n",
    "W0_2_sgd_step = W0_2.assign_sub(tf.scalar_mul(0.005, dLdW0_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Layer 2\n",
    "\n",
    "- Input shape: $10 \\times 20$\n",
    "- Output shape: $10 \\times 20$\n",
    "\n",
    "There are no parameters to update.\n",
    "\n",
    "### Linear Layer 3\n",
    "\n",
    "- Input shape: $10 \\times 20$\n",
    "- Output shape: $2 \\times 20$\n",
    "\n",
    "We update both the $W^{(3)}$ and $W_0^{(3)}$ matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_3_sgd_step = W_3.assign_sub(tf.scalar_mul(0.005, dLdW_3))\n",
    "W0_3_sgd_step = W0_3.assign_sub(tf.scalar_mul(0.005, dLdW0_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Layer\n",
    "\n",
    "- Input shape: $2 \\times 20$\n",
    "- Output shape: $2 \\times 20$\n",
    "\n",
    "There are no parameters to update.\n",
    "\n",
    "### Groups\n",
    "\n",
    "To make it easier to call this train process, we use a `tf.group`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_step = tf.group(W_3_sgd_step, W0_3_sgd_step, \n",
    "                    W_2_sgd_step, W0_2_sgd_step, \n",
    "                    W_1_sgd_step, W0_1_sgd_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "Now we can finally run the back-propogation and update the weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    L_eval_before = sess.run(L, feed_dict={ X: X_train, Y: Y_train })\n",
    "    \n",
    "    sess.run(sgd_step, feed_dict={ X: X_train, Y: Y_train })\n",
    "    \n",
    "    L_eval_after = sess.run(L, feed_dict={ X: X_train, Y: Y_train })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just make sure the loss went down after a single training update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16.044155, 13.103407)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_eval_before, L_eval_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to note that `L_eval` from way before, is **not** equal to `L_eval_before`. Shouldn't they be because we haven't performed any updates to the model since then? Wrong! Every time we create a `Session`, we create a new instance of our model with newly initialized variables.\n",
    "\n",
    "## Train\n",
    "\n",
    "We have all of the components of a model and how to train it. However, this only updates the model once. Let's put everything together and yield a fully trained model.\n",
    "\n",
    "### Saving\n",
    "\n",
    "Before we train, we need a way to save the model outside of a session after training it. For this, we use `tf.train.Saver`. We add it to our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "Now we can define a loop to train our model for many iterations and print the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.7 \tLoss = 0.6586426\n",
      "Iteration = 251 \tAcc = 0.8 \tLoss = 0.1255859\n",
      "Iteration = 501 \tAcc = 0.95 \tLoss = 0.3637161\n",
      "Iteration = 751 \tAcc = 0.9 \tLoss = 0.27488565\n",
      "Iteration = 1001 \tAcc = 0.95 \tLoss = 0.1424057\n",
      "Iteration = 1251 \tAcc = 0.95 \tLoss = 0.56308794\n",
      "Iteration = 1501 \tAcc = 0.95 \tLoss = 0.25681648\n",
      "Iteration = 1751 \tAcc = 0.9 \tLoss = 0.010139496\n",
      "Iteration = 2001 \tAcc = 0.95 \tLoss = 0.011756896\n",
      "Iteration = 2251 \tAcc = 0.9 \tLoss = 0.000577378\n",
      "Iteration = 2501 \tAcc = 0.95 \tLoss = 0.033755273\n",
      "Iteration = 2751 \tAcc = 0.95 \tLoss = 0.46667296\n",
      "Iteration = 3001 \tAcc = 0.95 \tLoss = 0.009796166\n",
      "Iteration = 3251 \tAcc = 0.95 \tLoss = 0.017909627\n",
      "Iteration = 3501 \tAcc = 0.95 \tLoss = 0.025559226\n",
      "Iteration = 3751 \tAcc = 0.95 \tLoss = 0.0029337483\n",
      "Iteration = 4001 \tAcc = 0.9 \tLoss = 0.33415917\n",
      "Iteration = 4251 \tAcc = 0.95 \tLoss = 0.07714847\n",
      "Iteration = 4501 \tAcc = 0.95 \tLoss = 0.008458524\n",
      "Iteration = 4751 \tAcc = 0.95 \tLoss = 0.2334516\n",
      "Iteration = 5001 \tAcc = 0.95 \tLoss = 0.010318885\n",
      "Iteration = 5251 \tAcc = 0.95 \tLoss = 0.0076059736\n",
      "Iteration = 5501 \tAcc = 0.95 \tLoss = 0.0019118573\n",
      "Iteration = 5751 \tAcc = 0.95 \tLoss = 0.006012778\n",
      "Iteration = 6001 \tAcc = 0.95 \tLoss = 6.30637e-05\n",
      "Iteration = 6251 \tAcc = 0.95 \tLoss = 0.0046208403\n",
      "Iteration = 6501 \tAcc = 0.95 \tLoss = 0.4395311\n",
      "Iteration = 6751 \tAcc = 0.95 \tLoss = 0.0014195977\n",
      "Iteration = 7001 \tAcc = 0.95 \tLoss = 0.41471088\n",
      "Iteration = 7251 \tAcc = 0.95 \tLoss = 0.02721109\n",
      "Iteration = 7501 \tAcc = 0.95 \tLoss = 0.0014018702\n",
      "Iteration = 7751 \tAcc = 0.95 \tLoss = 0.41138262\n",
      "Iteration = 8001 \tAcc = 0.95 \tLoss = 0.0013776966\n",
      "Iteration = 8251 \tAcc = 0.95 \tLoss = 0.024322363\n",
      "Iteration = 8501 \tAcc = 0.95 \tLoss = 0.0047848076\n",
      "Iteration = 8751 \tAcc = 0.95 \tLoss = 0.040704552\n",
      "Iteration = 9001 \tAcc = 0.95 \tLoss = 0.010861579\n",
      "Iteration = 9251 \tAcc = 0.95 \tLoss = 0.003448824\n",
      "Iteration = 9501 \tAcc = 0.85 \tLoss = 0.004714079\n",
      "Iteration = 9751 \tAcc = 0.95 \tLoss = 0.0037644343\n",
      "Iteration = 10001 \tAcc = 0.95 \tLoss = 0.0027251984\n",
      "Iteration = 10251 \tAcc = 0.95 \tLoss = 0.0010031126\n",
      "Iteration = 10501 \tAcc = 0.9 \tLoss = 0.008829237\n",
      "Iteration = 10751 \tAcc = 0.95 \tLoss = 0.0011680388\n",
      "Iteration = 11001 \tAcc = 0.95 \tLoss = 0.54258263\n",
      "Iteration = 11251 \tAcc = 0.95 \tLoss = 0.0009277593\n",
      "Iteration = 11501 \tAcc = 0.95 \tLoss = 0.01080223\n",
      "Iteration = 11751 \tAcc = 0.85 \tLoss = 1.3020401\n",
      "Iteration = 12001 \tAcc = 0.95 \tLoss = 0.015252388\n",
      "Iteration = 12251 \tAcc = 0.95 \tLoss = 0.12137014\n",
      "Iteration = 12501 \tAcc = 0.95 \tLoss = 0.48508886\n",
      "Iteration = 12751 \tAcc = 0.95 \tLoss = 0.002026882\n",
      "Iteration = 13001 \tAcc = 0.95 \tLoss = 0.00079979684\n",
      "Iteration = 13251 \tAcc = 0.9 \tLoss = 0.0021870192\n",
      "Iteration = 13501 \tAcc = 0.9 \tLoss = 0.3686876\n",
      "Iteration = 13751 \tAcc = 0.95 \tLoss = 1.6136333\n",
      "Iteration = 14001 \tAcc = 0.95 \tLoss = 0.007481057\n",
      "Iteration = 14251 \tAcc = 0.95 \tLoss = 0.00054141635\n",
      "Iteration = 14501 \tAcc = 0.85 \tLoss = 0.0015763541\n",
      "Iteration = 14751 \tAcc = 0.95 \tLoss = 7.629424e-06\n",
      "Iteration = 15001 \tAcc = 0.95 \tLoss = 0.43679667\n",
      "Iteration = 15251 \tAcc = 0.95 \tLoss = 0.043607816\n",
      "Iteration = 15501 \tAcc = 0.95 \tLoss = 0.0005160709\n",
      "Iteration = 15751 \tAcc = 0.95 \tLoss = 0.6406756\n",
      "Iteration = 16001 \tAcc = 0.95 \tLoss = 0.3405771\n",
      "Iteration = 16251 \tAcc = 0.95 \tLoss = 1.7730358\n",
      "Iteration = 16501 \tAcc = 0.95 \tLoss = 0.4125016\n",
      "Iteration = 16751 \tAcc = 0.9 \tLoss = 2.9802368e-06\n",
      "Iteration = 17001 \tAcc = 0.95 \tLoss = 0.43421656\n",
      "Iteration = 17251 \tAcc = 0.95 \tLoss = 0.0046001817\n",
      "Iteration = 17501 \tAcc = 0.95 \tLoss = 0.014442525\n",
      "Iteration = 17751 \tAcc = 0.95 \tLoss = 0.00023922205\n",
      "Iteration = 18001 \tAcc = 0.95 \tLoss = 0.0035690502\n",
      "Iteration = 18251 \tAcc = 0.95 \tLoss = 0.0010018001\n",
      "Iteration = 18501 \tAcc = 0.95 \tLoss = 0.37827754\n",
      "Iteration = 18751 \tAcc = 0.95 \tLoss = 0.003960754\n",
      "Iteration = 19001 \tAcc = 0.95 \tLoss = 0.0002509074\n",
      "Iteration = 19251 \tAcc = 0.95 \tLoss = 0.00065783435\n",
      "Iteration = 19501 \tAcc = 0.95 \tLoss = 0.008044686\n",
      "Iteration = 19751 \tAcc = 0.95 \tLoss = 0.00060505094\n",
      "Iteration = 20001 \tAcc = 0.95 \tLoss = 0.0002961001\n",
      "Iteration = 20251 \tAcc = 0.95 \tLoss = 0.0012702065\n",
      "Iteration = 20501 \tAcc = 0.95 \tLoss = 0.00025078817\n",
      "Iteration = 20751 \tAcc = 0.95 \tLoss = 0.00015271876\n",
      "Iteration = 21001 \tAcc = 0.95 \tLoss = 0.22345322\n",
      "Iteration = 21251 \tAcc = 0.95 \tLoss = 0.0054399846\n",
      "Iteration = 21501 \tAcc = 0.95 \tLoss = 0.0020020367\n",
      "Iteration = 21751 \tAcc = 0.95 \tLoss = 0.00046729037\n",
      "Iteration = 22001 \tAcc = 0.95 \tLoss = 0.5194394\n",
      "Iteration = 22251 \tAcc = 0.95 \tLoss = 0.3448601\n",
      "Iteration = 22501 \tAcc = 0.95 \tLoss = 0.00028787227\n",
      "Iteration = 22751 \tAcc = 0.95 \tLoss = 0.00020798223\n",
      "Iteration = 23001 \tAcc = 0.95 \tLoss = 0.0015071061\n",
      "Iteration = 23251 \tAcc = 0.95 \tLoss = 0.36261418\n",
      "Iteration = 23501 \tAcc = 0.95 \tLoss = 0.2313468\n",
      "Iteration = 23751 \tAcc = 0.95 \tLoss = 0.006165518\n",
      "Iteration = 24001 \tAcc = 0.95 \tLoss = 0.045515142\n",
      "Iteration = 24251 \tAcc = 0.95 \tLoss = 0.00011981251\n",
      "Iteration = 24501 \tAcc = 0.95 \tLoss = 1.64393\n",
      "Iteration = 24751 \tAcc = 0.95 \tLoss = 9.048395e-05\n",
      "Iteration = 25001 \tAcc = 0.95 \tLoss = 2.384186e-07\n",
      "Iteration = 25251 \tAcc = 0.95 \tLoss = 0.00030474536\n",
      "Iteration = 25501 \tAcc = 0.95 \tLoss = 0.00015987243\n",
      "Iteration = 25751 \tAcc = 0.95 \tLoss = 0.00010300213\n",
      "Iteration = 26001 \tAcc = 0.95 \tLoss = 0.078171104\n",
      "Iteration = 26251 \tAcc = 0.95 \tLoss = 8.392686e-05\n",
      "Iteration = 26501 \tAcc = 0.95 \tLoss = 0.00012291233\n",
      "Iteration = 26751 \tAcc = 0.95 \tLoss = 0.0004887583\n",
      "Iteration = 27001 \tAcc = 0.95 \tLoss = 0.35806343\n",
      "Iteration = 27251 \tAcc = 0.95 \tLoss = 0.31532696\n",
      "Iteration = 27501 \tAcc = 0.95 \tLoss = 0.044943154\n",
      "Iteration = 27751 \tAcc = 0.95 \tLoss = 0.000111347676\n",
      "Iteration = 28001 \tAcc = 0.95 \tLoss = 0.009520708\n",
      "Iteration = 28251 \tAcc = 0.95 \tLoss = 3.1829386e-05\n",
      "Iteration = 28501 \tAcc = 0.95 \tLoss = 0.0008050463\n",
      "Iteration = 28751 \tAcc = 0.95 \tLoss = 2.5987963e-05\n",
      "Iteration = 29001 \tAcc = 0.95 \tLoss = 0.45048752\n",
      "Iteration = 29251 \tAcc = 0.95 \tLoss = 3.3856013e-05\n",
      "Iteration = 29501 \tAcc = 0.95 \tLoss = 0.79916\n",
      "Iteration = 29751 \tAcc = 0.95 \tLoss = 0.00011611659\n",
      "Iteration = 30001 \tAcc = 0.95 \tLoss = 0.012055171\n",
      "Iteration = 30251 \tAcc = 0.95 \tLoss = 5.4360913e-05\n",
      "Iteration = 30501 \tAcc = 0.95 \tLoss = 0.033027068\n",
      "Iteration = 30751 \tAcc = 0.95 \tLoss = 0.5054551\n",
      "Iteration = 31001 \tAcc = 0.95 \tLoss = 0.0007193887\n",
      "Iteration = 31251 \tAcc = 0.95 \tLoss = 4.017434e-05\n",
      "Iteration = 31501 \tAcc = 0.95 \tLoss = 0.40687728\n",
      "Iteration = 31751 \tAcc = 0.95 \tLoss = 0.27366105\n",
      "Iteration = 32001 \tAcc = 0.95 \tLoss = 3.8982198e-05\n",
      "Iteration = 32251 \tAcc = 0.95 \tLoss = 1.8239187e-05\n",
      "Iteration = 32501 \tAcc = 0.95 \tLoss = 0.03557446\n",
      "Iteration = 32751 \tAcc = 0.95 \tLoss = 0.0010281124\n",
      "Iteration = 33001 \tAcc = 0.95 \tLoss = 0.007211459\n",
      "Iteration = 33251 \tAcc = 0.95 \tLoss = 0.033967745\n",
      "Iteration = 33501 \tAcc = 0.95 \tLoss = 0.10931331\n",
      "Iteration = 33751 \tAcc = 0.95 \tLoss = 2.0146574e-05\n",
      "Iteration = 34001 \tAcc = 0.95 \tLoss = 1.0490473e-05\n",
      "Iteration = 34251 \tAcc = 0.95 \tLoss = 9.298368e-06\n",
      "Iteration = 34501 \tAcc = 0.95 \tLoss = 0.006257581\n",
      "Iteration = 34751 \tAcc = 0.95 \tLoss = 0.46743745\n",
      "Iteration = 35001 \tAcc = 0.95 \tLoss = 0.0014660371\n",
      "Iteration = 35251 \tAcc = 0.95 \tLoss = 0.0013323358\n",
      "Iteration = 35501 \tAcc = 0.95 \tLoss = 6.3181124e-06\n",
      "Iteration = 35751 \tAcc = 0.95 \tLoss = 0.12569241\n",
      "Iteration = 36001 \tAcc = 1.0 \tLoss = 0.8936849\n",
      "Iteration = 36251 \tAcc = 0.95 \tLoss = 7.0333726e-06\n",
      "Iteration = 36501 \tAcc = 0.95 \tLoss = 0.60327977\n",
      "Iteration = 36751 \tAcc = 0.95 \tLoss = 8.106264e-06\n",
      "Iteration = 37001 \tAcc = 0.95 \tLoss = 0.59056926\n",
      "Iteration = 37251 \tAcc = 1.0 \tLoss = 2.6226078e-06\n",
      "Iteration = 37501 \tAcc = 0.95 \tLoss = 8.344685e-06\n",
      "Iteration = 37751 \tAcc = 0.95 \tLoss = 0.00582211\n",
      "Iteration = 38001 \tAcc = 1.0 \tLoss = 0.041870117\n",
      "Iteration = 38251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 38501 \tAcc = 1.0 \tLoss = 0.0009838413\n",
      "Iteration = 38751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 39001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 39251 \tAcc = 0.95 \tLoss = 0.004155436\n",
      "Iteration = 39501 \tAcc = 1.0 \tLoss = 0.008663704\n",
      "Iteration = 39751 \tAcc = 0.95 \tLoss = 1.0728842e-06\n",
      "Iteration = 40001 \tAcc = 0.95 \tLoss = 5.3644326e-06\n",
      "Iteration = 40251 \tAcc = 1.0 \tLoss = 0.011178514\n",
      "Iteration = 40501 \tAcc = 1.0 \tLoss = 0.07249942\n",
      "Iteration = 40751 \tAcc = 1.0 \tLoss = 0.022282679\n",
      "Iteration = 41001 \tAcc = 0.95 \tLoss = 0.0007216553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 41251 \tAcc = 1.0 \tLoss = 7.1525596e-07\n",
      "Iteration = 41501 \tAcc = 0.95 \tLoss = 9.536748e-07\n",
      "Iteration = 41751 \tAcc = 1.0 \tLoss = 0.04791498\n",
      "Iteration = 42001 \tAcc = 0.95 \tLoss = 0.0002709398\n",
      "Iteration = 42251 \tAcc = 0.95 \tLoss = 9.536748e-07\n",
      "Iteration = 42501 \tAcc = 1.0 \tLoss = 4.768373e-07\n",
      "Iteration = 42751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 43001 \tAcc = 1.0 \tLoss = 7.3910032e-06\n",
      "Iteration = 43251 \tAcc = 0.95 \tLoss = 5.1260126e-06\n",
      "Iteration = 43501 \tAcc = 0.95 \tLoss = 0.010487944\n",
      "Iteration = 43751 \tAcc = 1.0 \tLoss = 0.0041623195\n",
      "Iteration = 44001 \tAcc = 1.0 \tLoss = 0.0300967\n",
      "Iteration = 44251 \tAcc = 0.95 \tLoss = 4.768373e-07\n",
      "Iteration = 44501 \tAcc = 1.0 \tLoss = 4.768373e-07\n",
      "Iteration = 44751 \tAcc = 1.0 \tLoss = 5.960466e-07\n",
      "Iteration = 45001 \tAcc = 1.0 \tLoss = 0.0001440152\n",
      "Iteration = 45251 \tAcc = 0.95 \tLoss = 0.048196718\n",
      "Iteration = 45501 \tAcc = 1.0 \tLoss = 0.0034689205\n",
      "Iteration = 45751 \tAcc = 0.95 \tLoss = 0.01099397\n",
      "Iteration = 46001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 46251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 46501 \tAcc = 0.95 \tLoss = 0.00052042434\n",
      "Iteration = 46751 \tAcc = 1.0 \tLoss = 0.014163426\n",
      "Iteration = 47001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 47251 \tAcc = 1.0 \tLoss = 8.463895e-06\n",
      "Iteration = 47501 \tAcc = 1.0 \tLoss = 1.0403198\n",
      "Iteration = 47751 \tAcc = 1.0 \tLoss = 0.015300261\n",
      "Iteration = 48001 \tAcc = 0.95 \tLoss = 0.013571144\n",
      "Iteration = 48251 \tAcc = 0.95 \tLoss = 0.20544809\n",
      "Iteration = 48501 \tAcc = 1.0 \tLoss = 0.011260069\n",
      "Iteration = 48751 \tAcc = 1.0 \tLoss = 9.298368e-06\n",
      "Iteration = 49001 \tAcc = 0.95 \tLoss = 9.298368e-06\n",
      "Iteration = 49251 \tAcc = 0.95 \tLoss = 0.0051702172\n",
      "Iteration = 49501 \tAcc = 0.95 \tLoss = 1.1920929e-07\n",
      "Iteration = 49751 \tAcc = 1.0 \tLoss = 5.018837e-05\n",
      "Iteration = 50001 \tAcc = 0.95 \tLoss = 1.1920929e-07\n",
      "Iteration = 50251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 50501 \tAcc = 1.0 \tLoss = 1.1920929e-07\n",
      "Iteration = 50751 \tAcc = 1.0 \tLoss = 0.014488847\n",
      "Iteration = 51001 \tAcc = 1.0 \tLoss = 0.0034461324\n",
      "Iteration = 51251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 51501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 51751 \tAcc = 1.0 \tLoss = 1.1521697\n",
      "Iteration = 52001 \tAcc = 1.0 \tLoss = 5.102288e-05\n",
      "Iteration = 52251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 52501 \tAcc = 0.95 \tLoss = 0.0014527258\n",
      "Iteration = 52751 \tAcc = 1.0 \tLoss = 4.5538985e-05\n",
      "Iteration = 53001 \tAcc = 1.0 \tLoss = 0.00010347902\n",
      "Iteration = 53251 \tAcc = 1.0 \tLoss = 0.0022852286\n",
      "Iteration = 53501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 53751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 54001 \tAcc = 0.95 \tLoss = 0.00027415933\n",
      "Iteration = 54251 \tAcc = 1.0 \tLoss = 2.4915053e-05\n",
      "Iteration = 54501 \tAcc = 1.0 \tLoss = 7.176657e-05\n",
      "Iteration = 54751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 55001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 55251 \tAcc = 0.95 \tLoss = 0.00014723431\n",
      "Iteration = 55501 \tAcc = 1.0 \tLoss = 0.63034314\n",
      "Iteration = 55751 \tAcc = 0.95 \tLoss = 1.9669726e-05\n",
      "Iteration = 56001 \tAcc = 0.95 \tLoss = 0.2653946\n",
      "Iteration = 56251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 56501 \tAcc = 0.95 \tLoss = 0.006786854\n",
      "Iteration = 56751 \tAcc = 0.95 \tLoss = 0.03749349\n",
      "Iteration = 57001 \tAcc = 1.0 \tLoss = 0.00029514614\n",
      "Iteration = 57251 \tAcc = 1.0 \tLoss = 0.00067208934\n",
      "Iteration = 57501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 57751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 58001 \tAcc = 1.0 \tLoss = 0.85499656\n",
      "Iteration = 58251 \tAcc = 0.95 \tLoss = 7.987054e-06\n",
      "Iteration = 58501 \tAcc = 1.0 \tLoss = 0.00035375654\n",
      "Iteration = 58751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 59001 \tAcc = 1.0 \tLoss = 0.00087722857\n",
      "Iteration = 59251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 59501 \tAcc = 0.95 \tLoss = 0.22520044\n",
      "Iteration = 59751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 60001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 60251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 60501 \tAcc = 1.0 \tLoss = 0.00048207934\n",
      "Iteration = 60751 \tAcc = 1.0 \tLoss = 0.00037951517\n",
      "Iteration = 61001 \tAcc = 1.0 \tLoss = 0.0007872103\n",
      "Iteration = 61251 \tAcc = 1.0 \tLoss = 0.0012549881\n",
      "Iteration = 61501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 61751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 62001 \tAcc = 1.0 \tLoss = 0.00045065305\n",
      "Iteration = 62251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 62501 \tAcc = 0.95 \tLoss = 0.021271884\n",
      "Iteration = 62751 \tAcc = 1.0 \tLoss = 0.7681341\n",
      "Iteration = 63001 \tAcc = 1.0 \tLoss = 0.7709824\n",
      "Iteration = 63251 \tAcc = 0.95 \tLoss = 0.0010638529\n",
      "Iteration = 63501 \tAcc = 0.95 \tLoss = 0.00032752156\n",
      "Iteration = 63751 \tAcc = 0.95 \tLoss = 0.000293119\n",
      "Iteration = 64001 \tAcc = 1.0 \tLoss = 0.0069849086\n",
      "Iteration = 64251 \tAcc = 1.0 \tLoss = 0.00059026014\n",
      "Iteration = 64501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 64751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 65001 \tAcc = 1.0 \tLoss = 0.58528554\n",
      "Iteration = 65251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 65501 \tAcc = 1.0 \tLoss = 0.00014008072\n",
      "Iteration = 65751 \tAcc = 1.0 \tLoss = 0.008987942\n",
      "Iteration = 66001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 66251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 66501 \tAcc = 1.0 \tLoss = 0.00027559025\n",
      "Iteration = 66751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 67001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 67251 \tAcc = 1.0 \tLoss = 3.0994463e-06\n",
      "Iteration = 67501 \tAcc = 1.0 \tLoss = 0.00035363727\n",
      "Iteration = 67751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 68001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 68251 \tAcc = 1.0 \tLoss = 2.9802368e-06\n",
      "Iteration = 68501 \tAcc = 1.0 \tLoss = 0.0064866664\n",
      "Iteration = 68751 \tAcc = 1.0 \tLoss = 0.0054298565\n",
      "Iteration = 69001 \tAcc = 1.0 \tLoss = 0.0032156496\n",
      "Iteration = 69251 \tAcc = 1.0 \tLoss = 0.0005290119\n",
      "Iteration = 69501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 69751 \tAcc = 1.0 \tLoss = 0.47178745\n",
      "Iteration = 70001 \tAcc = 1.0 \tLoss = 0.00018556647\n",
      "Iteration = 70251 \tAcc = 1.0 \tLoss = 0.0006938003\n",
      "Iteration = 70501 \tAcc = 1.0 \tLoss = 0.1651967\n",
      "Iteration = 70751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 71001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 71251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 71501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 71751 \tAcc = 1.0 \tLoss = 0.00024661483\n",
      "Iteration = 72001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 72251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 72501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 72751 \tAcc = 1.0 \tLoss = 7.9158104e-05\n",
      "Iteration = 73001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 73251 \tAcc = 1.0 \tLoss = 0.004108932\n",
      "Iteration = 73501 \tAcc = 1.0 \tLoss = 0.00032680607\n",
      "Iteration = 73751 \tAcc = 1.0 \tLoss = 0.0042486317\n",
      "Iteration = 74001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 74251 \tAcc = 1.0 \tLoss = 0.00029061487\n",
      "Iteration = 74501 \tAcc = 1.0 \tLoss = 0.00092859456\n",
      "Iteration = 74751 \tAcc = 1.0 \tLoss = 0.0004588226\n",
      "Iteration = 75001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 75251 \tAcc = 1.0 \tLoss = 0.0003869686\n",
      "Iteration = 75501 \tAcc = 1.0 \tLoss = 0.00043025927\n",
      "Iteration = 75751 \tAcc = 1.0 \tLoss = 7.1525596e-07\n",
      "Iteration = 76001 \tAcc = 1.0 \tLoss = 0.0010951792\n",
      "Iteration = 76251 \tAcc = 1.0 \tLoss = 0.0024479167\n",
      "Iteration = 76501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 76751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 77001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 77251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 77501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 77751 \tAcc = 1.0 \tLoss = 4.768373e-07\n",
      "Iteration = 78001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 78251 \tAcc = 1.0 \tLoss = 0.00088772824\n",
      "Iteration = 78501 \tAcc = 1.0 \tLoss = 0.00019975512\n",
      "Iteration = 78751 \tAcc = 1.0 \tLoss = 0.00026998587\n",
      "Iteration = 79001 \tAcc = 1.0 \tLoss = 0.0026301132\n",
      "Iteration = 79251 \tAcc = 1.0 \tLoss = 0.06328618\n",
      "Iteration = 79501 \tAcc = 1.0 \tLoss = 0.00018437416\n",
      "Iteration = 79751 \tAcc = 1.0 \tLoss = 0.00019224346\n",
      "Iteration = 80001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 80251 \tAcc = 1.0 \tLoss = 0.00021132079\n",
      "Iteration = 80501 \tAcc = 1.0 \tLoss = 0.11050478\n",
      "Iteration = 80751 \tAcc = 1.0 \tLoss = 0.2929604\n",
      "Iteration = 81001 \tAcc = 1.0 \tLoss = 0.00022741758\n",
      "Iteration = 81251 \tAcc = 1.0 \tLoss = 0.003457018\n",
      "Iteration = 81501 \tAcc = 1.0 \tLoss = 0.21611321\n",
      "Iteration = 81751 \tAcc = 1.0 \tLoss = 0.012420219\n",
      "Iteration = 82001 \tAcc = 1.0 \tLoss = 0.00027845206\n",
      "Iteration = 82251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 82501 \tAcc = 1.0 \tLoss = 0.00025448462\n",
      "Iteration = 82751 \tAcc = 1.0 \tLoss = 0.00021465936\n",
      "Iteration = 83001 \tAcc = 1.0 \tLoss = 0.00013745776\n",
      "Iteration = 83251 \tAcc = 1.0 \tLoss = 0.00084447744\n",
      "Iteration = 83501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 83751 \tAcc = 1.0 \tLoss = 0.27038437\n",
      "Iteration = 84001 \tAcc = 1.0 \tLoss = 0.0068459664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 84251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 84501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 84751 \tAcc = 1.0 \tLoss = 0.00839583\n",
      "Iteration = 85001 \tAcc = 1.0 \tLoss = 0.27267826\n",
      "Iteration = 85251 \tAcc = 1.0 \tLoss = 0.0056754164\n",
      "Iteration = 85501 \tAcc = 1.0 \tLoss = 0.00021871335\n",
      "Iteration = 85751 \tAcc = 1.0 \tLoss = 0.00015629559\n",
      "Iteration = 86001 \tAcc = 1.0 \tLoss = 0.00832112\n",
      "Iteration = 86251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 86501 \tAcc = 1.0 \tLoss = 4.4466055e-05\n",
      "Iteration = 86751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 87001 \tAcc = 1.0 \tLoss = 0.31467137\n",
      "Iteration = 87251 \tAcc = 1.0 \tLoss = 4.3750766e-05\n",
      "Iteration = 87501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 87751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 88001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 88251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 88501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 88751 \tAcc = 1.0 \tLoss = 1.1920929e-07\n",
      "Iteration = 89001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 89251 \tAcc = 1.0 \tLoss = 0.00012064708\n",
      "Iteration = 89501 \tAcc = 1.0 \tLoss = 3.266388e-05\n",
      "Iteration = 89751 \tAcc = 1.0 \tLoss = 0.00020380905\n",
      "Iteration = 90001 \tAcc = 1.0 \tLoss = 7.808513e-05\n",
      "Iteration = 90251 \tAcc = 1.0 \tLoss = 0.0001252968\n",
      "Iteration = 90501 \tAcc = 1.0 \tLoss = 0.00012994657\n",
      "Iteration = 90751 \tAcc = 1.0 \tLoss = 0.048224553\n",
      "Iteration = 91001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 91251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 91501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 91751 \tAcc = 1.0 \tLoss = 0.0029253792\n",
      "Iteration = 92001 \tAcc = 1.0 \tLoss = 0.00012017018\n",
      "Iteration = 92251 \tAcc = 1.0 \tLoss = 0.005703591\n",
      "Iteration = 92501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 92751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 93001 \tAcc = 1.0 \tLoss = 0.0035217954\n",
      "Iteration = 93251 \tAcc = 1.0 \tLoss = 0.03864497\n",
      "Iteration = 93501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 93751 \tAcc = 1.0 \tLoss = 0.045132577\n",
      "Iteration = 94001 \tAcc = 1.0 \tLoss = 0.03690387\n",
      "Iteration = 94251 \tAcc = 1.0 \tLoss = 1.1920929e-07\n",
      "Iteration = 94501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 94751 \tAcc = 1.0 \tLoss = 0.00012005096\n",
      "Iteration = 95001 \tAcc = 1.0 \tLoss = 0.0001659531\n",
      "Iteration = 95251 \tAcc = 1.0 \tLoss = 1.1920929e-07\n",
      "Iteration = 95501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 95751 \tAcc = 1.0 \tLoss = 0.00011039389\n",
      "Iteration = 96001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 96251 \tAcc = 1.0 \tLoss = 0.0008179313\n",
      "Iteration = 96501 \tAcc = 1.0 \tLoss = 0.08652666\n",
      "Iteration = 96751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 97001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 97251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 97501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 97751 \tAcc = 1.0 \tLoss = 0.00010419435\n",
      "Iteration = 98001 \tAcc = 1.0 \tLoss = 0.073019296\n",
      "Iteration = 98251 \tAcc = 1.0 \tLoss = 0.102963336\n",
      "Iteration = 98501 \tAcc = 1.0 \tLoss = 3.0279618e-05\n",
      "Iteration = 98751 \tAcc = 1.0 \tLoss = 2.8372213e-05\n",
      "Iteration = 99001 \tAcc = 1.0 \tLoss = 0.00010204836\n",
      "Iteration = 99251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 99501 \tAcc = 1.0 \tLoss = 2.479584e-05\n",
      "Iteration = 99751 \tAcc = 1.0 \tLoss = -0.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for it in range(100000):\n",
    "        \n",
    "        t = np.random.randint(20)\n",
    "        \n",
    "        L_eval, _ = sess.run([L, sgd_step], feed_dict={ X: X_train[:, t:t+1], Y: Y_train[:, t:t+1] })\n",
    "        \n",
    "        if it % 250 == 1:\n",
    "            accuracy_eval = sess.run(accuracy, feed_dict={ X: X_train, Y: Y_train })\n",
    "            print('Iteration =', it, '\\tAcc =', accuracy_eval, '\\tLoss =', L_eval, flush=True)\n",
    "            \n",
    "    saver.save(sess, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Regions\n",
    "\n",
    "Our network successully classifies all of our points. Let's take a look at the decision regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0329 15:28:43.791294 140265422280512 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Restore variables\n",
    "    saver.restore(sess, 'model.ckpt')\n",
    "    \n",
    "    # Create a grid of points to classify\n",
    "    xx1, xx2 = np.meshgrid(np.arange(-3, 3, 0.005), np.arange(-3, 3, 0.005))\n",
    "\n",
    "    # Flatten the grid to pass into model\n",
    "    grid = np.c_[xx1.ravel(), xx2.ravel()].T\n",
    "\n",
    "    # Predict classification at every point on the grid\n",
    "    Z = sess.run(A_3, feed_dict={ X: grid, Y: Y_train })[1, :].reshape(xx1.shape)\n",
    "\n",
    "    # Plot the prediction regions.\n",
    "    plt.imshow(Z, interpolation='bicubic', origin='lower', extent=[-3, 3, -3, 3], \n",
    "               cmap=ListedColormap(['#1f77b4', '#ff7f0e']), alpha=0.55, aspect='auto')\n",
    "\n",
    "    # Plot the original points.\n",
    "    _ = plt.scatter(X_train[0,:], X_train[1,:], c=Y_train[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
