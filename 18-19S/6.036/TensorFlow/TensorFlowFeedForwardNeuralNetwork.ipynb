{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Feed-Forward Neural Network\n",
    "\n",
    "Instead of using NumPy to construct our neural network, we will use TensorFlow. The purpose of this notebook is to get familiar with TensorFlow's low-level functionality involving static graphs and sessions. As a result, we won't (yet) be using any of the great features in TensorFlow (like eager execution or automatic differentiation). This should nearly mirror the neural network implemented with NumPy.\n",
    "\n",
    "## Layers \n",
    "\n",
    "For our neural network, we want to abstract away from individual neurons and focus on layers. Each element of the network will be defined by a certain layer.\n",
    "\n",
    "### Base Layer\n",
    "\n",
    "The abstract base layer ensures that all called methods by the `Sequential` model exist on the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Base class for neural network layers.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we only need to worry about the `sgd_step` method as some layers won't need to update any weights because they don't have any (e.g. activation layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_sgd_step(self, lrate):\n",
    "        \"\"\"Some layers do not have weights to update on gradient descent steps.\n",
    "        \n",
    "        Args:\n",
    "            lrate (float): Learning rate for stochastic gradient descent.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "# Add this method to the Layer class\n",
    "Layer.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "\n",
    "This layer acts as a placeholder for data fed into our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Layer):\n",
    "    \"\"\"Placeholder for data inputs into the network.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up this layer, we just need to know the number of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, m):\n",
    "        \"\"\"Initializes the layer's dimensions.\n",
    "        \n",
    "        Args:\n",
    "            m (int): Number of input features to the network.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        \n",
    "# Add this method to the Input layer class\n",
    "Input.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the architecture is defined, we need to build the graph by connecting layers. This is accomplished through build methods. This first one defines variables within the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build(self, graph):\n",
    "        \"\"\"Creates the input layer placeholders in a provided graph.\n",
    "        \n",
    "        Args:\n",
    "            graph (Graph): A tf.Graph instance within which the variables should\n",
    "                be defined.\n",
    "        \n",
    "        \"\"\"\n",
    "        with graph.as_default():\n",
    "            with tf.variable_scope(name_or_scope=None, default_name='Input'):\n",
    "                self.X = tf.placeholder(tf.float32, shape=(self.m, None), name='X')\n",
    "\n",
    "# Add this method to the Input layer class\n",
    "Input.build = build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward pass of this layer is simple, just return the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_forward(self, graph):\n",
    "        \"\"\"Returns the forward tensor for the layer.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Result of forward operation.\n",
    "            \n",
    "        \"\"\"\n",
    "        return self.X\n",
    "    \n",
    "# Add this method to the Input layer class\n",
    "Input.build_forward = build_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward pass should do nothing as the function is the identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_backward(self, dLdZ):\n",
    "        \"\"\"Returns the backward tensor of the layer.\n",
    "        \n",
    "        Args:\n",
    "            dLdZ (Tensor): An n by b matrix of loss gradients with\n",
    "                respect to the output of the layer.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A m by b matrix of loss gradients with respect\n",
    "                to the input of the layer.\n",
    "                \n",
    "        \"\"\"\n",
    "        return dLdZ\n",
    "\n",
    "# Add this method to the Input layer class\n",
    "Input.build_backward = build_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no weights to update in `sgd_step`.\n",
    "\n",
    "### Linear Layer\n",
    "\n",
    "This is the simplest layer that makes up the majority of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"A simple, fully-connected linear layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up this layer, we need to know the input and output dimensions ahead of time. Using this information, we randomly initialize the weight matrices. We use TensorFlow variables instead of NumPy arrays. The `get_variable` defines node on the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, m, n):\n",
    "        \"\"\"Initializes the layer input and output dimensions. \n",
    "\n",
    "        Note: Kernel is initialized using normal distribution with mean 0 and \n",
    "        variance 1 / m. All biases are initialized to zero.\n",
    "\n",
    "        Args:\n",
    "            m (int): Number of inputs to the layer.\n",
    "            n (int): Number of outputs from the layer.\n",
    "\n",
    "        \"\"\"\n",
    "        self.m, self.n = m, n\n",
    "\n",
    "# Add this method to the Linear layer class\n",
    "Linear.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a build method to define the variables within the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build(self, graph):\n",
    "        \"\"\"Creates the layer variables in the provided graph.\n",
    "        \n",
    "        Args:\n",
    "            graph (Graph): A tf.Graph instance within which the variables should\n",
    "                be defined.        \n",
    "        \n",
    "        \"\"\"\n",
    "        with graph.as_default():\n",
    "            with tf.variable_scope(name_or_scope=None, default_name='Linear'):\n",
    "\n",
    "                self.W0 = tf.get_variable('W0', (self.n, 1), initializer=tf.zeros_initializer)\n",
    "                self.W = tf.get_variable('W', (self.m, self.n), initializer=tf.random_normal_initializer(0.0, tf.sqrt(1 / self.m)))\n",
    "                \n",
    "# Add this method to the Linear layer class\n",
    "Linear.build = build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method will compute the output of the layer given a set of $m$ inputs from the previous layer for a batch of size $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_forward(self, A):\n",
    "        \"\"\"Computes the forward pass through the linear network for a batch.\n",
    "\n",
    "        Args:\n",
    "            A (Tensor): An m by b matrix representing the m activations from the\n",
    "                previous layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: An n by b matrix representing the result of passing the \n",
    "                activations through the network layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "\n",
    "        return tf.add(tf.matmul(tf.transpose(self.W), self.A), self.W0)\n",
    "\n",
    "# Add this method to the Linear layer class\n",
    "Linear.build_forward = build_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method will compute the gradient of the loss with respect to the inputs to the layer for a batch of size $b$. Note: There is an implicit sum over all $b$ in the `dLdW` calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_backward(self, graph, dLdZ):\n",
    "        \"\"\"Uses the gradient of loss with respect to outputs of the layer for a \n",
    "        batch to update the sum of gradients of the loss with respect to the \n",
    "        weights for the entire batch. Also returns the gradient of the loss with \n",
    "        respect to the inputs to the layer for a batch.\n",
    "\n",
    "        Args:\n",
    "            dLdZ (Tensor): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the layer outputs for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: An m by b matrix representing the gradient of the loss with \n",
    "                respect to the inputs to the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.dLdW = tf.matmul(self.A, tf.transpose(dLdZ))  # Implicit sum over all b\n",
    "        self.dLdW0 = tf.reduce_sum(dLdZ, axis=1, keepdims=True)\n",
    "\n",
    "        return tf.matmul(self.W, dLdZ)\n",
    "\n",
    "# Add this method to the Linear layer class\n",
    "Linear.build_backward = build_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need a method to update the weight matrices using the current weight gradients for a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_sgd_step(self, lrate):\n",
    "        \"\"\"Performs a single step of gradient descent to update the weights for a \n",
    "        single batch of points.\n",
    "\n",
    "        Args:\n",
    "            lrate (float): A learning rate to scale the gradient for the update.\n",
    "\n",
    "        \"\"\"\n",
    "        self.W = self.W.assign(tf.subtract(self.W, tf.scalar_mul(lrate, self.dLdW)))\n",
    "        self.W0 = self.W0.assign(tf.subtract(self.W0, tf.scalar_mul(lrate, self.dLdW0)))\n",
    "        \n",
    "# Add this method to the Linear layer class\n",
    "Linear.build_sgd_step = build_sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent Activation Layer\n",
    "\n",
    "This layer encapsulates the hyperbolic tangent activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    \"\"\"Hyperbolic tangent activation layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method take a preactivation from the previous layer and computes the activation using the hyperbolic tangent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the output of the hyperbolic tangent activation layer.\n",
    "\n",
    "        Args:\n",
    "            Z (Tensor): An n by b matrix representing the input pre-activations\n",
    "                of the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: An n by b matrix representing the output of the layer after\n",
    "                using the hyperbolic tangent activation on all inputs for a batch\n",
    "                of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = tf.tanh(Z)\n",
    "\n",
    "        return self.A\n",
    "\n",
    "# Add this method to the Tanh layer class\n",
    "Tanh.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method computes the gradient of the loss with respect to the inputs to the activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (Tensor): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return tf.multiply(tf.subtract(tf.constant(1), tf.square(self.A)), dLdA)\n",
    "\n",
    "# Add this method to the Tanh layer class\n",
    "Tanh.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has no weights to update. Therefore, no `sgd_step` function is required.\n",
    "\n",
    "### Rectified Linear Unit Activation Layer\n",
    "\n",
    "This layer encapsulates the rectified linear unit activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"Rectified linear unit layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method take a preactivation from the previous layer and computes the activation using the relu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the output of the rectified linear unit layer.\n",
    "        \n",
    "        Args:\n",
    "            Z (Tensor): An n by b matrix representing the input pre-activations\n",
    "                of the layer for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: An n by b matrix representing the output of the layer after\n",
    "                using the rectified linear activation on all inputs for a batch\n",
    "                of size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.A = tf.maximum(tf.constant(0.0), Z)\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "# Add this method to the ReLU layer class\n",
    "ReLU.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method computes the gradient of the loss with respect to the inputs to the activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (Tensor): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return tf.scalar_mul(tf.sign(self.A), dLdA)\n",
    "    \n",
    "# Add this method to the ReLU layer class\n",
    "ReLU.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has no weights to update. Therefore, no `sgd_step` function is required.\n",
    "\n",
    "### Softmax Activation Layer\n",
    "\n",
    "This layer encapsulates the softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Layer):\n",
    "    \"\"\"Softmax activation layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method take a preactivation from the previous layer and computes the activation using the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the softmax activation given the inputs from the previous\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            Z (Tensor): An n by b matrix representing the inputs to the softmax\n",
    "                layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: An n by b matrix of outputs from softmax for a batch of \n",
    "                size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = tf.divide(tf.exp(Z), tf.reduce_sum(tf.exp(Z), axis=0, keepdims=True))\n",
    "        \n",
    "        return self.A\n",
    "\n",
    "# Add this method to the SoftMax layer class\n",
    "SoftMax.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method computes the gradient of the loss with respect to the inputs to the activation layer. Note that I *do not* assume that $\\partial \\mathrm{Loss} / \\partial Z^L$ is passed in directly. More information on how this works can be found in the 'Einstein Summation' notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (Tensor): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "                \n",
    "        \"\"\"\n",
    "        n, _ = dLdA.shape\n",
    "        \n",
    "        dAdZ = tf.add(tf.einsum('jk,jk,ji->ijk', self.A, tf.subtract(tf.constant(1), self.A), tf.eye(n)),\n",
    "                      tf.einsum('jk,ik,ji->ijk', tf.negative(self.A), self.A, tf.subtract(tf.constant(1), tf.eye(n))))\n",
    "        \n",
    "        return tf.einsum('ikj,kj->ij', dAdZ, dLdA)\n",
    "\n",
    "# Add this method to the SoftMax layer class\n",
    "SoftMax.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make prediction a little easier, we also provide a method that will determine the classes of highest probability as returned from a softmax prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        \"\"\"Computes the index of maximum value given the softmax outputs from a\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            Ypred (Tensor): An n by b matrix representing the softmax outputs of a\n",
    "                layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: A 1 by b row vectors representing the indices of maximum value\n",
    "                for each output from a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return tf.argmax(Ypred, axis=0)\n",
    "\n",
    "# Add this method to the SoftMax layer class\n",
    "SoftMax.class_fun = class_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has no weights to update. Therefore, no `sgd_step` function is required.\n",
    "\n",
    "### Negative Log-Likelihood Multi-Class Loss Layer\n",
    "\n",
    "This isn't really a layer, but it functions quite similarly to one. It will take predictions and actual labels and compute the categorical cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLL(Layer):\n",
    "    \"\"\"Negative log-likelihood loss layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method will compute the loss between predicted and actual labels using categorical cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Ypred, Y):\n",
    "        \"\"\"Computes the loss given the predicted and actual results.\n",
    "\n",
    "        Args:\n",
    "            Ypred (Tensor): An n by b matrix representing the predicted results\n",
    "                from the network for a batch of size b.\n",
    "            Y (Tensor): An n by b matrix representing the actual expected results\n",
    "                for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: A scalar representing the total loss for each of the outputs\n",
    "                in a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "\n",
    "        return tf.negative(tf.reduce_sum(tf.multiply(self.Y, tf.log(self.Ypred))))\n",
    "\n",
    "# Add this method to the NLL layer class\n",
    "NLL.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method will compute the gradient of the loss with respect to the predicted outputs from the network. (Note: this is *not* in terms of the pre-activations, but the actual activations. To learn more about this, look at the 'Einstein Summation' notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self):\n",
    "        \"\"\"Computes the gradient of the loss with respect to predicted targets for\n",
    "        a single batch.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: An n by b matrix representing the gradient of loss with\n",
    "                respect to predicted targets for a batch of size b.\n",
    "                \n",
    "        \"\"\"\n",
    "        return tf.negative(tf.divide(self.Y, self.Ypred))\n",
    "\n",
    "# Add this method to the NLL layer class\n",
    "NLL.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has no weights to update. Therefore, no `sgd_step` function is required.\n",
    "\n",
    "## Model\n",
    "\n",
    "Now that we have all the components to make up a simple neural network, we can combine them together into a model.\n",
    "\n",
    "### Sequential Model\n",
    "\n",
    "This is the simplest type of model which just linearly stacks each layer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"A standard neural network model with linear stacked layers.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do anything with the model, we need to know what layers should be included and what loss should be used to compute gradient updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, input, layers, loss):\n",
    "        \"\"\"Initialize the layers and the loss for the network.\n",
    "        \n",
    "        Args:\n",
    "            input (Layer): An input placeholder layer to begin the network.\n",
    "            layers (list of Layers): A list of layers to make up the linear\n",
    "                neural network.\n",
    "            loss (Layer): A final layer to use to compute the loss of the\n",
    "                neural network.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "\n",
    "# Add this method to the Sequential model class\n",
    "Sequential.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions with the network, we use the `forward` method. This passes the data through every layer and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def compile_forward(self):\n",
    "        \"\"\"Connects the layers to compute the output for a \n",
    "        training input batch.\n",
    "        \n",
    "        \"\"\"\n",
    "        Xt = self.input.forward()\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            Xt = layer.forward(Xt)\n",
    "            \n",
    "        self.forward = Xt\n",
    "\n",
    "# Add this method to the Sequential model class\n",
    "Sequential.compile_forward = compile_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we will use stochastic gradient descent. Before we define the stochastic gradient descent training loop, we have to back-propogate the error throughout the layers of the network. To do this, we use the `backward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def compile_backward(self, dLdA):\n",
    "        \"\"\"Computes the gradients of the loss with respect to each weight\n",
    "        in the neural network to prepare for stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the\n",
    "                loss with respect to the outputs of the neural network for a\n",
    "                batch of size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers[::-1]:\n",
    "            dLdA = layer.backward(dLdA)\n",
    "        \n",
    "        dLdA = self.input.backward(dLdA)\n",
    "\n",
    "# Add this method to the Sequential model class\n",
    "Sequential.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the error is propogated through all the layers, each layer can update their weight matrices. For a single step, this is achieved through the `sgd_step` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, lrate):\n",
    "        \"\"\"Runs a single update step on the weight matrices throughout the\n",
    "        neural network using stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            lrate (float): Learning rate for the update step.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.sgd_step(lrate)\n",
    "\n",
    "# Add this method to the Sequential model class\n",
    "Sequential.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop over the data applying many stochastic gradient descent update steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd(self, X, Y, iters=100, lrate=0.005):\n",
    "        \"\"\"Trains the neural network by running stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): A d by n matrix representing n training data points\n",
    "                each with d dimensions.\n",
    "            Y (ndarray): A 1 by n matrix representing n training labels.\n",
    "            iters (int): The number of iterations to run stochastic graident\n",
    "                descent.\n",
    "            lrate (float): The step size for stochastic gradient descent.\n",
    "        \n",
    "        \"\"\"\n",
    "        _, n = X.shape\n",
    "        \n",
    "        for it in range(iters):\n",
    "            \n",
    "            t = np.random.randint(n)\n",
    "            \n",
    "            Xt = X[:, t:t + 1]\n",
    "            Yt = Y[:, t:t + 1]\n",
    "            \n",
    "            loss = self.loss.forward(self.forward(Xt), Yt)\n",
    "            self.backward(self.loss.backward())      \n",
    "            \n",
    "            self.print_accuracy(it, X, Y, loss)\n",
    "            \n",
    "            self.sgd_step(lrate)\n",
    "            \n",
    "# Add this method to the Sequential model class\n",
    "Sequential.sgd = sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
