{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Feed-Forward Neural Network\n",
    "\n",
    "This notebook walks through how to build a feed-forward neural network using TensorFlow. Nearly everything is the same as the NumPy implementation, except a static graph is constructed using TensorFlow. To see the process in more detail (without level-by-level encapsulation), take a look at the TensorFlow Feed-Forward Neural Network Detailed notebook.\n",
    "\n",
    "## Layers\n",
    "\n",
    "Instead of unrolling the network as shown in the detailed notebook, we encapsulate the common, repeated behavior into layer classes.\n",
    "\n",
    "### Base Layer\n",
    "\n",
    "This layer provides the virtual methods that each layer has to implement. If the layer doesn't implement the method, we default to one of these empty methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Abstract base layer for our neural network.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__init__` method sets up constant information about the layer that is necessary to build the graph later. Typically, only dimensions of input/output data is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self):\n",
    "        \"\"\"Initializes layer constants necessary to construct the graph\n",
    "            for training. Likely: just dimension information or nothing\n",
    "            at all.\"\"\"\n",
    "        \n",
    "Layer.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the information about the layer is obtained from `__init__`, we can construct any variables in the graph needed by the layer using the `build` method. In the NumPy version, this is accomplished within `__init__`. We split this operation into two because at the time of initialization, we do not have a graph defined. Without a graph, we cannot create variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build(self):\n",
    "        \"\"\"Adds any variables to the graph required by the layer. (Such\n",
    "            as weight matrices.)\"\"\"\n",
    "        \n",
    "Layer.build = build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the layers need to be connected to each other for the forward pass. This is accomplished through the `build_forward` method. Note that this is not the same as a `forward` method in the NumPy version as nothing is being computed yet. In the `build_forward` method, we are constructing operations within the graph to be executed at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_forward(self, X):\n",
    "        \"\"\"Connects the layer to the previous layer through a new \n",
    "            operation in the forward pass process.\n",
    "            \n",
    "        Args:\n",
    "            X (Tensor): A tensor representing the inputs to the layer.\n",
    "                Likely: A or Z depending on the layer.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A tensor representing the outputs of the layer. Likely:\n",
    "                Z or A depending on the layer.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "Layer.build_forward = build_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers also need to be connected to form the backward pass. We do this using the `build_backward` method. Again, this is not the same as the `backward` method as nothing is executed, only the operation is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_backward(self, dL):\n",
    "        \"\"\"Connects the layer to the next layer through a new operation\n",
    "            in the backward pass process.\n",
    "            \n",
    "        Args:\n",
    "            dL (Tensor): A tensor representing the gradient of the loss\n",
    "                of the network with respect to the outputs of the current\n",
    "                layer. Likely: dLdA or dLdZ depending on the layer.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A tensor representign the gradient of the loss of the\n",
    "                network will respect to the inputs of the current layer.\n",
    "                Likely: dLdZ or dLdA depending on the layer.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "Layer.build_backward = build_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some layers need to update variables when they go through the training process. These variables are updated in the `build_sgd_step` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_sgd_step(self, eta):\n",
    "        \"\"\"Updates trainable variables based off the results from the\n",
    "            backward pass.\n",
    "            \n",
    "        Args:\n",
    "            eta (float): The learning rate to use for the stochastic\n",
    "                gradient descent update step.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "Layer.build_sgd_step = build_sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "This is the simplest possible layer where all inputs are connected to all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"Simple layer fully-connecting inputs to outputs linearly.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize this network, we just need to know the input dimensions and the output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, m, n):\n",
    "        \"\"\"Initializes the dimensions of the layer.\n",
    "        \n",
    "        Args:\n",
    "            m (int): Number of input features to the layer.\n",
    "            n (int): Number of output features of the layer.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        \n",
    "Linear.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dimensions known, we can construct the variables on the graph. Here are a few notes to consider:\n",
    "\n",
    "- We add new variables to our graph using the `get_variable`. This the standard preferred method to introduce trainable parameters.\n",
    "- Since we will likely have many of these `Linear` layers, we need to scope our variables as duplicate variable names are not allowed. By using `default_name`, TensorFlow will ensure our variable scopes are unique.\n",
    "- The variables need to be initialized to some values when the graph is constructed. To tell TensorFlow what these should be initialized to later, we use `zeros_initializer` for $W_0$ and `random_normal_initializer` for $W$.\n",
    "- This method assumes that the graph we wish to add the variables to is the default graph. This is only important to consider when training multiple different graphs at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build(self):\n",
    "        \"\"\"Creates the trainable variables on the default graph for \n",
    "            the Linear layer.\"\"\"\n",
    "        with tf.variable_scope(name_or_scope=None, default_name='Linear'):\n",
    "            \n",
    "            self.W = tf.get_variable(name='W', shape=(self.m, self.n), initializer=tf.random_normal_initializer(0.0, tf.sqrt(1 / self.m)))\n",
    "            self.W0 = tf.get_variable(name='W0', shape=(self.n, 1), initializer=tf.zeros_initializer)\n",
    "\n",
    "Linear.build = build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our variables defined, we can build the forward pass. We take the activation $A$ from the previous layer and produce the current $Z$ pre-activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_forward(self, A):\n",
    "        \"\"\"Connects the linear layer to the previous layer by operating\n",
    "            on the previous activation.\n",
    "            \n",
    "        Args:\n",
    "            A (Tensor): An m by b tensor representing the activations from\n",
    "                the previous layer with a batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            Tensor: An n by b tensor, Z, representing the pre-activations\n",
    "                as the output from this linear layer.\n",
    "        \n",
    "        \"\"\"\n",
    "        # We need this input later when computing the backward path.\n",
    "        self.A = A\n",
    "        \n",
    "        return tf.transpose(self.W) @ self.A + self.W0\n",
    "    \n",
    "Linear.build_forward = build_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will compute the gradients by hand using back-propogation. We take the gradient of the loss with respect to the pre-activations of the layer $\\partial \\mathrm{Loss} / \\partial Z$ and compute the gradient of the loss with respect to the activations of the previous layer $\\partial \\mathrm{Loss} / \\partial A$. \n",
    "\n",
    "In addition, we save gradients of the loss with respect to the weights ($\\partial \\mathrm{Loss} / \\partial W$ and $\\partial \\mathrm{Loss} / \\partial W_0$) for the stochastic gradient descent update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_backward(self, dLdZ):\n",
    "        \"\"\"Connects the next layer to the current layer using backward \n",
    "            process. Also records gradients of the loss with respect to\n",
    "            weights for later stochastic gradient descent updates.\n",
    "        \n",
    "        Args:\n",
    "            dLdZ (Tensor): An n by b tensor representing the gradient of\n",
    "                the loss with respect to the current layer's \n",
    "                pre-activations for a batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            Tensor: An m by b tensor, dLdA, representing the gradient of \n",
    "                the loss with respect to the previous layer's activations.\n",
    "        \n",
    "        \"\"\"\n",
    "        # We store these gradients for use later in the sgd_step\n",
    "        self.dLdW = self.A @ tf.transpose(dLdZ)\n",
    "        self.dLdW0 = tf.reduce_sum(dLdZ, axis=1, keepdims=True)\n",
    "\n",
    "        return self.W @ dLdZ\n",
    "    \n",
    "Linear.build_backward = build_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear layer has some trainable parameters to update. Using the specified learning rate $\\eta$, we can re-assign our variable values using stochastic gradient descent. Note that these are not executed when this method is called!Since these are isolated operations, we need to return handles to them to be executed later. To execute in parallel, we use `tf.group`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_sgd_step(self, eta):\n",
    "        \"\"\"Constructs the training update operations for the Linear layer\n",
    "            weight parameters.\n",
    "            \n",
    "        Args:\n",
    "            eta (float): The learning rate to use for the stochastic\n",
    "                gradient descent update step.\n",
    "        \n",
    "        Returns:\n",
    "            Operation: An operation that executes the stochastic gradient\n",
    "                descent step for all weights.\n",
    "        \n",
    "        \"\"\"\n",
    "        return tf.group(self.W.assign_sub(eta * self.dLdW),\n",
    "                        self.W0.assign_sub(eta * self.dLdW0))\n",
    "\n",
    "Linear.build_sgd_step = build_sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit Activation Layer\n",
    "\n",
    "This layer applies the relu activation function to each of the inputs element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"Applies relu activation function to all inputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have no variables to consider, we just need to construct the forward and backward passes. With the forward pass we compute the activation $A$ using the previous layer's pre-activation $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_forward(self, Z):\n",
    "        \"\"\"Connects the previous later to the current layer using the\n",
    "            ReLU operation forward pass.\n",
    "            \n",
    "        Args:\n",
    "            Z (Tensor): An m by b tensor representing the pre-activations\n",
    "                from the previous layer for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: An n by b tensor, A, representing the activations from\n",
    "                the current layer for a batch of size b. (Note: n and m \n",
    "                are equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        # We need this activation when computing the backward step later\n",
    "        self.A = tf.maximum(0.0, Z)\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "ReLU.build_forward = build_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backward pass, we use the gradient of the loss with respect to the layer's activations $\\partial \\mathrm{Loss} / \\partial A$ to compute the gradient of the loss with respect to the previous layer's pre-activations $\\partial \\mathrm{Loss} / \\partial Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_backward(self, dLdA):\n",
    "        \"\"\"Connects the current layer to the next using the backward pass \n",
    "            process.\n",
    "            \n",
    "        Args:\n",
    "            dLdA (Tensor): An n by b tensor representing the gradient of\n",
    "                the loss with respect to the current layer's activations\n",
    "                for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: An m by b tensor, dLdZ, representing the gradient of\n",
    "                the loss with respect to the previous layer's activations\n",
    "                for a batch of size b. (Note: n and m are equal.)\n",
    "        \n",
    "        \"\"\"\n",
    "        return tf.sign(self.A) * dLdA\n",
    "    \n",
    "ReLU.build_backward = build_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "\n",
    "    def build_forward(self, Z):\n",
    "\n",
    "        self.A = tf.tanh(Z)\n",
    "        return self.A\n",
    "    \n",
    "    def build_backward(self, dLdA):\n",
    "\n",
    "        return (1.0 - self.A ** 2.0) * dLdA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "\n",
    "    def build_forward(self, Z):\n",
    "\n",
    "        self.A = tf.exp(Z) / tf.reduce_sum(tf.exp(Z), axis=0, keepdims=True)\n",
    "        return self.A\n",
    "    \n",
    "    def build_backward(self, dLdA):\n",
    "        \n",
    "        n = dLdA.shape[0].value\n",
    "        \n",
    "        return tf.einsum('ikj,kj->ij', tf.einsum('jk,jk,ji->ijk', self.A, 1.0 - self.A, tf.eye(n)) + tf.einsum('jk,ik,ji->ijk', -self.A, self.A, 1.0 - tf.eye(n)), dLdA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLM(Layer):\n",
    "    \n",
    "    def build_forward(self, A, Y):\n",
    "\n",
    "        self.A = A\n",
    "        self.Y = Y\n",
    "\n",
    "        return -tf.reduce_sum(self.Y * tf.log(self.A))\n",
    "    \n",
    "    def build_backward(self):\n",
    "\n",
    "        return -self.Y / self.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(Layer):\n",
    "    \n",
    "    def build_forward(self, A, Y):\n",
    "        \n",
    "        return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(A, axis=0), tf.argmax(Y, axis=0)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "\n",
    "        self.layers = layers\n",
    "        \n",
    "        # When we build layer, we need to know input and output dimensions.\n",
    "        self.m = self.layers[0].m\n",
    "        self.n = self.layers[0].n\n",
    "        for layer in self.layers[1:]:\n",
    "            self.n = getattr(layer, 'n', self.n)\n",
    "        \n",
    "        # This is the static graph representing our model\n",
    "        self.graph = tf.Graph()\n",
    "        \n",
    "        # This is the runtime instance of our model\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "\n",
    "    def build(self, eta, loss, metrics):\n",
    "\n",
    "        # This ensures that the graph we add our variables to is the graph\n",
    "        # unique to the model.\n",
    "        with self.graph.as_default():\n",
    "        \n",
    "            self.X = tf.placeholder(name='X', shape=(self.m, None), dtype=tf.float32)\n",
    "            self.Y = tf.placeholder(name='Y', shape=(self.n, None), dtype=tf.float32)\n",
    "\n",
    "            for layer in self.layers + [loss] + metrics:\n",
    "                layer.build()\n",
    "\n",
    "            self.forward = self.build_forward(self.X)\n",
    "            self.loss_forward = loss.build_forward(self.forward, self.Y)\n",
    "            self.metrics_forward = tf.tuple([metric.build_forward(self.forward, self.Y) for metric in metrics])\n",
    "\n",
    "            loss_backward = loss.build_backward()\n",
    "            self.build_backward(loss_backward)\n",
    "\n",
    "            self.build_sgd_step(eta)\n",
    "            \n",
    "            initializer = tf.global_variables_initializer()\n",
    "        \n",
    "        # This initializes the variables in our graph using the current \n",
    "        # instance session\n",
    "        self.sess.run(initializer)\n",
    "        \n",
    "    def build_forward(self, X):\n",
    "        \n",
    "        self.forward = X\n",
    "        for layer in self.layers:\n",
    "            self.forward = layer.build_forward(self.forward)\n",
    "        \n",
    "        return self.forward\n",
    "\n",
    "    def build_backward(self, dL):\n",
    "        \n",
    "        for layer in self.layers[::-1]:\n",
    "            dL = layer.build_backward(dL)\n",
    "    \n",
    "    def build_sgd_step(self, eta):\n",
    "        \n",
    "        sgd_steps = [layer.build_sgd_step(eta) for layer in self.layers]\n",
    "        self.sgd_step = tf.group(*[sgd_step for sgd_step in sgd_steps if sgd_step is not None])\n",
    "    \n",
    "    def sgd(self, X_train, Y_train, epochs=100):\n",
    "\n",
    "        d, n = X_train.shape\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            t = np.random.randint(n)\n",
    "            \n",
    "            Xt = X_train[:, t:t + 1]\n",
    "            Yt = Y_train[:, t:t + 1]\n",
    "\n",
    "            self.sess.run(self.sgd_step, feed_dict={ self.X: Xt, self.Y: Yt })\n",
    "            \n",
    "            if epoch % 250 == 1:\n",
    "                \n",
    "                metrics_eval = self.sess.run(self.metrics_forward, feed_dict={ self.X: X_train, self.Y: Y_train })\n",
    "                print('Iteration =', epoch, '\\tAcc =', metrics_eval[1], '\\tLoss =', metrics_eval[0], flush=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        return self.sess.run(self.forward, feed_dict={ self.X: X })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Linear(2, 10), ReLU(),\n",
    "                    Linear(10, 10), ReLU(),\n",
    "                    Linear(10, 2), Softmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-0.23390341,  1.18151883, -2.46493986,  1.55322202,  1.27621763,\n",
    "                2.39710997, -1.34403040, -0.46903436, -0.64673502, -1.44029872,\n",
    "               -1.37537243,  1.05994811, -0.93311512,  1.02735575, -0.84138778,\n",
    "               -2.22585412, -0.42591102,  1.03561105,  0.91125595, -2.26550369],\n",
    "              [-0.92254932, -1.10309630, -2.41956036, -1.15509002, -1.04805327,\n",
    "                0.08717325,  0.81847250, -0.75171045,  0.60664705,  0.80410947,\n",
    "               -0.11600488,  1.03747218, -0.67210575,  0.99944446, -0.65559838,\n",
    "               -0.40744784, -0.58367642,  1.05972780, -0.95991874, -1.41720255]])\n",
    "\n",
    "Y = np.array([[0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
    "              [1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(0.005, NLLM(), [NLLM(), Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.3 \tLoss = 19.753824\n",
      "Iteration = 251 \tAcc = 0.9 \tLoss = 8.740222\n",
      "Iteration = 501 \tAcc = 0.9 \tLoss = 6.476474\n",
      "Iteration = 751 \tAcc = 0.9 \tLoss = 5.6205273\n",
      "Iteration = 1001 \tAcc = 0.9 \tLoss = 5.2962203\n",
      "Iteration = 1251 \tAcc = 0.9 \tLoss = 4.9513946\n",
      "Iteration = 1501 \tAcc = 0.9 \tLoss = 4.7240777\n",
      "Iteration = 1751 \tAcc = 0.9 \tLoss = 4.5196123\n",
      "Iteration = 2001 \tAcc = 0.9 \tLoss = 4.2559686\n",
      "Iteration = 2251 \tAcc = 0.9 \tLoss = 4.159255\n",
      "Iteration = 2501 \tAcc = 0.9 \tLoss = 4.164777\n",
      "Iteration = 2751 \tAcc = 0.95 \tLoss = 3.988433\n",
      "Iteration = 3001 \tAcc = 0.95 \tLoss = 3.9109304\n",
      "Iteration = 3251 \tAcc = 0.9 \tLoss = 3.8193903\n",
      "Iteration = 3501 \tAcc = 0.9 \tLoss = 3.7753713\n",
      "Iteration = 3751 \tAcc = 0.95 \tLoss = 3.7145877\n",
      "Iteration = 4001 \tAcc = 0.9 \tLoss = 3.71833\n",
      "Iteration = 4251 \tAcc = 0.95 \tLoss = 3.782719\n",
      "Iteration = 4501 \tAcc = 0.95 \tLoss = 3.6218638\n",
      "Iteration = 4751 \tAcc = 0.9 \tLoss = 3.6648734\n",
      "Iteration = 5001 \tAcc = 0.95 \tLoss = 3.6300411\n",
      "Iteration = 5251 \tAcc = 0.95 \tLoss = 3.5765283\n",
      "Iteration = 5501 \tAcc = 0.95 \tLoss = 3.564943\n",
      "Iteration = 5751 \tAcc = 0.95 \tLoss = 3.5781698\n",
      "Iteration = 6001 \tAcc = 0.9 \tLoss = 3.7000265\n",
      "Iteration = 6251 \tAcc = 0.95 \tLoss = 3.563818\n",
      "Iteration = 6501 \tAcc = 0.95 \tLoss = 3.531634\n",
      "Iteration = 6751 \tAcc = 0.9 \tLoss = 3.659658\n",
      "Iteration = 7001 \tAcc = 0.95 \tLoss = 3.5318067\n",
      "Iteration = 7251 \tAcc = 0.95 \tLoss = 3.5496643\n",
      "Iteration = 7501 \tAcc = 0.95 \tLoss = 3.5055969\n",
      "Iteration = 7751 \tAcc = 0.9 \tLoss = 3.9269288\n",
      "Iteration = 8001 \tAcc = 0.9 \tLoss = 3.7112188\n",
      "Iteration = 8251 \tAcc = 0.95 \tLoss = 3.649318\n",
      "Iteration = 8501 \tAcc = 0.85 \tLoss = 3.696322\n",
      "Iteration = 8751 \tAcc = 0.95 \tLoss = 3.5681615\n",
      "Iteration = 9001 \tAcc = 0.95 \tLoss = 3.504983\n",
      "Iteration = 9251 \tAcc = 0.95 \tLoss = 3.4761722\n",
      "Iteration = 9501 \tAcc = 0.95 \tLoss = 3.4978676\n",
      "Iteration = 9751 \tAcc = 0.95 \tLoss = 3.4838223\n",
      "Iteration = 10001 \tAcc = 0.95 \tLoss = 3.4649286\n",
      "Iteration = 10251 \tAcc = 0.95 \tLoss = 3.5172064\n",
      "Iteration = 10501 \tAcc = 0.95 \tLoss = 3.516128\n",
      "Iteration = 10751 \tAcc = 0.95 \tLoss = 3.4916987\n",
      "Iteration = 11001 \tAcc = 0.95 \tLoss = 3.5985212\n",
      "Iteration = 11251 \tAcc = 0.95 \tLoss = 3.4408846\n",
      "Iteration = 11501 \tAcc = 0.95 \tLoss = 3.489007\n",
      "Iteration = 11751 \tAcc = 0.95 \tLoss = 3.4515097\n",
      "Iteration = 12001 \tAcc = 0.95 \tLoss = 3.4671583\n",
      "Iteration = 12251 \tAcc = 0.9 \tLoss = 3.5322533\n",
      "Iteration = 12501 \tAcc = 0.95 \tLoss = 3.4305346\n",
      "Iteration = 12751 \tAcc = 0.95 \tLoss = 3.4065695\n",
      "Iteration = 13001 \tAcc = 0.85 \tLoss = 3.7779663\n",
      "Iteration = 13251 \tAcc = 0.95 \tLoss = 3.4005995\n",
      "Iteration = 13501 \tAcc = 0.95 \tLoss = 3.4653945\n",
      "Iteration = 13751 \tAcc = 0.95 \tLoss = 3.4003177\n",
      "Iteration = 14001 \tAcc = 0.95 \tLoss = 3.4419723\n",
      "Iteration = 14251 \tAcc = 0.95 \tLoss = 3.4003193\n",
      "Iteration = 14501 \tAcc = 0.95 \tLoss = 3.3776147\n",
      "Iteration = 14751 \tAcc = 0.95 \tLoss = 3.3596876\n",
      "Iteration = 15001 \tAcc = 0.95 \tLoss = 3.4303586\n",
      "Iteration = 15251 \tAcc = 0.95 \tLoss = 3.3446517\n",
      "Iteration = 15501 \tAcc = 0.95 \tLoss = 3.3665414\n",
      "Iteration = 15751 \tAcc = 0.95 \tLoss = 3.500869\n",
      "Iteration = 16001 \tAcc = 0.95 \tLoss = 3.321006\n",
      "Iteration = 16251 \tAcc = 0.95 \tLoss = 3.3241239\n",
      "Iteration = 16501 \tAcc = 0.95 \tLoss = 3.3017728\n",
      "Iteration = 16751 \tAcc = 0.95 \tLoss = 3.3102124\n",
      "Iteration = 17001 \tAcc = 0.9 \tLoss = 3.5355198\n",
      "Iteration = 17251 \tAcc = 0.95 \tLoss = 3.3520613\n",
      "Iteration = 17501 \tAcc = 0.95 \tLoss = 3.2795248\n",
      "Iteration = 17751 \tAcc = 0.9 \tLoss = 3.5776663\n",
      "Iteration = 18001 \tAcc = 0.95 \tLoss = 3.2601087\n",
      "Iteration = 18251 \tAcc = 0.95 \tLoss = 3.2883983\n",
      "Iteration = 18501 \tAcc = 0.95 \tLoss = 3.2183347\n",
      "Iteration = 18751 \tAcc = 0.95 \tLoss = 3.222659\n",
      "Iteration = 19001 \tAcc = 0.95 \tLoss = 3.2009332\n",
      "Iteration = 19251 \tAcc = 0.95 \tLoss = 3.2534924\n",
      "Iteration = 19501 \tAcc = 0.95 \tLoss = 3.192686\n",
      "Iteration = 19751 \tAcc = 0.95 \tLoss = 3.173245\n",
      "Iteration = 20001 \tAcc = 0.95 \tLoss = 3.1594007\n",
      "Iteration = 20251 \tAcc = 0.95 \tLoss = 3.1452568\n",
      "Iteration = 20501 \tAcc = 0.95 \tLoss = 3.1353917\n",
      "Iteration = 20751 \tAcc = 0.95 \tLoss = 3.1254354\n",
      "Iteration = 21001 \tAcc = 0.95 \tLoss = 3.1431735\n",
      "Iteration = 21251 \tAcc = 0.95 \tLoss = 3.1725504\n",
      "Iteration = 21501 \tAcc = 0.95 \tLoss = 3.0865343\n",
      "Iteration = 21751 \tAcc = 0.95 \tLoss = 3.055979\n",
      "Iteration = 22001 \tAcc = 0.95 \tLoss = 3.1130154\n",
      "Iteration = 22251 \tAcc = 0.95 \tLoss = 3.2540767\n",
      "Iteration = 22501 \tAcc = 0.95 \tLoss = 3.0620627\n",
      "Iteration = 22751 \tAcc = 0.95 \tLoss = 3.0308204\n",
      "Iteration = 23001 \tAcc = 0.95 \tLoss = 2.9827266\n",
      "Iteration = 23251 \tAcc = 0.95 \tLoss = 2.972024\n",
      "Iteration = 23501 \tAcc = 0.95 \tLoss = 2.949747\n",
      "Iteration = 23751 \tAcc = 0.95 \tLoss = 2.9222205\n",
      "Iteration = 24001 \tAcc = 0.95 \tLoss = 2.955141\n",
      "Iteration = 24251 \tAcc = 0.95 \tLoss = 2.8886786\n",
      "Iteration = 24501 \tAcc = 0.95 \tLoss = 2.9144292\n",
      "Iteration = 24751 \tAcc = 0.95 \tLoss = 2.8743825\n",
      "Iteration = 25001 \tAcc = 0.95 \tLoss = 2.917932\n",
      "Iteration = 25251 \tAcc = 0.95 \tLoss = 2.8306832\n",
      "Iteration = 25501 \tAcc = 0.95 \tLoss = 2.8507469\n",
      "Iteration = 25751 \tAcc = 0.95 \tLoss = 2.8136415\n",
      "Iteration = 26001 \tAcc = 0.95 \tLoss = 2.7473316\n",
      "Iteration = 26251 \tAcc = 0.95 \tLoss = 2.774147\n",
      "Iteration = 26501 \tAcc = 0.95 \tLoss = 2.7323833\n",
      "Iteration = 26751 \tAcc = 0.95 \tLoss = 2.7522624\n",
      "Iteration = 27001 \tAcc = 0.95 \tLoss = 2.7085838\n",
      "Iteration = 27251 \tAcc = 0.95 \tLoss = 2.6925235\n",
      "Iteration = 27501 \tAcc = 0.95 \tLoss = 2.677943\n",
      "Iteration = 27751 \tAcc = 0.95 \tLoss = 2.6578078\n",
      "Iteration = 28001 \tAcc = 0.95 \tLoss = 2.6188056\n",
      "Iteration = 28251 \tAcc = 0.95 \tLoss = 2.5951893\n",
      "Iteration = 28501 \tAcc = 0.95 \tLoss = 2.5697365\n",
      "Iteration = 28751 \tAcc = 0.95 \tLoss = 2.663862\n",
      "Iteration = 29001 \tAcc = 0.95 \tLoss = 2.838717\n",
      "Iteration = 29251 \tAcc = 0.95 \tLoss = 2.568037\n",
      "Iteration = 29501 \tAcc = 0.95 \tLoss = 2.4752235\n",
      "Iteration = 29751 \tAcc = 0.95 \tLoss = 2.4402905\n",
      "Iteration = 30001 \tAcc = 0.95 \tLoss = 2.5337348\n",
      "Iteration = 30251 \tAcc = 0.95 \tLoss = 2.4443614\n",
      "Iteration = 30501 \tAcc = 0.95 \tLoss = 2.6446362\n",
      "Iteration = 30751 \tAcc = 0.95 \tLoss = 2.436663\n",
      "Iteration = 31001 \tAcc = 0.95 \tLoss = 2.3662577\n",
      "Iteration = 31251 \tAcc = 0.95 \tLoss = 2.373498\n",
      "Iteration = 31501 \tAcc = 0.95 \tLoss = 2.3262641\n",
      "Iteration = 31751 \tAcc = 0.95 \tLoss = 2.3103073\n",
      "Iteration = 32001 \tAcc = 0.95 \tLoss = 2.2455466\n",
      "Iteration = 32251 \tAcc = 0.95 \tLoss = 2.2168531\n",
      "Iteration = 32501 \tAcc = 0.95 \tLoss = 2.199582\n",
      "Iteration = 32751 \tAcc = 0.95 \tLoss = 2.1928382\n",
      "Iteration = 33001 \tAcc = 0.95 \tLoss = 2.138625\n",
      "Iteration = 33251 \tAcc = 0.95 \tLoss = 2.183956\n",
      "Iteration = 33501 \tAcc = 0.95 \tLoss = 2.0755541\n",
      "Iteration = 33751 \tAcc = 0.95 \tLoss = 2.068082\n",
      "Iteration = 34001 \tAcc = 0.95 \tLoss = 2.1795084\n",
      "Iteration = 34251 \tAcc = 0.95 \tLoss = 2.022112\n",
      "Iteration = 34501 \tAcc = 0.95 \tLoss = 1.9863963\n",
      "Iteration = 34751 \tAcc = 0.95 \tLoss = 1.9735817\n",
      "Iteration = 35001 \tAcc = 0.95 \tLoss = 2.0149884\n",
      "Iteration = 35251 \tAcc = 0.95 \tLoss = 1.9990361\n",
      "Iteration = 35501 \tAcc = 0.95 \tLoss = 1.98191\n",
      "Iteration = 35751 \tAcc = 0.95 \tLoss = 1.9252844\n",
      "Iteration = 36001 \tAcc = 0.95 \tLoss = 1.8850955\n",
      "Iteration = 36251 \tAcc = 0.95 \tLoss = 1.8451917\n",
      "Iteration = 36501 \tAcc = 0.95 \tLoss = 1.8333296\n",
      "Iteration = 36751 \tAcc = 0.95 \tLoss = 1.8551074\n",
      "Iteration = 37001 \tAcc = 0.95 \tLoss = 1.8277986\n",
      "Iteration = 37251 \tAcc = 0.95 \tLoss = 1.7663692\n",
      "Iteration = 37501 \tAcc = 0.95 \tLoss = 1.7464144\n",
      "Iteration = 37751 \tAcc = 0.95 \tLoss = 1.8836029\n",
      "Iteration = 38001 \tAcc = 0.95 \tLoss = 1.7094654\n",
      "Iteration = 38251 \tAcc = 0.95 \tLoss = 1.7572713\n",
      "Iteration = 38501 \tAcc = 0.95 \tLoss = 1.7414479\n",
      "Iteration = 38751 \tAcc = 1.0 \tLoss = 1.7421523\n",
      "Iteration = 39001 \tAcc = 0.95 \tLoss = 1.6723235\n",
      "Iteration = 39251 \tAcc = 1.0 \tLoss = 1.6657323\n",
      "Iteration = 39501 \tAcc = 0.95 \tLoss = 1.6413889\n",
      "Iteration = 39751 \tAcc = 0.95 \tLoss = 1.5992271\n",
      "Iteration = 40001 \tAcc = 0.95 \tLoss = 1.6404605\n",
      "Iteration = 40251 \tAcc = 1.0 \tLoss = 1.5991805\n",
      "Iteration = 40501 \tAcc = 0.95 \tLoss = 1.5514314\n",
      "Iteration = 40751 \tAcc = 0.95 \tLoss = 1.6409171\n",
      "Iteration = 41001 \tAcc = 1.0 \tLoss = 1.574094\n",
      "Iteration = 41251 \tAcc = 1.0 \tLoss = 1.5171314\n",
      "Iteration = 41501 \tAcc = 0.95 \tLoss = 1.5259976\n",
      "Iteration = 41751 \tAcc = 0.95 \tLoss = 1.6885283\n",
      "Iteration = 42001 \tAcc = 0.95 \tLoss = 1.473858\n",
      "Iteration = 42251 \tAcc = 0.95 \tLoss = 1.5656611\n",
      "Iteration = 42501 \tAcc = 0.95 \tLoss = 1.5490772\n",
      "Iteration = 42751 \tAcc = 0.95 \tLoss = 1.4479103\n",
      "Iteration = 43001 \tAcc = 0.95 \tLoss = 1.4790745\n",
      "Iteration = 43251 \tAcc = 0.95 \tLoss = 1.4688383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 43501 \tAcc = 0.95 \tLoss = 1.478596\n",
      "Iteration = 43751 \tAcc = 0.95 \tLoss = 1.4386342\n",
      "Iteration = 44001 \tAcc = 1.0 \tLoss = 1.4074268\n",
      "Iteration = 44251 \tAcc = 1.0 \tLoss = 1.3774326\n",
      "Iteration = 44501 \tAcc = 1.0 \tLoss = 1.3782749\n",
      "Iteration = 44751 \tAcc = 1.0 \tLoss = 1.351286\n",
      "Iteration = 45001 \tAcc = 0.95 \tLoss = 1.379332\n",
      "Iteration = 45251 \tAcc = 0.95 \tLoss = 1.4688029\n",
      "Iteration = 45501 \tAcc = 1.0 \tLoss = 1.3227298\n",
      "Iteration = 45751 \tAcc = 1.0 \tLoss = 1.3397712\n",
      "Iteration = 46001 \tAcc = 0.95 \tLoss = 1.3824768\n",
      "Iteration = 46251 \tAcc = 0.95 \tLoss = 1.3363438\n",
      "Iteration = 46501 \tAcc = 1.0 \tLoss = 1.2874571\n",
      "Iteration = 46751 \tAcc = 1.0 \tLoss = 1.2826799\n",
      "Iteration = 47001 \tAcc = 0.95 \tLoss = 1.2901405\n",
      "Iteration = 47251 \tAcc = 0.95 \tLoss = 1.2910634\n",
      "Iteration = 47501 \tAcc = 0.95 \tLoss = 1.4181383\n",
      "Iteration = 47751 \tAcc = 1.0 \tLoss = 1.2533374\n",
      "Iteration = 48001 \tAcc = 1.0 \tLoss = 1.2415631\n",
      "Iteration = 48251 \tAcc = 1.0 \tLoss = 1.2417206\n",
      "Iteration = 48501 \tAcc = 1.0 \tLoss = 1.2220026\n",
      "Iteration = 48751 \tAcc = 0.95 \tLoss = 1.2442895\n",
      "Iteration = 49001 \tAcc = 1.0 \tLoss = 1.2087213\n",
      "Iteration = 49251 \tAcc = 0.95 \tLoss = 1.2393584\n",
      "Iteration = 49501 \tAcc = 1.0 \tLoss = 1.2031568\n",
      "Iteration = 49751 \tAcc = 0.95 \tLoss = 1.2111675\n",
      "Iteration = 50001 \tAcc = 1.0 \tLoss = 1.1832942\n",
      "Iteration = 50251 \tAcc = 1.0 \tLoss = 1.1694441\n",
      "Iteration = 50501 \tAcc = 1.0 \tLoss = 1.1668886\n",
      "Iteration = 50751 \tAcc = 1.0 \tLoss = 1.1699998\n",
      "Iteration = 51001 \tAcc = 0.95 \tLoss = 1.2631238\n",
      "Iteration = 51251 \tAcc = 1.0 \tLoss = 1.1464486\n",
      "Iteration = 51501 \tAcc = 1.0 \tLoss = 1.1431161\n",
      "Iteration = 51751 \tAcc = 0.95 \tLoss = 1.1666741\n",
      "Iteration = 52001 \tAcc = 0.95 \tLoss = 1.1591392\n",
      "Iteration = 52251 \tAcc = 0.95 \tLoss = 1.1799952\n",
      "Iteration = 52501 \tAcc = 1.0 \tLoss = 1.1165857\n",
      "Iteration = 52751 \tAcc = 0.95 \tLoss = 1.1857584\n",
      "Iteration = 53001 \tAcc = 0.95 \tLoss = 1.2322192\n",
      "Iteration = 53251 \tAcc = 1.0 \tLoss = 1.0958134\n",
      "Iteration = 53501 \tAcc = 1.0 \tLoss = 1.112365\n",
      "Iteration = 53751 \tAcc = 1.0 \tLoss = 1.0778201\n",
      "Iteration = 54001 \tAcc = 1.0 \tLoss = 1.07253\n",
      "Iteration = 54251 \tAcc = 0.95 \tLoss = 1.1502203\n",
      "Iteration = 54501 \tAcc = 1.0 \tLoss = 1.0608448\n",
      "Iteration = 54751 \tAcc = 1.0 \tLoss = 1.0841928\n",
      "Iteration = 55001 \tAcc = 0.95 \tLoss = 1.3118843\n",
      "Iteration = 55251 \tAcc = 1.0 \tLoss = 1.0425539\n",
      "Iteration = 55501 \tAcc = 1.0 \tLoss = 1.037885\n",
      "Iteration = 55751 \tAcc = 0.95 \tLoss = 1.0848715\n",
      "Iteration = 56001 \tAcc = 1.0 \tLoss = 1.0479652\n",
      "Iteration = 56251 \tAcc = 0.95 \tLoss = 1.0778937\n",
      "Iteration = 56501 \tAcc = 1.0 \tLoss = 1.0392658\n",
      "Iteration = 56751 \tAcc = 1.0 \tLoss = 1.0299673\n",
      "Iteration = 57001 \tAcc = 1.0 \tLoss = 1.0477828\n",
      "Iteration = 57251 \tAcc = 1.0 \tLoss = 1.0058844\n",
      "Iteration = 57501 \tAcc = 1.0 \tLoss = 1.0003091\n",
      "Iteration = 57751 \tAcc = 1.0 \tLoss = 1.0116724\n",
      "Iteration = 58001 \tAcc = 1.0 \tLoss = 1.0169533\n",
      "Iteration = 58251 \tAcc = 1.0 \tLoss = 1.002304\n",
      "Iteration = 58501 \tAcc = 1.0 \tLoss = 0.97397333\n",
      "Iteration = 58751 \tAcc = 1.0 \tLoss = 0.96891403\n",
      "Iteration = 59001 \tAcc = 1.0 \tLoss = 1.0067683\n",
      "Iteration = 59251 \tAcc = 1.0 \tLoss = 1.027388\n",
      "Iteration = 59501 \tAcc = 1.0 \tLoss = 1.0162995\n",
      "Iteration = 59751 \tAcc = 0.95 \tLoss = 1.2082378\n",
      "Iteration = 60001 \tAcc = 1.0 \tLoss = 0.95804954\n",
      "Iteration = 60251 \tAcc = 1.0 \tLoss = 0.9545874\n",
      "Iteration = 60501 \tAcc = 1.0 \tLoss = 0.94056755\n",
      "Iteration = 60751 \tAcc = 1.0 \tLoss = 0.96350646\n",
      "Iteration = 61001 \tAcc = 1.0 \tLoss = 0.9335504\n",
      "Iteration = 61251 \tAcc = 1.0 \tLoss = 0.98547983\n",
      "Iteration = 61501 \tAcc = 1.0 \tLoss = 0.9282174\n",
      "Iteration = 61751 \tAcc = 0.95 \tLoss = 1.0993084\n",
      "Iteration = 62001 \tAcc = 0.95 \tLoss = 1.1681659\n",
      "Iteration = 62251 \tAcc = 1.0 \tLoss = 0.91353756\n",
      "Iteration = 62501 \tAcc = 1.0 \tLoss = 0.93370056\n",
      "Iteration = 62751 \tAcc = 1.0 \tLoss = 0.9024089\n",
      "Iteration = 63001 \tAcc = 1.0 \tLoss = 0.8997741\n",
      "Iteration = 63251 \tAcc = 1.0 \tLoss = 0.90059465\n",
      "Iteration = 63501 \tAcc = 1.0 \tLoss = 0.9127924\n",
      "Iteration = 63751 \tAcc = 0.95 \tLoss = 1.0400578\n",
      "Iteration = 64001 \tAcc = 0.95 \tLoss = 1.0339586\n",
      "Iteration = 64251 \tAcc = 0.95 \tLoss = 1.1208582\n",
      "Iteration = 64501 \tAcc = 1.0 \tLoss = 0.9424561\n",
      "Iteration = 64751 \tAcc = 1.0 \tLoss = 0.87747014\n",
      "Iteration = 65001 \tAcc = 1.0 \tLoss = 0.91880864\n",
      "Iteration = 65251 \tAcc = 1.0 \tLoss = 0.857841\n",
      "Iteration = 65501 \tAcc = 1.0 \tLoss = 0.8597543\n",
      "Iteration = 65751 \tAcc = 1.0 \tLoss = 0.9166068\n",
      "Iteration = 66001 \tAcc = 0.95 \tLoss = 1.0408645\n",
      "Iteration = 66251 \tAcc = 1.0 \tLoss = 0.87093306\n",
      "Iteration = 66501 \tAcc = 1.0 \tLoss = 0.8348914\n",
      "Iteration = 66751 \tAcc = 1.0 \tLoss = 0.84622097\n",
      "Iteration = 67001 \tAcc = 1.0 \tLoss = 0.8350859\n",
      "Iteration = 67251 \tAcc = 0.95 \tLoss = 0.99148536\n",
      "Iteration = 67501 \tAcc = 1.0 \tLoss = 0.8481168\n",
      "Iteration = 67751 \tAcc = 1.0 \tLoss = 0.8377971\n",
      "Iteration = 68001 \tAcc = 1.0 \tLoss = 0.82080895\n",
      "Iteration = 68251 \tAcc = 1.0 \tLoss = 0.80706936\n",
      "Iteration = 68501 \tAcc = 1.0 \tLoss = 0.79502475\n",
      "Iteration = 68751 \tAcc = 1.0 \tLoss = 0.81342643\n",
      "Iteration = 69001 \tAcc = 1.0 \tLoss = 0.90542436\n",
      "Iteration = 69251 \tAcc = 1.0 \tLoss = 0.87217414\n",
      "Iteration = 69501 \tAcc = 1.0 \tLoss = 0.8433955\n",
      "Iteration = 69751 \tAcc = 1.0 \tLoss = 0.7971892\n",
      "Iteration = 70001 \tAcc = 1.0 \tLoss = 0.8077024\n",
      "Iteration = 70251 \tAcc = 1.0 \tLoss = 0.8001263\n",
      "Iteration = 70501 \tAcc = 1.0 \tLoss = 0.88493574\n",
      "Iteration = 70751 \tAcc = 1.0 \tLoss = 0.78997165\n",
      "Iteration = 71001 \tAcc = 1.0 \tLoss = 0.86153907\n",
      "Iteration = 71251 \tAcc = 1.0 \tLoss = 0.819351\n",
      "Iteration = 71501 \tAcc = 0.95 \tLoss = 0.9079657\n",
      "Iteration = 71751 \tAcc = 1.0 \tLoss = 0.7359746\n",
      "Iteration = 72001 \tAcc = 1.0 \tLoss = 0.84877384\n",
      "Iteration = 72251 \tAcc = 1.0 \tLoss = 0.722287\n",
      "Iteration = 72501 \tAcc = 1.0 \tLoss = 0.7165202\n",
      "Iteration = 72751 \tAcc = 1.0 \tLoss = 0.72969866\n",
      "Iteration = 73001 \tAcc = 1.0 \tLoss = 0.70459163\n",
      "Iteration = 73251 \tAcc = 0.95 \tLoss = 0.88009703\n",
      "Iteration = 73501 \tAcc = 1.0 \tLoss = 0.7628598\n",
      "Iteration = 73751 \tAcc = 1.0 \tLoss = 0.71020013\n",
      "Iteration = 74001 \tAcc = 1.0 \tLoss = 0.6824668\n",
      "Iteration = 74251 \tAcc = 0.95 \tLoss = 0.8893254\n",
      "Iteration = 74501 \tAcc = 0.95 \tLoss = 0.9888811\n",
      "Iteration = 74751 \tAcc = 1.0 \tLoss = 0.73338765\n",
      "Iteration = 75001 \tAcc = 1.0 \tLoss = 0.6659071\n",
      "Iteration = 75251 \tAcc = 1.0 \tLoss = 0.6742867\n",
      "Iteration = 75501 \tAcc = 1.0 \tLoss = 0.7143304\n",
      "Iteration = 75751 \tAcc = 1.0 \tLoss = 0.7878758\n",
      "Iteration = 76001 \tAcc = 1.0 \tLoss = 0.6440545\n",
      "Iteration = 76251 \tAcc = 0.95 \tLoss = 0.9253702\n",
      "Iteration = 76501 \tAcc = 1.0 \tLoss = 0.7404253\n",
      "Iteration = 76751 \tAcc = 1.0 \tLoss = 0.63687414\n",
      "Iteration = 77001 \tAcc = 1.0 \tLoss = 0.62413543\n",
      "Iteration = 77251 \tAcc = 1.0 \tLoss = 0.7007789\n",
      "Iteration = 77501 \tAcc = 1.0 \tLoss = 0.6786876\n",
      "Iteration = 77751 \tAcc = 1.0 \tLoss = 0.6102446\n",
      "Iteration = 78001 \tAcc = 1.0 \tLoss = 0.6748771\n",
      "Iteration = 78251 \tAcc = 1.0 \tLoss = 0.62184703\n",
      "Iteration = 78501 \tAcc = 1.0 \tLoss = 0.6336063\n",
      "Iteration = 78751 \tAcc = 1.0 \tLoss = 0.6065054\n",
      "Iteration = 79001 \tAcc = 1.0 \tLoss = 0.58459336\n",
      "Iteration = 79251 \tAcc = 1.0 \tLoss = 0.5868007\n",
      "Iteration = 79501 \tAcc = 1.0 \tLoss = 0.75495064\n",
      "Iteration = 79751 \tAcc = 0.95 \tLoss = 0.96593225\n",
      "Iteration = 80001 \tAcc = 1.0 \tLoss = 0.5843039\n",
      "Iteration = 80251 \tAcc = 1.0 \tLoss = 0.57166743\n",
      "Iteration = 80501 \tAcc = 1.0 \tLoss = 0.5744131\n",
      "Iteration = 80751 \tAcc = 1.0 \tLoss = 0.6075543\n",
      "Iteration = 81001 \tAcc = 0.95 \tLoss = 0.9776384\n",
      "Iteration = 81251 \tAcc = 1.0 \tLoss = 0.5869656\n",
      "Iteration = 81501 \tAcc = 1.0 \tLoss = 0.53308666\n",
      "Iteration = 81751 \tAcc = 1.0 \tLoss = 0.70608664\n",
      "Iteration = 82001 \tAcc = 0.95 \tLoss = 0.9248451\n",
      "Iteration = 82251 \tAcc = 1.0 \tLoss = 0.5373135\n",
      "Iteration = 82501 \tAcc = 1.0 \tLoss = 0.6849571\n",
      "Iteration = 82751 \tAcc = 1.0 \tLoss = 0.61033654\n",
      "Iteration = 83001 \tAcc = 1.0 \tLoss = 0.59808034\n",
      "Iteration = 83251 \tAcc = 1.0 \tLoss = 0.5292271\n",
      "Iteration = 83501 \tAcc = 0.95 \tLoss = 1.0220643\n",
      "Iteration = 83751 \tAcc = 1.0 \tLoss = 0.60104567\n",
      "Iteration = 84001 \tAcc = 1.0 \tLoss = 0.5704176\n",
      "Iteration = 84251 \tAcc = 1.0 \tLoss = 0.7086111\n",
      "Iteration = 84501 \tAcc = 1.0 \tLoss = 0.4807954\n",
      "Iteration = 84751 \tAcc = 1.0 \tLoss = 0.666298\n",
      "Iteration = 85001 \tAcc = 1.0 \tLoss = 0.4741709\n",
      "Iteration = 85251 \tAcc = 1.0 \tLoss = 0.49634874\n",
      "Iteration = 85501 \tAcc = 1.0 \tLoss = 0.51967084\n",
      "Iteration = 85751 \tAcc = 1.0 \tLoss = 0.4583696\n",
      "Iteration = 86001 \tAcc = 1.0 \tLoss = 0.5989152\n",
      "Iteration = 86251 \tAcc = 1.0 \tLoss = 0.50825506\n",
      "Iteration = 86501 \tAcc = 1.0 \tLoss = 0.46186858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 86751 \tAcc = 1.0 \tLoss = 0.4411411\n",
      "Iteration = 87001 \tAcc = 1.0 \tLoss = 0.441506\n",
      "Iteration = 87251 \tAcc = 1.0 \tLoss = 0.4428255\n",
      "Iteration = 87501 \tAcc = 1.0 \tLoss = 0.52358294\n",
      "Iteration = 87751 \tAcc = 1.0 \tLoss = 0.42652\n",
      "Iteration = 88001 \tAcc = 1.0 \tLoss = 0.5184572\n",
      "Iteration = 88251 \tAcc = 1.0 \tLoss = 0.4296863\n",
      "Iteration = 88501 \tAcc = 1.0 \tLoss = 0.42373165\n",
      "Iteration = 88751 \tAcc = 1.0 \tLoss = 0.5212043\n",
      "Iteration = 89001 \tAcc = 1.0 \tLoss = 0.4576082\n",
      "Iteration = 89251 \tAcc = 1.0 \tLoss = 0.71803844\n",
      "Iteration = 89501 \tAcc = 1.0 \tLoss = 0.40014848\n",
      "Iteration = 89751 \tAcc = 1.0 \tLoss = 0.40600574\n",
      "Iteration = 90001 \tAcc = 1.0 \tLoss = 0.46285495\n",
      "Iteration = 90251 \tAcc = 1.0 \tLoss = 0.7130199\n",
      "Iteration = 90501 \tAcc = 1.0 \tLoss = 0.42278603\n",
      "Iteration = 90751 \tAcc = 1.0 \tLoss = 0.3791124\n",
      "Iteration = 91001 \tAcc = 1.0 \tLoss = 0.6275976\n",
      "Iteration = 91251 \tAcc = 1.0 \tLoss = 0.3729471\n",
      "Iteration = 91501 \tAcc = 1.0 \tLoss = 0.3828577\n",
      "Iteration = 91751 \tAcc = 1.0 \tLoss = 0.57249695\n",
      "Iteration = 92001 \tAcc = 1.0 \tLoss = 0.3653578\n",
      "Iteration = 92251 \tAcc = 1.0 \tLoss = 0.37165385\n",
      "Iteration = 92501 \tAcc = 1.0 \tLoss = 0.46070075\n",
      "Iteration = 92751 \tAcc = 1.0 \tLoss = 0.3582308\n",
      "Iteration = 93001 \tAcc = 1.0 \tLoss = 0.47701195\n",
      "Iteration = 93251 \tAcc = 1.0 \tLoss = 0.3616014\n",
      "Iteration = 93501 \tAcc = 1.0 \tLoss = 0.33945417\n",
      "Iteration = 93751 \tAcc = 1.0 \tLoss = 0.3430683\n",
      "Iteration = 94001 \tAcc = 1.0 \tLoss = 0.44534764\n",
      "Iteration = 94251 \tAcc = 1.0 \tLoss = 0.42727154\n",
      "Iteration = 94501 \tAcc = 1.0 \tLoss = 0.4497513\n",
      "Iteration = 94751 \tAcc = 1.0 \tLoss = 0.3259553\n",
      "Iteration = 95001 \tAcc = 1.0 \tLoss = 0.3267895\n",
      "Iteration = 95251 \tAcc = 1.0 \tLoss = 0.32473224\n",
      "Iteration = 95501 \tAcc = 1.0 \tLoss = 0.38707656\n",
      "Iteration = 95751 \tAcc = 1.0 \tLoss = 0.3235756\n",
      "Iteration = 96001 \tAcc = 1.0 \tLoss = 0.31168863\n",
      "Iteration = 96251 \tAcc = 1.0 \tLoss = 0.45683855\n",
      "Iteration = 96501 \tAcc = 1.0 \tLoss = 0.31433845\n",
      "Iteration = 96751 \tAcc = 1.0 \tLoss = 0.31508276\n",
      "Iteration = 97001 \tAcc = 1.0 \tLoss = 0.36576694\n",
      "Iteration = 97251 \tAcc = 1.0 \tLoss = 0.29114506\n",
      "Iteration = 97501 \tAcc = 1.0 \tLoss = 0.33115983\n",
      "Iteration = 97751 \tAcc = 1.0 \tLoss = 0.28948405\n",
      "Iteration = 98001 \tAcc = 1.0 \tLoss = 0.2961708\n",
      "Iteration = 98251 \tAcc = 1.0 \tLoss = 0.31110734\n",
      "Iteration = 98501 \tAcc = 1.0 \tLoss = 0.29826385\n",
      "Iteration = 98751 \tAcc = 1.0 \tLoss = 0.34096596\n",
      "Iteration = 99001 \tAcc = 1.0 \tLoss = 0.30413786\n",
      "Iteration = 99251 \tAcc = 1.0 \tLoss = 0.32574734\n",
      "Iteration = 99501 \tAcc = 1.0 \tLoss = 0.32488546\n",
      "Iteration = 99751 \tAcc = 1.0 \tLoss = 0.2599631\n"
     ]
    }
   ],
   "source": [
    "model.sgd(X, Y, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a grid of points to classify\n",
    "xx1, xx2 = np.meshgrid(np.arange(-3, 3, 0.005), np.arange(-3, 3, 0.005))\n",
    "\n",
    "# Flatten the grid to pass into model\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()].T\n",
    "\n",
    "# Predict classification at every point on the grid\n",
    "Z = model.predict(grid)[1, :].reshape(xx1.shape)\n",
    "\n",
    "# Plot the prediction regions.\n",
    "plt.imshow(Z, interpolation='bicubic', origin='lower', extent=[-3, 3, -3, 3], \n",
    "           cmap=ListedColormap(['#1f77b4', '#ff7f0e']), alpha=0.55, aspect='auto')\n",
    "\n",
    "# Plot the original points.\n",
    "_ = plt.scatter(X[0,:], X[1,:], c=Y[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
