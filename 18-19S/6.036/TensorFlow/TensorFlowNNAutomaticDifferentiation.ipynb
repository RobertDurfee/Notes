{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow NN Automatic Differentiation\n",
    "\n",
    "This notebook walks through how to build a feed-forward neural network using TensorFlow. Instead of computing the gradients by hand, we use TensorFlow's automatic differentiation.\n",
    "\n",
    "## Layers\n",
    "\n",
    "Instead of unrolling the network as shown in the detailed notebook, we encapsulate the common, repeated behavior into layer classes.\n",
    "\n",
    "### Base Layer\n",
    "\n",
    "This layer provides the virtual methods that each layer has to implement. If the layer doesn't implement the method, we default to one of these empty methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Abstract base layer for our neural network.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__init__` method sets up constant information about the layer that is necessary to build the graph later. Typically, only dimensions of input/output data is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self):\n",
    "        \"\"\"Initializes layer constants necessary to construct the graph\n",
    "            for training. Likely: just dimension information or nothing\n",
    "            at all.\"\"\"\n",
    "        \n",
    "Layer.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the information about the layer is obtained from `__init__`, we can construct any variables in the graph needed by the layer using the `build` method. In the NumPy version, this is accomplished within `__init__`. We split this operation into two because at the time of initialization, we do not have a graph defined. Without a graph, we cannot create variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build(self):\n",
    "        \"\"\"Adds any variables to the graph required by the layer. (Such\n",
    "            as weight matrices.)\"\"\"\n",
    "        \n",
    "Layer.build = build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the layers need to be connected to each other for the forward pass. This is accomplished through the `build_forward` method. Note that this is not the same as a `forward` method in the NumPy version as nothing is being computed yet. In the `build_forward` method, we are constructing operations within the graph to be executed at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_forward(self, X):\n",
    "        \"\"\"Connects the layer to the previous layer through a new \n",
    "            operation in the forward pass process.\n",
    "            \n",
    "        Args:\n",
    "            X (Tensor): A tensor representing the inputs to the layer.\n",
    "                Likely: A or Z depending on the layer.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A tensor representing the outputs of the layer. Likely:\n",
    "                Z or A depending on the layer.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "Layer.build_forward = build_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers also need to be connected to form the backward pass. We do this using the `build_backward` method. Again, this is not the same as the `backward` method as nothing is executed, only the operation is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_backward(self, dL):\n",
    "        \"\"\"Connects the layer to the next layer through a new operation\n",
    "            in the backward pass process.\n",
    "            \n",
    "        Args:\n",
    "            dL (Tensor): A tensor representing the gradient of the loss\n",
    "                of the network with respect to the outputs of the current\n",
    "                layer. Likely: dLdA or dLdZ depending on the layer.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A tensor representing the gradient of the loss of the\n",
    "                network will respect to the inputs of the current layer.\n",
    "                Likely: dLdZ or dLdA depending on the layer.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "Layer.build_backward = build_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some layers need to update variables when they go through the training process. These variables are updated in the `build_sgd_step` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_sgd_step(self, eta):\n",
    "        \"\"\"Updates trainable variables based off the results from the\n",
    "            backward pass.\n",
    "            \n",
    "        Args:\n",
    "            eta (float): The learning rate to use for the stochastic\n",
    "                gradient descent update step.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "Layer.build_sgd_step = build_sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "This is the simplest possible layer where all inputs are connected to all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"Simple layer fully-connecting inputs to outputs linearly.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize this network, we just need to know the input dimensions and the output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, m, n):\n",
    "        \"\"\"Initializes the dimensions of the layer.\n",
    "        \n",
    "        Args:\n",
    "            m (int): Number of input features to the layer.\n",
    "            n (int): Number of output features of the layer.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        \n",
    "Linear.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dimensions known, we can construct the variables on the graph. Here are a few notes to consider:\n",
    "\n",
    "- We add new variables to our graph using the `get_variable`. This the standard preferred method to introduce trainable parameters.\n",
    "- Since we will likely have many of these `Linear` layers, we need to scope our variables as duplicate variable names are not allowed. By using `default_name`, TensorFlow will ensure our variable scopes are unique.\n",
    "- The variables need to be initialized to some values when the graph is constructed. To tell TensorFlow what these should be initialized to later, we use `zeros_initializer` for $W_0$ and `random_normal_initializer` for $W$.\n",
    "- This method assumes that the graph we wish to add the variables to is the default graph. This is only important to consider when training multiple different graphs at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build(self):\n",
    "        \"\"\"Creates the trainable variables on the default graph for \n",
    "            the Linear layer.\"\"\"\n",
    "        with tf.variable_scope(name_or_scope=None, default_name='Linear'):\n",
    "            \n",
    "            self.W = tf.get_variable(name='W', shape=(self.m, self.n), initializer=tf.random_normal_initializer(0.0, tf.sqrt(1 / self.m)))\n",
    "            self.W0 = tf.get_variable(name='W0', shape=(self.n, 1), initializer=tf.zeros_initializer)\n",
    "\n",
    "Linear.build = build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our variables defined, we can build the forward pass. We take the activation $A$ from the previous layer and produce the current $Z$ pre-activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_forward(self, A):\n",
    "        \"\"\"Connects the linear layer to the previous layer by operating\n",
    "            on the previous activation.\n",
    "            \n",
    "        Args:\n",
    "            A (Tensor): An m by b tensor representing the activations from\n",
    "                the previous layer with a batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            Tensor: An n by b tensor, Z, representing the pre-activations\n",
    "                as the output from this linear layer.\n",
    "        \n",
    "        \"\"\"\n",
    "        # We need these later when computing the backward path.\n",
    "        self.A = A\n",
    "        self.Z = tf.transpose(self.W) @ self.A + self.W0\n",
    "        \n",
    "        return self.Z\n",
    "    \n",
    "Linear.build_forward = build_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will compute the gradients by hand using back-propogation. We take the gradient of the loss with respect to the pre-activations of the layer $\\partial \\mathrm{Loss} / \\partial Z$ and compute the gradient of the loss with respect to the activations of the previous layer $\\partial \\mathrm{Loss} / \\partial A$. \n",
    "\n",
    "In addition, we save gradients of the loss with respect to the weights ($\\partial \\mathrm{Loss} / \\partial W$ and $\\partial \\mathrm{Loss} / \\partial W_0$) for the stochastic gradient descent update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_backward(self, dLdZ):\n",
    "        \"\"\"Connects the next layer to the current layer using backward \n",
    "            process. Also records gradients of the loss with respect to\n",
    "            weights for later stochastic gradient descent updates.\n",
    "        \n",
    "        Args:\n",
    "            dLdZ (Tensor): An n by b tensor representing the gradient of\n",
    "                the loss with respect to the current layer's \n",
    "                pre-activations for a batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            Tensor: An m by b tensor, dLdA, representing the gradient of \n",
    "                the loss with respect to the previous layer's activations.\n",
    "        \n",
    "        \"\"\"\n",
    "        # We store these gradients for use later in the sgd_step\n",
    "        self.dLdW, self.dLdW0 = tf.gradients(self.Z, [self.W, self.W0], dLdZ)\n",
    "\n",
    "        return tf.gradients(self.Z, self.A, dLdZ)\n",
    "    \n",
    "Linear.build_backward = build_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear layer has some trainable parameters to update. Using the specified learning rate $\\eta$, we can re-assign our variable values using stochastic gradient descent. Note that these are not executed when this method is called!Since these are isolated operations, we need to return handles to them to be executed later. To execute in parallel, we use `tf.group`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_sgd_step(self, eta):\n",
    "        \"\"\"Constructs the training update operations for the Linear layer\n",
    "            weight parameters.\n",
    "            \n",
    "        Args:\n",
    "            eta (float): The learning rate to use for the stochastic\n",
    "                gradient descent update step.\n",
    "        \n",
    "        Returns:\n",
    "            Operation: An operation that executes the stochastic gradient\n",
    "                descent step for all weights.\n",
    "        \n",
    "        \"\"\"\n",
    "        return tf.group(self.W.assign_sub(eta * self.dLdW),\n",
    "                        self.W0.assign_sub(eta * self.dLdW0))\n",
    "\n",
    "Linear.build_sgd_step = build_sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit Activation Layer\n",
    "\n",
    "This layer applies the relu activation function to each of the inputs element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"Applies relu activation function to all inputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have no variables to consider, we just need to construct the forward and backward passes. With the forward pass we compute the activation $A$ using the previous layer's pre-activation $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_forward(self, Z):\n",
    "        \"\"\"Connects the previous later to the current layer using the\n",
    "            ReLU operation forward pass.\n",
    "            \n",
    "        Args:\n",
    "            Z (Tensor): An m by b tensor representing the pre-activations\n",
    "                from the previous layer for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: An n by b tensor, A, representing the activations from\n",
    "                the current layer for a batch of size b. (Note: n and m \n",
    "                are equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        # We need these when computing the backward step later\n",
    "        self.Z = Z\n",
    "        self.A = tf.maximum(0.0, self.Z)\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "ReLU.build_forward = build_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backward pass, we use the gradient of the loss with respect to the layer's activations $\\partial \\mathrm{Loss} / \\partial A$ to compute the gradient of the loss with respect to the previous layer's pre-activations $\\partial \\mathrm{Loss} / \\partial Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_backward(self, dLdA):\n",
    "        \"\"\"Connects the current layer to the next using the backward pass \n",
    "            process.\n",
    "            \n",
    "        Args:\n",
    "            dLdA (Tensor): An n by b tensor representing the gradient of\n",
    "                the loss with respect to the current layer's activations\n",
    "                for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: An m by b tensor, dLdZ, representing the gradient of\n",
    "                the loss with respect to the previous layer's activations\n",
    "                for a batch of size b. (Note: n and m are equal.)\n",
    "        \n",
    "        \"\"\"\n",
    "        return tf.gradients(self.A, self.Z, dLdA)\n",
    "    \n",
    "ReLU.build_backward = build_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent Activation Layer\n",
    "\n",
    "This layer applies the hyperbolic tangent activation function to each input element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    \"\"\"Applies hyperbolic tangent activation function to all inputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no variables in this layer, thus we only need to worry about forward and backward passes. The forward pass takes the previous layer's pre-activation $Z$ and produces the activation $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_forward(self, Z):\n",
    "        \"\"\"Connects the previous layer to the current layer using the\n",
    "            backward pass process.\n",
    "        \n",
    "        Args:\n",
    "            Z (Tensor): An m by b tensor representing the previous layer's\n",
    "                pre-activation for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: An n by b tensor, A, representing the layer's \n",
    "                activation for a batch of size b. (Note: m and n are\n",
    "                equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        # We need these when computing the backward step later\n",
    "        self.Z = Z\n",
    "        self.A = tf.tanh(self.Z)\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "Tanh.build_forward = build_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward pass uses the gradient of the loss with respect to the layer's activation $\\partial \\mathrm{Loss} / \\partial A$ to compute the gradient of the loss with respect to the previous layer's pre-activation $\\partial \\mathrm{Loss} / \\partial Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_backward(self, dLdA):\n",
    "        \"\"\"Connects the next layer to the current layer using the\n",
    "            backward pass process.\n",
    "            \n",
    "        Args:\n",
    "            dLdA (Tensor): An n by b tensor representing the gradient of\n",
    "                the loss with respect to this layer's activation for a \n",
    "                batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            Tensor: An m by b tensor, dLdZ, representing the gradient of \n",
    "                the loss with respect to the previous layer's \n",
    "                pre-activation for a batch of size b. (Note: n and m are \n",
    "                equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        return tf.gradients(self.A, self.Z, dLdA)\n",
    "\n",
    "Tanh.build_backward = build_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Layer\n",
    "\n",
    "This layer applies the softmax activation function to the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    \"\"\"Applies the softmax activation function to layer inputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the other activation layers, there are no variables to consider. The forward pass takes the previous layer's pre-activations $Z$ and computes the current layer's activation $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_forward(self, Z):\n",
    "        \"\"\"Connects the previous layer to the current layer using the\n",
    "            forward pass process.\n",
    "            \n",
    "        Args:\n",
    "            Z (Tensor): An m by b tensor representing the previous layer's\n",
    "                pre-activation for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: An n by b tensor, A, representing the current layer's\n",
    "                activation for a batch of size b. (Note: m and n are\n",
    "                equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        # We need this activation when computing the backward step later\n",
    "        self.Z = Z\n",
    "        self.A = tf.exp(self.Z) / tf.reduce_sum(tf.exp(self.Z), axis=0, keepdims=True)\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "Softmax.build_forward = build_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backward pass, we use the gradient of the loss with respect to the current layer's activations $\\partial \\mathrm{Loss} / \\partial A$ to compute the gradient of the loss with respect to the previous layer's pre-activations $\\partial \\mathrm{Loss} / \\partial Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_backward(self, dLdA):\n",
    "        \"\"\"Connects the next layer to the current layer using the\n",
    "            backward pass process.\n",
    "            \n",
    "        Args:\n",
    "            dLdA (Tensor): An n by b tensor representing the gradient of\n",
    "                the loss with respect to the current layer's activation.\n",
    "                \n",
    "        Returns:\n",
    "            Tensor: An m by b tensor, dLdZ, representing the gradient of \n",
    "                the loss with respect to the previous layer's \n",
    "                pre-activation. (Note: n and m are equal.)\n",
    "            \n",
    "        \"\"\"\n",
    "        return tf.gradients(self.A, self.Z, dLdA)\n",
    "    \n",
    "Softmax.build_backward = build_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log-Likelihood Multi-Class Loss Layer\n",
    "\n",
    "This layer computes the loss of the output of the network compared with the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLM(Layer):\n",
    "    \"\"\"Computes the negative log-likelihood multi-class loss for neural\n",
    "        network outputs and expected outputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the activation layers, there are no variables to consider. The forward pass takes the neural network's final activations $A$ and the expected outputs\n",
    "$Y$ and computes the loss scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_forward(self, A, Y):\n",
    "        \"\"\"Connects the neural network to the loss layer using the forward \n",
    "            pass process.\n",
    "            \n",
    "        Args:\n",
    "            A (Tensor): An n by b tensor representing the neural network's\n",
    "                outputs for a batch of size b.\n",
    "            Y (Tensor): An n by b tensor representing the expected outputs\n",
    "                from the neural network for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            float: A scalar, L, which represents the loss of the neural\n",
    "                network for a batch of size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        # We will need these later to compute the backward pass.\n",
    "        self.A = A\n",
    "        self.Y = Y\n",
    "        self.L = -tf.reduce_sum(self.Y * tf.log(self.A))\n",
    "        \n",
    "        return self.L\n",
    "    \n",
    "NLLM.build_forward = build_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward pass computes the gradient of the loss with respect to the neural network's final activations $\\partial \\mathrm{Loss} / \\partial A$. Note, this is not immediately computing $\\partial \\mathrm{Loss} / \\partial Z$ by assuming softmax activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_backward(self):\n",
    "        \"\"\"Starts off the whole backward pass process.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: An n by b tensor, dLdA, representing the gradient of\n",
    "                the loss with respect to the neural network's outputs.\n",
    "                \n",
    "        \"\"\"\n",
    "        return tf.gradients(self.L, self.A)\n",
    "\n",
    "NLLM.build_backward = build_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Layer\n",
    "\n",
    "Unlike the NumPy implementation, we need to explicitly provide a layer that computes the accuracy (quite similar to the loss layer above). Any value we wish to compute from the neural network must be implemented in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(Layer):\n",
    "    \"\"\"Computes the accuracy of the current neural network outputs and\n",
    "        expected outputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only care about the forward pass as nothing uses the gradient of the accuracy. The forward pass uses the network's current outputs $A$ and the expected outputs $Y$ to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_forward(self, A, Y):\n",
    "        \"\"\"Connects the neural network to the accuracy layer using the forward\n",
    "            pass process.\n",
    "            \n",
    "        Args:\n",
    "            A (Tensor): An n by b tensor representing the neural network's\n",
    "                outputs for a batch of size b.\n",
    "            Y (Tensor): An n by b tensor representing the expected outputs\n",
    "                from the neural network for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            float: A scalar, acc, which represents the accuracy of the neural\n",
    "                network for a batch of size b.\n",
    "        \n",
    "        \"\"\"        \n",
    "        return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(A, axis=0), tf.argmax(Y, axis=0)), tf.float32))\n",
    "    \n",
    "Accuracy.build_forward = build_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now we have all the components to construct our neural network, but we need a model to connect them together.\n",
    "\n",
    "### Sequential Model\n",
    "\n",
    "The sequential model simply connects all the layer linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"A standard neural network model with linearly stacked layers.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to construct the model is to provide a list of layers. This is a little different from the NumPy implementation. From this list of layers, a graph is initialized for the model construction, and a graph session is initialized for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, layers):\n",
    "        \"\"\"Initializes the model by saving the layers provided. Note the\n",
    "            model is not ready for training yet. Please call the build\n",
    "            method.\n",
    "            \n",
    "        Args:\n",
    "            layers (list of Layer): A list of layers in sequential order\n",
    "                to construct the model from.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        \n",
    "        # When we build layer, we need to know input and output dimensions.\n",
    "        self.m = self.layers[0].m\n",
    "        self.n = self.layers[0].n\n",
    "        for layer in self.layers[1:]:\n",
    "            self.n = getattr(layer, 'n', self.n)\n",
    "        \n",
    "        # This is the static graph representing our model\n",
    "        self.graph = tf.Graph()\n",
    "        \n",
    "        # This is the runtime instance of our model\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "\n",
    "Sequential.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train, we need to construct the entire model graph from the provided layers. This is accomplished through a build method (similar to the compile method used in `tf.keras`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build(self, eta, loss, metrics):\n",
    "        \"\"\"Constructs the model's graph from the provided layers with the\n",
    "            specified loss and metrics.\n",
    "            \n",
    "        Args:\n",
    "            eta (float): A scalar representing the learning rate for\n",
    "                stochastic gradient descent.\n",
    "            loss (Layer): A layer used to construct the objective for\n",
    "                stochastic gradient descent.\n",
    "            metrics (list of Layers): A list of layers to use when\n",
    "                evaluating model performance.\n",
    "                \n",
    "        \"\"\"\n",
    "        # This ensures that the graph we add our variables to the graph\n",
    "        # unique to the model.\n",
    "        with self.graph.as_default():\n",
    "        \n",
    "            self.X = tf.placeholder(name='X', shape=(self.m, None), dtype=tf.float32)\n",
    "            self.Y = tf.placeholder(name='Y', shape=(self.n, None), dtype=tf.float32)\n",
    "\n",
    "            for layer in self.layers + [loss] + metrics:\n",
    "                layer.build()\n",
    "\n",
    "            self.forward = self.build_forward(self.X)\n",
    "            self.loss_forward = loss.build_forward(self.forward, self.Y)\n",
    "            self.metrics_forward = tf.tuple([metric.build_forward(self.forward, self.Y) for metric in metrics])\n",
    "\n",
    "            loss_backward = loss.build_backward()\n",
    "            self.build_backward(loss_backward)\n",
    "\n",
    "            self.build_sgd_step(eta)\n",
    "            \n",
    "            initializer = tf.global_variables_initializer()\n",
    "        \n",
    "        # This initializes the variables in our graph using the current \n",
    "        # instance session\n",
    "        self.sess.run(initializer)\n",
    "        \n",
    "Sequential.build = build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various helper methods called by the build method similar to those used in training in the NumPy implementation. The first is `build_forward`. This method constructs the entire forward pass and saves a handle to the operation in `self.forward`. Note that this does not return a value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_forward(self, X):\n",
    "        \"\"\"Constructs the entire forward pass for the network by \n",
    "            connecting each layer together.\n",
    "        \n",
    "        Args:\n",
    "            X (Tensor): An m by b tensor representing the placeholder \n",
    "                inputs to the neural network for a batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            Tensor: An n by b tensor representing the final output of the \n",
    "                neural network for a batch of size b to be evaluated in a \n",
    "                graph session.\n",
    "                \n",
    "        \"\"\"        \n",
    "        self.forward = X\n",
    "        for layer in self.layers:\n",
    "            self.forward = layer.build_forward(self.forward)\n",
    "        \n",
    "        return self.forward\n",
    "\n",
    "Sequential.build_forward = build_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build_backward` method constructs the entire backward pass of the network to compute the gradients for the training step later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_backward(self, dL):\n",
    "        \"\"\"Constructs the neural network's backward pass by connecting \n",
    "            each layer together.\n",
    "            \n",
    "        Args:\n",
    "            dL (Tensor): An n by b tensor representing the gradient of the\n",
    "                loss with respect to the output of the neural network.\n",
    "                \n",
    "        \"\"\"        \n",
    "        for layer in self.layers[::-1]:\n",
    "            dL = layer.build_backward(dL)\n",
    "    \n",
    "Sequential.build_backward = build_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we need to construct an operation that updates all the weights within the model. This is accomplished by the `build_sgd_step` method which iterates all `sgd_step` methods in the layers and groups together the ones which have weights to update. When `self.sgd_step` is called later, these updates will take place in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_sgd_step(self, eta):\n",
    "        \"\"\"Constructs the stochastic gradient descent training update step \n",
    "            for the entire neural network.\n",
    "            \n",
    "        Args:\n",
    "            eta (float): A learning rate for stochastic gradient descent.\n",
    "            \n",
    "        Returns:\n",
    "            Operation: An operation that executes each layer's stochastic\n",
    "                gradient descent update step in parallel.\n",
    "                \n",
    "        \"\"\"        \n",
    "        sgd_steps = [layer.build_sgd_step(eta) for layer in self.layers]\n",
    "        self.sgd_step = tf.group(*[sgd_step for sgd_step in sgd_steps if sgd_step is not None])\n",
    "        \n",
    "        return self.sgd_step\n",
    "    \n",
    "Sequential.build_sgd_step = build_sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our entire graph is constructed. To train the model, we use the `sgd` method. In this method, since the logic is saved in the graph from the `build` method, we just need to execute the training operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd(self, X_train, Y_train, epochs=100):\n",
    "        \"\"\"Performs stochastic gradient descent on the model.\n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): A d by n NumPy array representing n input\n",
    "                training points each with d features.\n",
    "            Y_train (ndarray): A c by n NumPy array representing n output\n",
    "                training points each with c features.\n",
    "            epochs (int): Number of iterations to run stochastic gradient\n",
    "                descent.\n",
    "                \n",
    "        \"\"\"\n",
    "        _, n = X_train.shape\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            t = np.random.randint(n)\n",
    "            \n",
    "            Xt = X_train[:, t:t + 1]\n",
    "            Yt = Y_train[:, t:t + 1]\n",
    "\n",
    "            self.sess.run(self.sgd_step, feed_dict={ self.X: Xt, self.Y: Yt })\n",
    "            \n",
    "            if epoch % 250 == 1:\n",
    "                \n",
    "                metrics_eval = self.sess.run(self.metrics_forward, feed_dict={ self.X: X_train, self.Y: Y_train })\n",
    "                print('Iteration =', epoch, '\\tAcc =', metrics_eval[1], '\\tLoss =', metrics_eval[0], flush=True)\n",
    "\n",
    "Sequential.sgd = sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the NumPy implementation where the `forward` method could be used for prediction, the TensorFlow implementation needs the execution to take place within a graph session. So, we wrap the forward result in a prediction method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict(self, X):\n",
    "        \"\"\"Returns the output from the forward pass.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): A d by n NumPy array representing n points each\n",
    "                with d features to predict outputs.\n",
    "                \n",
    "        Returns:\n",
    "            ndarray: A c by n NumPy array representing the outputs from\n",
    "                n points each with c features.\n",
    "                \n",
    "        \"\"\"\n",
    "        return self.sess.run(self.forward, feed_dict={ self.X: X })\n",
    "    \n",
    "Sequential.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Our model is complete! Let's train it on some data and see how will it can classify. We will use the standard 'hard' data set used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-0.23390341,  1.18151883, -2.46493986,  1.55322202,  1.27621763,\n",
    "                2.39710997, -1.34403040, -0.46903436, -0.64673502, -1.44029872,\n",
    "               -1.37537243,  1.05994811, -0.93311512,  1.02735575, -0.84138778,\n",
    "               -2.22585412, -0.42591102,  1.03561105,  0.91125595, -2.26550369],\n",
    "              [-0.92254932, -1.10309630, -2.41956036, -1.15509002, -1.04805327,\n",
    "                0.08717325,  0.81847250, -0.75171045,  0.60664705,  0.80410947,\n",
    "               -0.11600488,  1.03747218, -0.67210575,  0.99944446, -0.65559838,\n",
    "               -0.40744784, -0.58367642,  1.05972780, -0.95991874, -1.41720255]])\n",
    "\n",
    "Y = np.array([[0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
    "              [1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.scatter(X[0,:], X[1,:], c=Y[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct a neural network we think might be able to classify these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Linear(2, 10), ReLU(),\n",
    "                    Linear(10, 10), ReLU(),\n",
    "                    Linear(10, 2), Softmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget that we need to build the model before we use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model.build(0.005, NLLM(), [NLLM(), Accuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training the model on the data for a few thousand iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.2 \tLoss = 25.01606\n",
      "Iteration = 251 \tAcc = 0.95 \tLoss = 8.195608\n",
      "Iteration = 501 \tAcc = 0.9 \tLoss = 6.50253\n",
      "Iteration = 751 \tAcc = 0.9 \tLoss = 5.5067177\n",
      "Iteration = 1001 \tAcc = 0.95 \tLoss = 5.021373\n",
      "Iteration = 1251 \tAcc = 0.95 \tLoss = 4.8109136\n",
      "Iteration = 1501 \tAcc = 0.95 \tLoss = 4.537528\n",
      "Iteration = 1751 \tAcc = 0.95 \tLoss = 4.4280267\n",
      "Iteration = 2001 \tAcc = 0.95 \tLoss = 4.2647\n",
      "Iteration = 2251 \tAcc = 0.95 \tLoss = 4.143876\n",
      "Iteration = 2501 \tAcc = 0.95 \tLoss = 4.112421\n",
      "Iteration = 2751 \tAcc = 0.95 \tLoss = 4.0145397\n",
      "Iteration = 3001 \tAcc = 0.95 \tLoss = 3.9033072\n",
      "Iteration = 3251 \tAcc = 0.95 \tLoss = 3.8070154\n",
      "Iteration = 3501 \tAcc = 0.95 \tLoss = 3.7814898\n",
      "Iteration = 3751 \tAcc = 0.95 \tLoss = 3.7210689\n",
      "Iteration = 4001 \tAcc = 0.95 \tLoss = 3.64487\n",
      "Iteration = 4251 \tAcc = 0.95 \tLoss = 3.590065\n",
      "Iteration = 4501 \tAcc = 0.95 \tLoss = 3.5908434\n",
      "Iteration = 4751 \tAcc = 0.95 \tLoss = 3.5203233\n",
      "Iteration = 5001 \tAcc = 0.95 \tLoss = 3.4782405\n",
      "Iteration = 5251 \tAcc = 0.95 \tLoss = 3.418406\n",
      "Iteration = 5501 \tAcc = 0.95 \tLoss = 3.3799686\n",
      "Iteration = 5751 \tAcc = 0.95 \tLoss = 3.348053\n",
      "Iteration = 6001 \tAcc = 0.95 \tLoss = 3.3143344\n",
      "Iteration = 6251 \tAcc = 0.95 \tLoss = 3.307083\n",
      "Iteration = 6501 \tAcc = 0.95 \tLoss = 3.2594075\n",
      "Iteration = 6751 \tAcc = 0.95 \tLoss = 3.2705371\n",
      "Iteration = 7001 \tAcc = 0.95 \tLoss = 3.223093\n",
      "Iteration = 7251 \tAcc = 0.95 \tLoss = 3.1836967\n",
      "Iteration = 7501 \tAcc = 0.95 \tLoss = 3.1457372\n",
      "Iteration = 7751 \tAcc = 0.95 \tLoss = 3.1389961\n",
      "Iteration = 8001 \tAcc = 0.95 \tLoss = 3.0955307\n",
      "Iteration = 8251 \tAcc = 0.95 \tLoss = 3.0681126\n",
      "Iteration = 8501 \tAcc = 0.95 \tLoss = 3.0528362\n",
      "Iteration = 8751 \tAcc = 0.95 \tLoss = 3.0615878\n",
      "Iteration = 9001 \tAcc = 0.95 \tLoss = 3.0184321\n",
      "Iteration = 9251 \tAcc = 0.95 \tLoss = 2.9956503\n",
      "Iteration = 9501 \tAcc = 0.95 \tLoss = 2.9709682\n",
      "Iteration = 9751 \tAcc = 0.95 \tLoss = 2.9616659\n",
      "Iteration = 10001 \tAcc = 0.95 \tLoss = 2.932987\n",
      "Iteration = 10251 \tAcc = 0.95 \tLoss = 2.9718895\n",
      "Iteration = 10501 \tAcc = 0.95 \tLoss = 2.884018\n",
      "Iteration = 10751 \tAcc = 0.95 \tLoss = 2.8514352\n",
      "Iteration = 11001 \tAcc = 0.95 \tLoss = 2.8238459\n",
      "Iteration = 11251 \tAcc = 0.95 \tLoss = 2.9100478\n",
      "Iteration = 11501 \tAcc = 0.95 \tLoss = 2.808318\n",
      "Iteration = 11751 \tAcc = 0.95 \tLoss = 2.8784232\n",
      "Iteration = 12001 \tAcc = 0.95 \tLoss = 2.746776\n",
      "Iteration = 12251 \tAcc = 0.95 \tLoss = 2.7227356\n",
      "Iteration = 12501 \tAcc = 0.95 \tLoss = 2.6906075\n",
      "Iteration = 12751 \tAcc = 0.95 \tLoss = 2.684509\n",
      "Iteration = 13001 \tAcc = 0.95 \tLoss = 2.6301405\n",
      "Iteration = 13251 \tAcc = 0.95 \tLoss = 2.554929\n",
      "Iteration = 13501 \tAcc = 0.95 \tLoss = 2.533187\n",
      "Iteration = 13751 \tAcc = 0.95 \tLoss = 2.5583322\n",
      "Iteration = 14001 \tAcc = 0.95 \tLoss = 2.4301186\n",
      "Iteration = 14251 \tAcc = 0.95 \tLoss = 2.3995628\n",
      "Iteration = 14501 \tAcc = 0.95 \tLoss = 2.4026976\n",
      "Iteration = 14751 \tAcc = 0.95 \tLoss = 2.3360877\n",
      "Iteration = 15001 \tAcc = 0.95 \tLoss = 2.327806\n",
      "Iteration = 15251 \tAcc = 0.95 \tLoss = 2.2811816\n",
      "Iteration = 15501 \tAcc = 0.95 \tLoss = 2.2527375\n",
      "Iteration = 15751 \tAcc = 0.95 \tLoss = 2.206275\n",
      "Iteration = 16001 \tAcc = 0.95 \tLoss = 2.2058272\n",
      "Iteration = 16251 \tAcc = 0.95 \tLoss = 2.175063\n",
      "Iteration = 16501 \tAcc = 0.95 \tLoss = 2.1322088\n",
      "Iteration = 16751 \tAcc = 0.95 \tLoss = 2.0903113\n",
      "Iteration = 17001 \tAcc = 0.95 \tLoss = 2.1710176\n",
      "Iteration = 17251 \tAcc = 0.95 \tLoss = 2.050619\n",
      "Iteration = 17501 \tAcc = 0.95 \tLoss = 2.0752764\n",
      "Iteration = 17751 \tAcc = 0.95 \tLoss = 1.9973102\n",
      "Iteration = 18001 \tAcc = 0.95 \tLoss = 1.9694096\n",
      "Iteration = 18251 \tAcc = 0.95 \tLoss = 2.0268157\n",
      "Iteration = 18501 \tAcc = 0.95 \tLoss = 1.9680762\n",
      "Iteration = 18751 \tAcc = 0.95 \tLoss = 1.9501151\n",
      "Iteration = 19001 \tAcc = 0.95 \tLoss = 1.8749721\n",
      "Iteration = 19251 \tAcc = 0.95 \tLoss = 1.8567057\n",
      "Iteration = 19501 \tAcc = 0.95 \tLoss = 1.8647991\n",
      "Iteration = 19751 \tAcc = 0.95 \tLoss = 1.8166834\n",
      "Iteration = 20001 \tAcc = 0.95 \tLoss = 1.821011\n",
      "Iteration = 20251 \tAcc = 0.95 \tLoss = 1.8818557\n",
      "Iteration = 20501 \tAcc = 0.95 \tLoss = 1.7756308\n",
      "Iteration = 20751 \tAcc = 0.95 \tLoss = 1.8051327\n",
      "Iteration = 21001 \tAcc = 0.95 \tLoss = 1.7499286\n",
      "Iteration = 21251 \tAcc = 0.95 \tLoss = 1.8459687\n",
      "Iteration = 21501 \tAcc = 0.95 \tLoss = 1.7189641\n",
      "Iteration = 21751 \tAcc = 0.95 \tLoss = 1.6895443\n",
      "Iteration = 22001 \tAcc = 0.95 \tLoss = 1.6935334\n",
      "Iteration = 22251 \tAcc = 0.95 \tLoss = 1.7067671\n",
      "Iteration = 22501 \tAcc = 0.95 \tLoss = 1.6691525\n",
      "Iteration = 22751 \tAcc = 0.95 \tLoss = 1.5919148\n",
      "Iteration = 23001 \tAcc = 0.95 \tLoss = 1.790838\n",
      "Iteration = 23251 \tAcc = 0.95 \tLoss = 1.5654936\n",
      "Iteration = 23501 \tAcc = 0.95 \tLoss = 1.5699178\n",
      "Iteration = 23751 \tAcc = 0.95 \tLoss = 1.6209583\n",
      "Iteration = 24001 \tAcc = 0.95 \tLoss = 1.5159256\n",
      "Iteration = 24251 \tAcc = 0.95 \tLoss = 1.5896053\n",
      "Iteration = 24501 \tAcc = 0.95 \tLoss = 1.5312588\n",
      "Iteration = 24751 \tAcc = 0.95 \tLoss = 1.5664065\n",
      "Iteration = 25001 \tAcc = 0.95 \tLoss = 1.5525402\n",
      "Iteration = 25251 \tAcc = 0.95 \tLoss = 1.5358394\n",
      "Iteration = 25501 \tAcc = 0.95 \tLoss = 1.5237062\n",
      "Iteration = 25751 \tAcc = 0.95 \tLoss = 1.4674363\n",
      "Iteration = 26001 \tAcc = 0.95 \tLoss = 1.4255161\n",
      "Iteration = 26251 \tAcc = 0.95 \tLoss = 1.521777\n",
      "Iteration = 26501 \tAcc = 0.95 \tLoss = 1.4170425\n",
      "Iteration = 26751 \tAcc = 0.95 \tLoss = 1.4179208\n",
      "Iteration = 27001 \tAcc = 0.95 \tLoss = 1.3876659\n",
      "Iteration = 27251 \tAcc = 0.95 \tLoss = 1.5189356\n",
      "Iteration = 27501 \tAcc = 0.95 \tLoss = 1.3544415\n",
      "Iteration = 27751 \tAcc = 0.95 \tLoss = 1.4391022\n",
      "Iteration = 28001 \tAcc = 0.95 \tLoss = 1.3295382\n",
      "Iteration = 28251 \tAcc = 0.95 \tLoss = 1.3659694\n",
      "Iteration = 28501 \tAcc = 1.0 \tLoss = 1.3155278\n",
      "Iteration = 28751 \tAcc = 0.95 \tLoss = 1.3769771\n",
      "Iteration = 29001 \tAcc = 1.0 \tLoss = 1.3396968\n",
      "Iteration = 29251 \tAcc = 0.95 \tLoss = 1.28967\n",
      "Iteration = 29501 \tAcc = 0.95 \tLoss = 1.2757132\n",
      "Iteration = 29751 \tAcc = 1.0 \tLoss = 1.2729372\n",
      "Iteration = 30001 \tAcc = 0.95 \tLoss = 1.336861\n",
      "Iteration = 30251 \tAcc = 0.95 \tLoss = 1.4328792\n",
      "Iteration = 30501 \tAcc = 0.95 \tLoss = 1.2997643\n",
      "Iteration = 30751 \tAcc = 1.0 \tLoss = 1.2264932\n",
      "Iteration = 31001 \tAcc = 0.95 \tLoss = 1.2499496\n",
      "Iteration = 31251 \tAcc = 0.95 \tLoss = 1.2793729\n",
      "Iteration = 31501 \tAcc = 0.95 \tLoss = 1.2604381\n",
      "Iteration = 31751 \tAcc = 1.0 \tLoss = 1.2413383\n",
      "Iteration = 32001 \tAcc = 0.95 \tLoss = 1.8606192\n",
      "Iteration = 32251 \tAcc = 0.95 \tLoss = 1.5026642\n",
      "Iteration = 32501 \tAcc = 0.95 \tLoss = 1.306649\n",
      "Iteration = 32751 \tAcc = 1.0 \tLoss = 1.1728389\n",
      "Iteration = 33001 \tAcc = 1.0 \tLoss = 1.1605718\n",
      "Iteration = 33251 \tAcc = 1.0 \tLoss = 1.187154\n",
      "Iteration = 33501 \tAcc = 1.0 \tLoss = 1.2011685\n",
      "Iteration = 33751 \tAcc = 0.95 \tLoss = 1.2434309\n",
      "Iteration = 34001 \tAcc = 0.95 \tLoss = 1.2328573\n",
      "Iteration = 34251 \tAcc = 1.0 \tLoss = 1.200211\n",
      "Iteration = 34501 \tAcc = 1.0 \tLoss = 1.1237575\n",
      "Iteration = 34751 \tAcc = 0.95 \tLoss = 1.3952512\n",
      "Iteration = 35001 \tAcc = 1.0 \tLoss = 1.1183627\n",
      "Iteration = 35251 \tAcc = 1.0 \tLoss = 1.1288363\n",
      "Iteration = 35501 \tAcc = 1.0 \tLoss = 1.1053994\n",
      "Iteration = 35751 \tAcc = 0.95 \tLoss = 1.231001\n",
      "Iteration = 36001 \tAcc = 0.95 \tLoss = 1.2266945\n",
      "Iteration = 36251 \tAcc = 0.95 \tLoss = 1.3123221\n",
      "Iteration = 36501 \tAcc = 1.0 \tLoss = 1.0757004\n",
      "Iteration = 36751 \tAcc = 0.95 \tLoss = 1.1329333\n",
      "Iteration = 37001 \tAcc = 0.95 \tLoss = 1.134955\n",
      "Iteration = 37251 \tAcc = 1.0 \tLoss = 1.0568419\n",
      "Iteration = 37501 \tAcc = 1.0 \tLoss = 1.0532333\n",
      "Iteration = 37751 \tAcc = 1.0 \tLoss = 1.0793347\n",
      "Iteration = 38001 \tAcc = 1.0 \tLoss = 1.0417881\n",
      "Iteration = 38251 \tAcc = 0.95 \tLoss = 1.1152741\n",
      "Iteration = 38501 \tAcc = 1.0 \tLoss = 1.0518622\n",
      "Iteration = 38751 \tAcc = 0.95 \tLoss = 1.2351373\n",
      "Iteration = 39001 \tAcc = 0.95 \tLoss = 1.0740458\n",
      "Iteration = 39251 \tAcc = 1.0 \tLoss = 1.0112325\n",
      "Iteration = 39501 \tAcc = 0.95 \tLoss = 1.251281\n",
      "Iteration = 39751 \tAcc = 1.0 \tLoss = 1.0742183\n",
      "Iteration = 40001 \tAcc = 0.95 \tLoss = 1.0649525\n",
      "Iteration = 40251 \tAcc = 0.95 \tLoss = 1.0561749\n",
      "Iteration = 40501 \tAcc = 1.0 \tLoss = 1.0244792\n",
      "Iteration = 40751 \tAcc = 0.95 \tLoss = 1.4462166\n",
      "Iteration = 41001 \tAcc = 0.95 \tLoss = 1.1838561\n",
      "Iteration = 41251 \tAcc = 1.0 \tLoss = 0.9762721\n",
      "Iteration = 41501 \tAcc = 1.0 \tLoss = 0.9681248\n",
      "Iteration = 41751 \tAcc = 1.0 \tLoss = 1.0655355\n",
      "Iteration = 42001 \tAcc = 1.0 \tLoss = 1.0795926\n",
      "Iteration = 42251 \tAcc = 1.0 \tLoss = 0.98878413\n",
      "Iteration = 42501 \tAcc = 1.0 \tLoss = 0.9452475\n",
      "Iteration = 42751 \tAcc = 1.0 \tLoss = 0.9444044\n",
      "Iteration = 43001 \tAcc = 1.0 \tLoss = 0.9551431\n",
      "Iteration = 43251 \tAcc = 1.0 \tLoss = 0.94923127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 43501 \tAcc = 0.95 \tLoss = 1.0662855\n",
      "Iteration = 43751 \tAcc = 1.0 \tLoss = 0.98059946\n",
      "Iteration = 44001 \tAcc = 1.0 \tLoss = 0.91388685\n",
      "Iteration = 44251 \tAcc = 1.0 \tLoss = 0.9952152\n",
      "Iteration = 44501 \tAcc = 0.95 \tLoss = 1.1657723\n",
      "Iteration = 44751 \tAcc = 1.0 \tLoss = 0.9747345\n",
      "Iteration = 45001 \tAcc = 1.0 \tLoss = 0.94701004\n",
      "Iteration = 45251 \tAcc = 0.95 \tLoss = 1.148492\n",
      "Iteration = 45501 \tAcc = 0.95 \tLoss = 1.0012403\n",
      "Iteration = 45751 \tAcc = 1.0 \tLoss = 0.8991046\n",
      "Iteration = 46001 \tAcc = 1.0 \tLoss = 0.9092682\n",
      "Iteration = 46251 \tAcc = 0.95 \tLoss = 0.9729304\n",
      "Iteration = 46501 \tAcc = 1.0 \tLoss = 0.85670954\n",
      "Iteration = 46751 \tAcc = 0.95 \tLoss = 1.0316261\n",
      "Iteration = 47001 \tAcc = 1.0 \tLoss = 0.90296185\n",
      "Iteration = 47251 \tAcc = 1.0 \tLoss = 0.8993231\n",
      "Iteration = 47501 \tAcc = 0.95 \tLoss = 1.4505754\n",
      "Iteration = 47751 \tAcc = 1.0 \tLoss = 0.85622007\n",
      "Iteration = 48001 \tAcc = 1.0 \tLoss = 0.8766241\n",
      "Iteration = 48251 \tAcc = 1.0 \tLoss = 0.8738384\n",
      "Iteration = 48501 \tAcc = 1.0 \tLoss = 0.86451703\n",
      "Iteration = 48751 \tAcc = 1.0 \tLoss = 0.87969244\n",
      "Iteration = 49001 \tAcc = 1.0 \tLoss = 0.89391786\n",
      "Iteration = 49251 \tAcc = 1.0 \tLoss = 0.8044895\n",
      "Iteration = 49501 \tAcc = 1.0 \tLoss = 0.9433481\n",
      "Iteration = 49751 \tAcc = 1.0 \tLoss = 0.8265517\n",
      "Iteration = 50001 \tAcc = 1.0 \tLoss = 0.78791267\n",
      "Iteration = 50251 \tAcc = 1.0 \tLoss = 0.76981616\n",
      "Iteration = 50501 \tAcc = 1.0 \tLoss = 0.88111985\n",
      "Iteration = 50751 \tAcc = 1.0 \tLoss = 0.79788\n",
      "Iteration = 51001 \tAcc = 1.0 \tLoss = 0.8069487\n",
      "Iteration = 51251 \tAcc = 1.0 \tLoss = 0.7622535\n",
      "Iteration = 51501 \tAcc = 1.0 \tLoss = 0.7599638\n",
      "Iteration = 51751 \tAcc = 1.0 \tLoss = 0.7994752\n",
      "Iteration = 52001 \tAcc = 1.0 \tLoss = 0.79273087\n",
      "Iteration = 52251 \tAcc = 1.0 \tLoss = 0.77140164\n",
      "Iteration = 52501 \tAcc = 0.95 \tLoss = 1.0958508\n",
      "Iteration = 52751 \tAcc = 0.95 \tLoss = 0.9943652\n",
      "Iteration = 53001 \tAcc = 1.0 \tLoss = 0.76244587\n",
      "Iteration = 53251 \tAcc = 1.0 \tLoss = 0.788931\n",
      "Iteration = 53501 \tAcc = 1.0 \tLoss = 0.7683749\n",
      "Iteration = 53751 \tAcc = 1.0 \tLoss = 0.72007483\n",
      "Iteration = 54001 \tAcc = 1.0 \tLoss = 0.77444327\n",
      "Iteration = 54251 \tAcc = 1.0 \tLoss = 0.6986818\n",
      "Iteration = 54501 \tAcc = 1.0 \tLoss = 0.74501675\n",
      "Iteration = 54751 \tAcc = 0.95 \tLoss = 1.1472238\n",
      "Iteration = 55001 \tAcc = 1.0 \tLoss = 0.7578668\n",
      "Iteration = 55251 \tAcc = 1.0 \tLoss = 0.6722271\n",
      "Iteration = 55501 \tAcc = 1.0 \tLoss = 0.66623944\n",
      "Iteration = 55751 \tAcc = 1.0 \tLoss = 0.7487008\n",
      "Iteration = 56001 \tAcc = 0.95 \tLoss = 0.95213646\n",
      "Iteration = 56251 \tAcc = 1.0 \tLoss = 0.66043335\n",
      "Iteration = 56501 \tAcc = 1.0 \tLoss = 0.6742123\n",
      "Iteration = 56751 \tAcc = 1.0 \tLoss = 0.749367\n",
      "Iteration = 57001 \tAcc = 1.0 \tLoss = 0.7908064\n",
      "Iteration = 57251 \tAcc = 1.0 \tLoss = 0.76781875\n",
      "Iteration = 57501 \tAcc = 1.0 \tLoss = 0.73179334\n",
      "Iteration = 57751 \tAcc = 1.0 \tLoss = 0.62204456\n",
      "Iteration = 58001 \tAcc = 1.0 \tLoss = 0.63769436\n",
      "Iteration = 58251 \tAcc = 1.0 \tLoss = 0.6469604\n",
      "Iteration = 58501 \tAcc = 1.0 \tLoss = 0.66818106\n",
      "Iteration = 58751 \tAcc = 0.95 \tLoss = 0.99106133\n",
      "Iteration = 59001 \tAcc = 1.0 \tLoss = 0.6475863\n",
      "Iteration = 59251 \tAcc = 1.0 \tLoss = 0.6166608\n",
      "Iteration = 59501 \tAcc = 1.0 \tLoss = 0.58545893\n",
      "Iteration = 59751 \tAcc = 1.0 \tLoss = 0.5839363\n",
      "Iteration = 60001 \tAcc = 1.0 \tLoss = 0.6667744\n",
      "Iteration = 60251 \tAcc = 1.0 \tLoss = 0.7218217\n",
      "Iteration = 60501 \tAcc = 1.0 \tLoss = 0.5893633\n",
      "Iteration = 60751 \tAcc = 1.0 \tLoss = 0.6128693\n",
      "Iteration = 61001 \tAcc = 1.0 \tLoss = 0.6040117\n",
      "Iteration = 61251 \tAcc = 1.0 \tLoss = 0.5802284\n",
      "Iteration = 61501 \tAcc = 1.0 \tLoss = 0.5501968\n",
      "Iteration = 61751 \tAcc = 1.0 \tLoss = 0.5746168\n",
      "Iteration = 62001 \tAcc = 1.0 \tLoss = 0.6073054\n",
      "Iteration = 62251 \tAcc = 1.0 \tLoss = 0.77373904\n",
      "Iteration = 62501 \tAcc = 1.0 \tLoss = 0.52795136\n",
      "Iteration = 62751 \tAcc = 1.0 \tLoss = 0.6947317\n",
      "Iteration = 63001 \tAcc = 1.0 \tLoss = 0.570819\n",
      "Iteration = 63251 \tAcc = 1.0 \tLoss = 0.5889889\n",
      "Iteration = 63501 \tAcc = 1.0 \tLoss = 0.5150562\n",
      "Iteration = 63751 \tAcc = 1.0 \tLoss = 0.6215464\n",
      "Iteration = 64001 \tAcc = 1.0 \tLoss = 0.5136494\n",
      "Iteration = 64251 \tAcc = 1.0 \tLoss = 0.62399346\n",
      "Iteration = 64501 \tAcc = 1.0 \tLoss = 0.7745985\n",
      "Iteration = 64751 \tAcc = 1.0 \tLoss = 0.56087613\n",
      "Iteration = 65001 \tAcc = 1.0 \tLoss = 0.7542894\n",
      "Iteration = 65251 \tAcc = 1.0 \tLoss = 0.54883456\n",
      "Iteration = 65501 \tAcc = 1.0 \tLoss = 0.4802531\n",
      "Iteration = 65751 \tAcc = 1.0 \tLoss = 0.4804566\n",
      "Iteration = 66001 \tAcc = 1.0 \tLoss = 0.5935386\n",
      "Iteration = 66251 \tAcc = 1.0 \tLoss = 0.5055378\n",
      "Iteration = 66501 \tAcc = 1.0 \tLoss = 0.45939633\n",
      "Iteration = 66751 \tAcc = 1.0 \tLoss = 0.4729119\n",
      "Iteration = 67001 \tAcc = 1.0 \tLoss = 0.5574156\n",
      "Iteration = 67251 \tAcc = 1.0 \tLoss = 0.59365976\n",
      "Iteration = 67501 \tAcc = 1.0 \tLoss = 0.45528454\n",
      "Iteration = 67751 \tAcc = 1.0 \tLoss = 0.48768476\n",
      "Iteration = 68001 \tAcc = 1.0 \tLoss = 0.43924087\n",
      "Iteration = 68251 \tAcc = 1.0 \tLoss = 0.56393135\n",
      "Iteration = 68501 \tAcc = 1.0 \tLoss = 0.4677448\n",
      "Iteration = 68751 \tAcc = 1.0 \tLoss = 0.53626347\n",
      "Iteration = 69001 \tAcc = 1.0 \tLoss = 0.6278515\n",
      "Iteration = 69251 \tAcc = 1.0 \tLoss = 0.48387063\n",
      "Iteration = 69501 \tAcc = 1.0 \tLoss = 0.49098048\n",
      "Iteration = 69751 \tAcc = 1.0 \tLoss = 0.4386888\n",
      "Iteration = 70001 \tAcc = 1.0 \tLoss = 0.4429337\n",
      "Iteration = 70251 \tAcc = 1.0 \tLoss = 0.44136027\n",
      "Iteration = 70501 \tAcc = 1.0 \tLoss = 0.4273934\n",
      "Iteration = 70751 \tAcc = 1.0 \tLoss = 0.47779682\n",
      "Iteration = 71001 \tAcc = 1.0 \tLoss = 0.43823308\n",
      "Iteration = 71251 \tAcc = 1.0 \tLoss = 0.39098158\n",
      "Iteration = 71501 \tAcc = 1.0 \tLoss = 0.39808077\n",
      "Iteration = 71751 \tAcc = 1.0 \tLoss = 0.39778936\n",
      "Iteration = 72001 \tAcc = 1.0 \tLoss = 0.38310564\n",
      "Iteration = 72251 \tAcc = 1.0 \tLoss = 0.3802067\n",
      "Iteration = 72501 \tAcc = 1.0 \tLoss = 0.4060323\n",
      "Iteration = 72751 \tAcc = 1.0 \tLoss = 0.42499962\n",
      "Iteration = 73001 \tAcc = 1.0 \tLoss = 0.39002913\n",
      "Iteration = 73251 \tAcc = 1.0 \tLoss = 0.36679593\n",
      "Iteration = 73501 \tAcc = 1.0 \tLoss = 0.5414448\n",
      "Iteration = 73751 \tAcc = 1.0 \tLoss = 0.5396926\n",
      "Iteration = 74001 \tAcc = 1.0 \tLoss = 0.36420947\n",
      "Iteration = 74251 \tAcc = 1.0 \tLoss = 0.38010168\n",
      "Iteration = 74501 \tAcc = 1.0 \tLoss = 0.37008235\n",
      "Iteration = 74751 \tAcc = 1.0 \tLoss = 0.35745272\n",
      "Iteration = 75001 \tAcc = 1.0 \tLoss = 0.3892587\n",
      "Iteration = 75251 \tAcc = 1.0 \tLoss = 0.3867507\n",
      "Iteration = 75501 \tAcc = 1.0 \tLoss = 0.4023842\n",
      "Iteration = 75751 \tAcc = 1.0 \tLoss = 0.3726369\n",
      "Iteration = 76001 \tAcc = 1.0 \tLoss = 0.3716042\n",
      "Iteration = 76251 \tAcc = 1.0 \tLoss = 0.36568624\n",
      "Iteration = 76501 \tAcc = 1.0 \tLoss = 0.32506236\n",
      "Iteration = 76751 \tAcc = 1.0 \tLoss = 0.37427968\n",
      "Iteration = 77001 \tAcc = 1.0 \tLoss = 0.34724858\n",
      "Iteration = 77251 \tAcc = 1.0 \tLoss = 0.33385983\n",
      "Iteration = 77501 \tAcc = 1.0 \tLoss = 0.34202528\n",
      "Iteration = 77751 \tAcc = 1.0 \tLoss = 0.37159616\n",
      "Iteration = 78001 \tAcc = 1.0 \tLoss = 0.34426758\n",
      "Iteration = 78251 \tAcc = 1.0 \tLoss = 0.3551571\n",
      "Iteration = 78501 \tAcc = 1.0 \tLoss = 0.34373567\n",
      "Iteration = 78751 \tAcc = 1.0 \tLoss = 0.33141303\n",
      "Iteration = 79001 \tAcc = 1.0 \tLoss = 0.33981845\n",
      "Iteration = 79251 \tAcc = 1.0 \tLoss = 0.30528656\n",
      "Iteration = 79501 \tAcc = 1.0 \tLoss = 0.53127337\n",
      "Iteration = 79751 \tAcc = 1.0 \tLoss = 0.3106727\n",
      "Iteration = 80001 \tAcc = 1.0 \tLoss = 0.2986814\n",
      "Iteration = 80251 \tAcc = 1.0 \tLoss = 0.30416992\n",
      "Iteration = 80501 \tAcc = 1.0 \tLoss = 0.32659414\n",
      "Iteration = 80751 \tAcc = 1.0 \tLoss = 0.32847944\n",
      "Iteration = 81001 \tAcc = 1.0 \tLoss = 0.32453778\n",
      "Iteration = 81251 \tAcc = 1.0 \tLoss = 0.28069523\n",
      "Iteration = 81501 \tAcc = 1.0 \tLoss = 0.30984434\n",
      "Iteration = 81751 \tAcc = 1.0 \tLoss = 0.30928734\n",
      "Iteration = 82001 \tAcc = 1.0 \tLoss = 0.28884795\n",
      "Iteration = 82251 \tAcc = 1.0 \tLoss = 0.3104828\n",
      "Iteration = 82501 \tAcc = 1.0 \tLoss = 0.2712806\n",
      "Iteration = 82751 \tAcc = 1.0 \tLoss = 0.27575448\n",
      "Iteration = 83001 \tAcc = 1.0 \tLoss = 0.29641926\n",
      "Iteration = 83251 \tAcc = 1.0 \tLoss = 0.29115152\n",
      "Iteration = 83501 \tAcc = 1.0 \tLoss = 0.26020837\n",
      "Iteration = 83751 \tAcc = 1.0 \tLoss = 0.28483635\n",
      "Iteration = 84001 \tAcc = 1.0 \tLoss = 0.26642743\n",
      "Iteration = 84251 \tAcc = 1.0 \tLoss = 0.26963776\n",
      "Iteration = 84501 \tAcc = 1.0 \tLoss = 0.25893533\n",
      "Iteration = 84751 \tAcc = 1.0 \tLoss = 0.2602803\n",
      "Iteration = 85001 \tAcc = 1.0 \tLoss = 0.25988877\n",
      "Iteration = 85251 \tAcc = 1.0 \tLoss = 0.2795642\n",
      "Iteration = 85501 \tAcc = 1.0 \tLoss = 0.25241905\n",
      "Iteration = 85751 \tAcc = 1.0 \tLoss = 0.24454406\n",
      "Iteration = 86001 \tAcc = 1.0 \tLoss = 0.24119201\n",
      "Iteration = 86251 \tAcc = 1.0 \tLoss = 0.26909223\n",
      "Iteration = 86501 \tAcc = 1.0 \tLoss = 0.26413295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 86751 \tAcc = 1.0 \tLoss = 0.2750498\n",
      "Iteration = 87001 \tAcc = 1.0 \tLoss = 0.2834404\n",
      "Iteration = 87251 \tAcc = 1.0 \tLoss = 0.27181256\n",
      "Iteration = 87501 \tAcc = 1.0 \tLoss = 0.23848884\n",
      "Iteration = 87751 \tAcc = 1.0 \tLoss = 0.2630318\n",
      "Iteration = 88001 \tAcc = 1.0 \tLoss = 0.2342635\n",
      "Iteration = 88251 \tAcc = 1.0 \tLoss = 0.30098337\n",
      "Iteration = 88501 \tAcc = 1.0 \tLoss = 0.27100283\n",
      "Iteration = 88751 \tAcc = 1.0 \tLoss = 0.23173597\n",
      "Iteration = 89001 \tAcc = 1.0 \tLoss = 0.25625822\n",
      "Iteration = 89251 \tAcc = 1.0 \tLoss = 0.2195088\n",
      "Iteration = 89501 \tAcc = 1.0 \tLoss = 0.21940753\n",
      "Iteration = 89751 \tAcc = 1.0 \tLoss = 0.2239819\n",
      "Iteration = 90001 \tAcc = 1.0 \tLoss = 0.2187289\n",
      "Iteration = 90251 \tAcc = 1.0 \tLoss = 0.21820526\n",
      "Iteration = 90501 \tAcc = 1.0 \tLoss = 0.24639423\n",
      "Iteration = 90751 \tAcc = 1.0 \tLoss = 0.22933148\n",
      "Iteration = 91001 \tAcc = 1.0 \tLoss = 0.219897\n",
      "Iteration = 91251 \tAcc = 1.0 \tLoss = 0.21001445\n",
      "Iteration = 91501 \tAcc = 1.0 \tLoss = 0.20962006\n",
      "Iteration = 91751 \tAcc = 1.0 \tLoss = 0.23251504\n",
      "Iteration = 92001 \tAcc = 1.0 \tLoss = 0.20961577\n",
      "Iteration = 92251 \tAcc = 1.0 \tLoss = 0.22378637\n",
      "Iteration = 92501 \tAcc = 1.0 \tLoss = 0.2026087\n",
      "Iteration = 92751 \tAcc = 1.0 \tLoss = 0.2151879\n",
      "Iteration = 93001 \tAcc = 1.0 \tLoss = 0.23074852\n",
      "Iteration = 93251 \tAcc = 1.0 \tLoss = 0.24083084\n",
      "Iteration = 93501 \tAcc = 1.0 \tLoss = 0.20763476\n",
      "Iteration = 93751 \tAcc = 1.0 \tLoss = 0.36205542\n",
      "Iteration = 94001 \tAcc = 1.0 \tLoss = 0.2160599\n",
      "Iteration = 94251 \tAcc = 1.0 \tLoss = 0.22619075\n",
      "Iteration = 94501 \tAcc = 1.0 \tLoss = 0.20992401\n",
      "Iteration = 94751 \tAcc = 1.0 \tLoss = 0.23945948\n",
      "Iteration = 95001 \tAcc = 1.0 \tLoss = 0.18831755\n",
      "Iteration = 95251 \tAcc = 1.0 \tLoss = 0.20907192\n",
      "Iteration = 95501 \tAcc = 1.0 \tLoss = 0.20558922\n",
      "Iteration = 95751 \tAcc = 1.0 \tLoss = 0.19186373\n",
      "Iteration = 96001 \tAcc = 1.0 \tLoss = 0.206018\n",
      "Iteration = 96251 \tAcc = 1.0 \tLoss = 0.1842994\n",
      "Iteration = 96501 \tAcc = 1.0 \tLoss = 0.20033781\n",
      "Iteration = 96751 \tAcc = 1.0 \tLoss = 0.20737213\n",
      "Iteration = 97001 \tAcc = 1.0 \tLoss = 0.19299783\n",
      "Iteration = 97251 \tAcc = 1.0 \tLoss = 0.19472471\n",
      "Iteration = 97501 \tAcc = 1.0 \tLoss = 0.17949373\n",
      "Iteration = 97751 \tAcc = 1.0 \tLoss = 0.1798927\n",
      "Iteration = 98001 \tAcc = 1.0 \tLoss = 0.19969407\n",
      "Iteration = 98251 \tAcc = 1.0 \tLoss = 0.17408189\n",
      "Iteration = 98501 \tAcc = 1.0 \tLoss = 0.17881119\n",
      "Iteration = 98751 \tAcc = 1.0 \tLoss = 0.1942354\n",
      "Iteration = 99001 \tAcc = 1.0 \tLoss = 0.20705758\n",
      "Iteration = 99251 \tAcc = 1.0 \tLoss = 0.19070409\n",
      "Iteration = 99501 \tAcc = 1.0 \tLoss = 0.19604656\n",
      "Iteration = 99751 \tAcc = 1.0 \tLoss = 0.1739333\n"
     ]
    }
   ],
   "source": [
    "model.sgd(X, Y, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like it has fitted the data 100\\%. Let's see what the decision boundary looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a grid of points to classify\n",
    "xx1, xx2 = np.meshgrid(np.arange(-3, 3, 0.005), np.arange(-3, 3, 0.005))\n",
    "\n",
    "# Flatten the grid to pass into model\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()].T\n",
    "\n",
    "# Predict classification at every point on the grid\n",
    "Z = model.predict(grid)[1, :].reshape(xx1.shape)\n",
    "\n",
    "# Plot the prediction regions.\n",
    "plt.imshow(Z, interpolation='bicubic', origin='lower', extent=[-3, 3, -3, 3], \n",
    "           cmap=ListedColormap(['#1f77b4', '#ff7f0e']), alpha=0.55, aspect='auto')\n",
    "\n",
    "# Plot the original points.\n",
    "_ = plt.scatter(X[0,:], X[1,:], c=Y[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
