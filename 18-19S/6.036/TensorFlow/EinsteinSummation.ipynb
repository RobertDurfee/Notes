{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einstein Summation\n",
    "\n",
    "This notebook is used to help me get a feel of Einstein summation notation. As an example, I am trying to construct the categorical cross-entropy (negative log-likelihood multi-class) loss gradient with respect to the inputs to softmax for a mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolically\n",
    "\n",
    "Before diving into the Einstein summation notation, we need to know what we are trying to construct (and where it comes from). We are trying to compute\n",
    "\n",
    "$$ \\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L} $$\n",
    "\n",
    "Using chain rule, we want to break up the computation into two parts: the gradient of the negative log-likelihood multiclass loss with respect to its inputs and the gradient of the softmax function with respect to its inputs. That is,\n",
    "\n",
    "$$ \\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L} =\\frac{\\partial A^L}{\\partial Z^L}\\frac{\\partial \\mathrm{NLLM}}{\\partial A^L} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "\n",
    "First, we consider the softmax function. The softmax function is defined for a vector $Z^L$ with dimensions $n^L \\times b$ (the output of a neural network with $n^L$ neurons in the final layer $L$ for a batch of size $b$) as the following:\n",
    "\n",
    "$$ \\mathrm{softmax}\\left(Z^L\\right) = \\begin{pmatrix}\n",
    "    \\exp{z_{11}^L}/\\sum_{m=1}^{n^L}\\exp{z_{m1}^L} & \\cdots & \\exp{z_{1b}^L}/\\sum_{m=1}^{n^L}\\exp{z_{mb}^L} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\exp{z_{n^L1}^L}/\\sum_{m=1}^{n^L}\\exp{z_{m1}^L} & \\cdots & \\exp{z_{n^Lb}^L}/\\sum_{m=1}^{n^L}\\exp{z_{mb}^L}\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "For simplicity, let $\\mathrm{softmax}\\left(Z^L\\right) = A^L$. Thus, elementwise, this is the same as\n",
    "\n",
    "$$ a_{ij}^L = \\frac{\\exp{z_{ij}^L}}{\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Gradient\n",
    "\n",
    "Now, we want to construct the gradient of this function with respect to the input matrix. This is a matrix by matrix derivative. We should expect a $4$-dimensional tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Element Derivative\n",
    "\n",
    "Let's consider the derivative of a single element of softmax $a_{ij}^L$ with respect to a single input $z_{k\\ell}^L$. That is, we are looking for $\\partial a_{ij}^L / \\partial z_{k\\ell}^L$. Writing this out in terms of the softmax:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial z_{k\\ell}}\\left(\\frac{\\exp{z_{ij}^L}}{\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}}\\right) $$\n",
    "\n",
    "Using the quotient rule,\n",
    "\n",
    "$$ \\frac{\\partial a_{ij}^L}{\\partial z_{k\\ell}^L} = \\frac{\\frac{\\partial}{\\partial z_{k\\ell}^L}\\left(\\exp{z_{ij}^L}\\right) \\cdot \\left(\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}\\right) - \\left(\\exp{z_{ij}^L}\\right) \\cdot \\frac{\\partial}{\\partial z_{k\\ell}^L}\\left(\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}\\right)}{\\left(\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}\\right)^2} $$\n",
    "\n",
    "First, let's focus on the first derivative in the numerator,\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial z_{k\\ell}^L}\\left(\\exp{z_{ij}^L}\\right) $$\n",
    "\n",
    "This will always be zero, unless $i = k$ and $j = \\ell$. Then, the derivative will be $\\exp{z_{k\\ell}^L}$. We will represent this using an indicator variable,\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial z_{k\\ell}^L}\\left(\\exp{z_{ij}^L}\\right) = \\exp{z_{k\\ell}^L} \\cdot \\mathbb{1}_{i=k, j=\\ell}$$\n",
    "\n",
    "Next, let's consider the last derivative in the numerator,\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial z_{k\\ell}^L} \\left(\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}\\right) $$\n",
    "\n",
    "Remember, $k \\in \\left\\{1, \\ldots, n^L\\right\\}$ and this is also the range of $m$. Therefore, no matter what $k$ is, as long as $j = \\ell$, the derivative will be $\\exp{z_{k\\ell}^L}$. Otherwise, the derivative will be zero. We will represent this using an indicator variable,\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial z_{k\\ell}^L} \\left(\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}\\right) = \\exp{z_{k\\ell}^L} \\cdot \\mathbb{1}_{j=\\ell} $$\n",
    "\n",
    "Putting this all together,\n",
    "\n",
    "$$ \\frac{\\partial a_{ij}^L}{\\partial z_{k\\ell}^L} = \\frac{\\left(\\exp{z_{k\\ell}^L} \\cdot \\mathbb{1}_{i=k, j=\\ell}\\right) \\cdot \\left(\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}\\right) - \\left(\\exp{z_{ij}^L}\\right) \\cdot \\left(\\exp{z_{k\\ell}^L} \\cdot \\mathbb{1}_{j=\\ell}\\right)}{\\left(\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}\\right)^2} $$\n",
    "\n",
    "With some simplification,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial a_{ij}^L}{\\partial z_{k\\ell}^L} &= \\exp{z_{k\\ell}^L} \\cdot \\left(\\frac{\\left(\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}\\right) \\cdot \\mathbb{1}_{i=k} - \\exp{z_{ij}^L}}{\\left(\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}\\right)^2}\\right) \\cdot \\mathbb{1}_{j=\\ell} \\\\\n",
    "    &= \\frac{\\exp{z_{k\\ell}^L}}{\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}} \\cdot \\left(\\frac{\\left(\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}\\right) \\cdot \\mathbb{1}_{i=k} - \\exp{z_{ij}^L}}{\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}}\\right) \\cdot \\mathbb{1}_{j=\\ell} \\\\\n",
    "    &= \\frac{\\exp{z_{k\\ell}^L}}{\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}} \\cdot \\left(\\mathbb{1}_{i=k} - \\frac{\\exp{z_{ij}^L}}{\\sum_{m=1}^{n^L} \\exp{z_{mj}^L}}\\right) \\cdot \\mathbb{1}_{j=\\ell} \\\\\n",
    "    &= a_{k\\ell}^L \\cdot \\left(\\mathbb{1}_{i=k} - a_{ij}^L\\right) \\cdot \\mathbb{1}_{j=\\ell}\n",
    "\\end{align*}\n",
    "\n",
    "If you are more familiar with piecewise notation, this is equivalent to,\n",
    "\n",
    "$$ \\frac{\\partial a_{ij}^L}{\\partial z_{k\\ell}^L} = \\begin{cases}\n",
    "    a_{ij}^L \\cdot \\left(1 - a_{ij}^L\\right) & \\mathrm{if}\\ i = k\\ \\mathrm{and}\\ j = \\ell \\\\\n",
    "    - a_{kj}^L \\cdot a_{ij}^L & \\mathrm{if}\\ i \\neq k\\ \\mathrm{and}\\ j = \\ell \\\\\n",
    "    0 & \\mathrm{if}\\ i \\neq k\\ \\mathrm{and}\\ j \\neq \\ell \\\\\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Gradient\n",
    "\n",
    "Using the results from the previous section, we can easily construct a $4$-dimensional tensor of all the derivatives. However, it is important to note that the gradients are all zero when $j \\neq \\ell$. Intuitively, this is stating that the results from one batch are completely unrelated to another batch (which should be clear by design). Therefore, we will simply drop this component of our tensor and we are left with a $3$-dimensional tensor (which is easier to visualize and define). \n",
    "\n",
    "Using the definition from the previous section, we can simplify it a little,\n",
    "\n",
    "$$ \\frac{\\partial a_{ij}^L}{\\partial z_{kj}^L} = \\begin{cases}\n",
    "    a_{ij}^L \\cdot \\left(1 - a_{ij}^L\\right) & \\mathrm{if}\\ i = k \\\\\n",
    "    - a_{kj}^L \\cdot a_{ij}^L & \\mathrm{if}\\ i \\neq k\n",
    "\\end{cases} $$\n",
    "\n",
    "Let's consider what this matrix looks like for a single batch. That is, assume that $j=1$ and the $b=1$. We can simplify our result even further by dropping the $j$ index altogether,\n",
    "\n",
    "$$ \\frac{\\partial a_{i}^L}{\\partial z_{k}^L} = \\begin{cases}\n",
    "    a_{i}^L \\cdot \\left(1 - a_{i}^L\\right) & \\mathrm{if}\\ i = k \\\\\n",
    "    - a_{k}^L \\cdot a_{i}^L & \\mathrm{if}\\ i \\neq k\n",
    "\\end{cases} $$\n",
    "\n",
    "Now we can construct this matrix (remember, we are assuming $j=1$ and $b=1$),\n",
    "\n",
    "$$ \\frac{\\partial A^L}{\\partial Z^L} = \\begin{pmatrix}\n",
    "    a_1^L \\left(1 - a_1^L\\right) & -a_2^L a_1^L & \\cdots & -a_{n^L}^L a_1^L \\\\\n",
    "    -a_1^L a_2^L & a_2^L \\left(1 - a_2^L\\right) & \\cdots & -a_{n^L}^L a_2^L \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    -a_1^L a_{n^L}^L & -a_2^L a_{n^L}^L & \\cdots & a_{n^L}^L \\left(1 - a_{n^L}^L\\right)\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "If our assumptions do not hold (which is definitely true for batch updates), this matrix simply duplicates into the third dimension for every $j \\in \\left\\{1, \\ldots, b\\right\\}$. That is,\n",
    "\n",
    "$$ \\left(\\frac{\\partial A^L}{\\partial Z^L}\\right)_{::j} = \\begin{pmatrix}\n",
    "    a_{1j}^L \\left(1 - a_{1j}^L\\right) & -a_{2j}^L a_{1j}^L & \\cdots & -a_{n^Lj}^L a_{1j}^L \\\\\n",
    "    -a_{1j}^L a_{2j}^L & a_{2j}^L \\left(1 - a_{2j}^L\\right) & \\cdots & -a_{n^Lj}^L a_{2j}^L \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    -a_{1j}^L a_{n^Lj}^L & -a_{2j}^L a_{n^Lj}^L & \\cdots & a_{n^Lj}^L \\left(1 - a_{n^Lj}^L\\right)\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "(Where the subscript $::j$ for the gradient represents taking the $j$ matrix from the third dimension.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log-Likelihood Multiclass Function\n",
    "\n",
    "Now we can consider the categorical cross-entropy function. The categorical cross-entropy function is defined for a prediction vector $A^L$ and an expected vector $Y$ both with dimensions $n^L \\times b$ (the output of a neural network with $n^L$ neurons in the final layer $L$ for a batch of size $b$) as the following:\n",
    "\n",
    "$$ \\mathrm{NLLM}(A^L, Y) = - \\sum_{i=1}^{n^L}\\sum_{j=1}^{b} y_{ij} \\cdot \\log a_{ij}^L $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log-Likelihood Multi-Class Gradient\n",
    "\n",
    "#### Single Element Derivative\n",
    "\n",
    "Let's consider the derivative of the loss with respect to a single input $a_{k\\ell}^L$. That is, we are looking for $\\partial \\mathrm{NLLM} / \\partial a_{k\\ell}^L $. Writing this in terms of the negative log-likelihood function,\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial a_{k\\ell}^L}\\left(- \\sum_{i=1}^{n^L}\\sum_{j=1}^{b} y_{ij} \\cdot \\log a_{ij}^L\\right) $$\n",
    "\n",
    "This derivative is much easier to compute than the previous,\n",
    "\n",
    "$$ \\frac{\\partial \\mathrm{NLLM}}{\\partial a_{k\\ell}^L} = -\\frac{y_{k\\ell}}{a_{k\\ell}^L} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Gradient\n",
    "\n",
    "Using the results from the previous section, we can construct a $2$ dimensional matrix containing the gradient with respect to each input\n",
    "\n",
    "$$ \\frac{\\partial \\mathrm{NLLM}}{\\partial A^L} = \\begin{pmatrix}\n",
    "    -y_{11} / a_{11}^L & \\cdots & -y_{1 b} / a_{1 b}^L \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    -y_{n^L 1} / a_{n^L 1}^L & \\cdots & -y_{n^L b} / a_{n^L b}^L\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything Together\n",
    "\n",
    "We have a $3$-dimensional tensor and a $2$-dimensional matrix, how can we combine these to get the overall gradient of negative log-likelihood multi-class function with respect to the inputs to softmax for a batch? Remember, we are looking for,\n",
    "\n",
    "$$ \\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L} =\\frac{\\partial A^L}{\\partial Z^L}\\frac{\\partial \\mathrm{NLLM}}{\\partial A^L} $$\n",
    "\n",
    "To make this simpler, we can consider the case when we only have a single batch. That is, $b=1$ and $j = 1$. Then, this reduces to the matrix operation,\n",
    "\n",
    "$$ \\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L} = \\begin{pmatrix}\n",
    "    a_1^L \\left(1 - a_1^L\\right) & -a_2^L a_1^L & \\cdots & -a_{n^L}^L a_1^L \\\\\n",
    "    -a_1^L a_2^L & a_2^L \\left(1 - a_2^L\\right) & \\cdots & -a_{n^L}^L a_2^L \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    -a_1^L a_{n^L}^L & -a_2^L a_{n^L}^L & \\cdots & a_{n^L}^L \\left(1 - a_{n^L}^L\\right)\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "    -y_{1} / a_{1}^L \\\\\n",
    "    \\vdots \\\\\n",
    "    -y_{n^L} / a_{n^L}^L\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "If we have more than one batch, both of these matrices increase in a single dimension. To compute this weird product, we just consider layer-by-layer. That is,\n",
    "\n",
    "$$ \\left(\\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L}\\right)_{:j} = \\begin{pmatrix}\n",
    "    a_{1j}^L \\left(1 - a_{1j}^L\\right) & -a_{2j}^L a_{1j}^L & \\cdots & -a_{n^Lj}^L a_{1j}^L \\\\\n",
    "    -a_{1j}^L a_{2j}^L & a_{2j}^L \\left(1 - a_{2j}^L\\right) & \\cdots & -a_{n^Lj}^L a_{2j}^L \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    -a_{1j}^L a_{n^Lj}^L & -a_{2j}^L a_{n^Lj}^L & \\cdots & a_{n^Lj}^L \\left(1 - a_{n^Lj}^L\\right)\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "    -y_{1j} / a_{1j}^L \\\\\n",
    "    \\vdots \\\\\n",
    "    -y_{n^Lj} / a_{n^Lj}^L\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "(Where the subscript $:j$ for the gradient represents taking the $j$ column from the matrix.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Simplification\n",
    "\n",
    "These matrices we have defined (and their product) are likely never computed directly in practice. Instead, relying on the fact that $Y$ is one-hot encoded, this whole result simplifies into a very simple closed form.\n",
    "\n",
    "For now, consider the case with a single batch. Remember, we want to compute the following product,\n",
    "\n",
    "$$ \\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L} = \\begin{pmatrix}\n",
    "    a_1^L \\left(1 - a_1^L\\right) & -a_2^L a_1^L & \\cdots & -a_{n^L}^L a_1^L \\\\\n",
    "    -a_1^L a_2^L & a_2^L \\left(1 - a_2^L\\right) & \\cdots & -a_{n^L}^L a_2^L \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    -a_1^L a_{n^L}^L & -a_2^L a_{n^L}^L & \\cdots & a_{n^L}^L \\left(1 - a_{n^L}^L\\right)\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "    -y_{1} / a_{1}^L \\\\\n",
    "    \\vdots \\\\\n",
    "    -y_{n^L} / a_{n^L}^L\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "The result should be an $n^L \\times 1$ vector. Let's take it element-by-element. The first element of this vector will be,\n",
    "\n",
    "$$ \\left(\\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L}\\right)_1 = \\begin{pmatrix} a_1^L \\left(1 - a_1^L\\right) & -a_2^L a_1^L & \\cdots & -a_{n^L}^L a_1^L \\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "    -y_{1} / a_{1}^L \\\\\n",
    "    \\vdots \\\\\n",
    "    -y_{n^L} / a_{n^L}^L\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "Writing this out as a continued sum,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left(\\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L}\\right)_1 &= \\left(a_1^L \\left(1 - a_1^L\\right)\\right) \\left(-\\frac{y_{1}}{a_{1}^L}\\right) + \\left(-a_2^L a_1^L\\right) \\left(-\\frac{y_{2}}{a_{2}^L}\\right) + \\ldots + \\left(-a_{n^L}^L a_1^L\\right) \\left(-\\frac{y_{n^L}}{a_{n^L}^L}\\right) \\\\\n",
    "    &= y_1 a_1^L - y_1 + y_2 a_1^L + \\ldots + y_{n^L} a_1^L \\\\\n",
    "    &= a_1^L \\cdot \\left(y_1 + y_2 + \\ldots + y_{n^L}\\right) - y_1\n",
    "\\end{align*}\n",
    "\n",
    "Since we assume $Y$ is one-hot encoded, $\\sum_{i = 1}^n y_i = 1$. Thus,\n",
    "\n",
    "$$ \\left(\\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L}\\right)_1 = a_1^L - y_1 $$\n",
    "\n",
    "In complete matrix form, the gradient we are looking for is,\n",
    "\n",
    "$$ \\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L} = \\begin{pmatrix}\n",
    "    a_1^L - y_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    a_{n^L}^L - y_{n^L}\n",
    "    \\end{pmatrix} $$\n",
    "    \n",
    "If we do not have a single batch, this result just expands into another dimension. That is,\n",
    "\n",
    "$$ \\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L} = \\begin{pmatrix}\n",
    "    a_{11}^L - y_{11} & \\cdots & a_{1b}^L - y_{1b} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{n^L1}^L - y_{n^L 1} & \\cdots & a_{n^L b}^L - y_{n^L b}\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "We will use this simple result to confirm that I have constructed the matrices correctly and multiply them together correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerically\n",
    "\n",
    "Now this is all well and good, but how do we construct this $3$-dimensional tensor in practice? And then how do we multiply it with a $2$-dimensional matrix? We don't want to use a bunch of `for`-loops in Python (which would be a major performance-killer). This is where Einstein summation notation comes into play. If we can construct this matrix using a set of matrix prducts and sums (along arbitrary axes), then the `np.einsum` function can (more) efficiently compute the result using the C-compiled backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Environment\n",
    "\n",
    "Let's set up the testing environment to make sure I am getting the proper results. Consider a situtation involving classification into three categories for a batch of two data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, b = 3, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted classifications are,\n",
    "\n",
    "$$ A^L = \\begin{pmatrix}\n",
    "    0.2 & 0.5 \\\\\n",
    "    0.5 & 0.1 \\\\\n",
    "    0.3 & 0.4\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "The actual classifications are,\n",
    "\n",
    "$$ Y = \\begin{pmatrix}\n",
    "    1.0 & 0.0 \\\\\n",
    "    0.0 & 0.0 \\\\\n",
    "    0.0 & 1.0\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0.2, 0.5],\n",
    "              [0.5, 0.1],\n",
    "              [0.3, 0.4]])\n",
    "\n",
    "Y = np.array([[1.0, 0.0],\n",
    "              [0.0, 0.0],\n",
    "              [0.0, 1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this data, let's try to construct the tensor for $\\partial A^L / \\partial Z^L$. By hand, the result should be,\n",
    "\n",
    "$$ \\left(\\frac{A^L}{Z^L}\\right)_1 = \\begin{pmatrix}\n",
    "    0.2 \\cdot (1 - 0.2) & -0.5 \\cdot 0.2 & -0.3 \\cdot 0.2 \\\\\n",
    "    -0.2 \\cdot 0.5 & 0.5 \\cdot (1 - 0.5) & -0.3 \\cdot 0.5 \\\\\n",
    "    -0.2 \\cdot 0.3 & -0.5 \\cdot 0.3 & 0.3 \\cdot (1 - 0.3)\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    0.16 & -0.10 & -0.06 \\\\\n",
    "    -0.10 & 0.25 & -0.15 \\\\\n",
    "    -0.06 & -0.15 & 0.21\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "\n",
    "$$ \\left(\\frac{A^L}{Z^L}\\right)_2 = \\begin{pmatrix}\n",
    "    0.5 \\cdot (1 - 0.5) & -0.1 \\cdot 0.5 & -0.4 \\cdot 0.5 \\\\\n",
    "    -0.5 \\cdot 0.1 & 0.1 \\cdot (1 - 0.1) & -0.4 \\cdot 0.1 \\\\\n",
    "    -0.5 \\cdot 0.4 & -0.1 \\cdot 0.4 & 0.4 \\cdot (1 - 0.4)\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    0.25 & -0.05 & -0.20 \\\\\n",
    "    -0.05 & 0.09 & -0.04 \\\\\n",
    "    -0.20 & -\n",
    "    0.04 & 0.24\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAdZ = np.array([[[ 0.16, -0.10, -0.06],\n",
    "                  [-0.10,  0.25, -0.15],\n",
    "                  [-0.06, -0.15,  0.21]],\n",
    "                \n",
    "                 [[ 0.25, -0.05, -0.20],\n",
    "                  [-0.05,  0.09, -0.04],\n",
    "                  [-0.20, -0.04,  0.24]]])\n",
    "\n",
    "# Since I want the batch dimension last, swap axes\n",
    "dAdZ = dAdZ.swapaxes(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the data, we can also construct the matrix for $\\partial \\mathrm{NLLM} / \\partial A^L$. By hand, this comes out to be,\n",
    "\n",
    "$$ \\frac{\\partial \\mathrm{NLLM}}{\\partial A^L} = \\begin{pmatrix}\n",
    "    -1.0 / 0.2 & -0.0 / 0.5 \\\\\n",
    "    -0.0 / 0.5 & -0.0 / 0.1 \\\\\n",
    "    -0.0 / 0.3 & -1.0 / 0.4\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    -5.0 & 0.0 \\\\\n",
    "    0.0 & 0.0 \\\\\n",
    "    0.0 & -2.5\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdA = np.array([[-5.0,  0.0],\n",
    "                 [ 0.0,  0.0],\n",
    "                 [ 0.0, -2.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We multiply these matrices together to get the final $\\partial \\mathrm{NLLM} / \\partial Z^L$,\n",
    "\n",
    "$$ \\left(\\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L}\\right)_1 = \\begin{pmatrix}\n",
    "    0.16 & -0.10 & -0.06 \\\\\n",
    "    -0.10 & 0.25 & -0.15 \\\\\n",
    "    -0.06 & -0.15 & 0.21\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "    -5.0 \\\\\n",
    "    0.0 \\\\\n",
    "    0.0\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    -0.8 \\\\\n",
    "    -0.5 \\\\\n",
    "    -0.3\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "$$ \\left(\\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L}\\right)_2 =  \\begin{pmatrix}\n",
    "    0.25 & -0.05 & -0.20 \\\\\n",
    "    -0.05 & 0.09 & -0.04 \\\\\n",
    "    -0.20 & -0.04 & 0.24\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "    0.0 \\\\\n",
    "    0.0 \\\\\n",
    "    -2.5\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    -0.5 \\\\\n",
    "    -0.1 \\\\\n",
    "    -0.6\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "Putting the two results together,\n",
    "\n",
    "$$ \\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L} = \\begin{pmatrix}\n",
    "    -0.8 & 0.5 \\\\\n",
    "    0.5 & 0.1 \\\\\n",
    "    0.3 & -0.6\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8],\n",
       "       [ 0.5],\n",
       "       [ 0.3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAdZ[:,:,0] @ dLdA[:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5],\n",
       "       [ 0.1],\n",
       "       [-0.6]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAdZ[:,:,1] @ dLdA[:,1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the simplified representation, we confirm our math is correct,\n",
    "\n",
    "$$ \\frac{\\partial \\mathrm{NLLM}}{\\partial Z^L} = \\begin{pmatrix}\n",
    "    0.2 & 0.5 \\\\\n",
    "    0.5 & 0.1 \\\\\n",
    "    0.3 & 0.4\n",
    "\\end{pmatrix} - \\begin{pmatrix}\n",
    "    1.0 & 0.0 \\\\\n",
    "    0.0 & 0.0 \\\\\n",
    "    0.0 & 1.0\n",
    "\\end{pmatrix} =  \\begin{pmatrix}\n",
    "    -0.8 & 0.5 \\\\\n",
    "    0.5 & 0.1 \\\\\n",
    "    0.3 & -0.6\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8,  0.5],\n",
       "       [ 0.5,  0.1],\n",
       "       [ 0.3, -0.6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A - Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Gradient\n",
    "\n",
    "First, let's show how to construct the tensor $\\partial A^L / \\partial Z^L$ using nested `for`-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAdZ_ = np.zeros((n, n, b))\n",
    "\n",
    "for i in range(n):    \n",
    "    for j in range(n):        \n",
    "        for k in range(b):            \n",
    "            if i == j:\n",
    "                dAdZ_[i, j, k] = A[i, k] * (1 - A)[i, k]\n",
    "            else:\n",
    "                dAdZ_[i, j, k] = -A[j, k] * A[i, k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.16, -0.1 , -0.06],\n",
       "        [-0.1 ,  0.25, -0.15],\n",
       "        [-0.06, -0.15,  0.21]],\n",
       "\n",
       "       [[ 0.25, -0.05, -0.2 ],\n",
       "        [-0.05,  0.09, -0.04],\n",
       "        [-0.2 , -0.04,  0.24]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAdZ_.swapaxes(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! So that is the matrix we are trying to create. However, we can't use conditionals in Einstein summation. We can construct the diagonal of the matrix using the following set of `for`-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAdZ_diag = np.zeros((n, n, b))\n",
    "\n",
    "for i in range(n):    \n",
    "    for j in range(n):        \n",
    "        for k in range(b):            \n",
    "            dAdZ_diag[i, j, k] = A[j, k] * (1 - A)[j, k] * np.eye(n)[j, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.16, 0.  , 0.  ],\n",
       "        [0.  , 0.25, 0.  ],\n",
       "        [0.  , 0.  , 0.21]],\n",
       "\n",
       "       [[0.25, 0.  , 0.  ],\n",
       "        [0.  , 0.09, 0.  ],\n",
       "        [0.  , 0.  , 0.24]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAdZ_diag.swapaxes(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so that is the diagonal we wish to use in our matrix. Let's see if we can encode this in Einstein summation notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAdZ_diag = np.einsum('jk,jk,ji->ijk', A, 1 - A, np.eye(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.16, 0.  , 0.  ],\n",
       "        [0.  , 0.25, 0.  ],\n",
       "        [0.  , 0.  , 0.21]],\n",
       "\n",
       "       [[0.25, 0.  , 0.  ],\n",
       "        [0.  , 0.09, 0.  ],\n",
       "        [0.  , 0.  , 0.24]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAdZ_diag.swapaxes(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! So we now have the diagonal of our matrix. Let's see if we can construct the rest of the matrix using `for`-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAdZ_offdiag = np.zeros((n, n, b))\n",
    "\n",
    "for i in range(n):    \n",
    "    for j in range(n):        \n",
    "        for k in range(b):            \n",
    "            dAdZ_offdiag[i, j, k] = -A[j, k] * A[i, k] * (1 - np.eye(n))[j, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.  , -0.1 , -0.06],\n",
       "        [-0.1 , -0.  , -0.15],\n",
       "        [-0.06, -0.15, -0.  ]],\n",
       "\n",
       "       [[-0.  , -0.05, -0.2 ],\n",
       "        [-0.05, -0.  , -0.04],\n",
       "        [-0.2 , -0.04, -0.  ]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAdZ_offdiag.swapaxes(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so that is the off-diagonal we wish to use in our matrix. Let's see if we can encode this in Einstein summation notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAdZ_offdiag = np.einsum('jk,ik,ji->ijk', -A, A, 1 - np.eye(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.  , -0.1 , -0.06],\n",
       "        [-0.1 ,  0.  , -0.15],\n",
       "        [-0.06, -0.15,  0.  ]],\n",
       "\n",
       "       [[ 0.  , -0.05, -0.2 ],\n",
       "        [-0.05,  0.  , -0.04],\n",
       "        [-0.2 , -0.04,  0.  ]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAdZ_offdiag.swapaxes(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful! Now, we can just add these two together and be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAdZ_ = dAdZ_diag + dAdZ_offdiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.16, -0.1 , -0.06],\n",
       "        [-0.1 ,  0.25, -0.15],\n",
       "        [-0.06, -0.15,  0.21]],\n",
       "\n",
       "       [[ 0.25, -0.05, -0.2 ],\n",
       "        [-0.05,  0.09, -0.04],\n",
       "        [-0.2 , -0.04,  0.24]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAdZ_.swapaxes(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the methods to see which is more performant (and by how much). We start with the naive approach using three nested `for`-loops and a conditional statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dAdZ_loop_conditional(A):\n",
    "    \"\"\"Constructs the dAdZ gradient tensor using nested for loops\n",
    "    and a conditional statement.\n",
    "    \n",
    "    Args:\n",
    "        A (ndarray): An n by b matrix of activations for a batch \n",
    "            of size b.\n",
    "            \n",
    "    Returns:\n",
    "        ndarray: An n by n by b tensor representing the gradient\n",
    "            of softmax activation with respect to inputs.\n",
    "            \n",
    "    \"\"\"\n",
    "    n, b = A.shape\n",
    "    \n",
    "    dAdZ = np.zeros((n, n, b))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            for k in range(b):\n",
    "                if i == j:\n",
    "                    dAdZ[i, j, k] = A[i, k] * (1 - A)[i, k]\n",
    "                else:\n",
    "                    dAdZ[i, j, k] = -A[j, k] * A[i, k]\n",
    "    \n",
    "    return dAdZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.6 µs ± 1.08 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dAdZ_loop_conditional(A).swapaxes(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we consider the approach where we remove the conditional, but keep the `for`-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dAdZ_loop_nonconditional(A):\n",
    "    \"\"\"Constructs the dAdZ gradient tensor using nested for loops\n",
    "    alone.\n",
    "    \n",
    "    Args:\n",
    "        A (ndarray): An n by b matrix of activations for a batch \n",
    "            of size b.\n",
    "            \n",
    "    Returns:\n",
    "        ndarray: An n by n by b tensor representing the gradient\n",
    "            of softmax activation with respect to inputs.\n",
    "            \n",
    "    \"\"\"\n",
    "    n, b = A.shape\n",
    "    \n",
    "    dAdZ_diag = np.zeros((n, n, b))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            for k in range(b):            \n",
    "                dAdZ_diag[i, j, k] = A[j, k] * (1 - A)[j, k] * np.eye(n)[j, i]\n",
    "\n",
    "    dAdZ_offdiag = np.zeros((n, n, b))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            for k in range(b):            \n",
    "                dAdZ_offdiag[i, j, k] = (-A)[j, k] * A[i, k] * (1 - np.eye(n))[j, i]\n",
    "    \n",
    "    return dAdZ_diag + dAdZ_offdiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234 µs ± 127 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dAdZ_loop_nonconditional(A).swapaxes(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we consider the `np.einsum` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dAdZ_einsum(A):\n",
    "    \"\"\"Constructs the dAdZ gradient tensor using np.einsum.\n",
    "    \n",
    "    Args:\n",
    "        A (ndarray): An n by b matrix of activations for a batch \n",
    "            of size b.\n",
    "            \n",
    "    Returns:\n",
    "        ndarray: An n by n by b tensor representing the gradient\n",
    "            of softmax activation with respect to inputs.\n",
    "            \n",
    "    \"\"\"\n",
    "    return np.einsum('jk,jk,ji->ijk', A, 1 - A, np.eye(n)) \\\n",
    "           + np.einsum('jk,ik,ji->ijk', -A, A, 1 - np.eye(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.6 µs ± 1.24 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dAdZ_einsum(A).swapaxes(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, our work seems to have paid off--slightly. Although `np.einsum` is faster, it doesn't improve by that much. In practice, I'd probably construct the matrix using the `for`-loops with the conditional. It might be possible to reduce to a single call to `np.einsum` by making use of summation indices, but it is not obvious to me how to construct that.\n",
    "\n",
    "### Negative Log-Likelihood Multi-Class Gradient\n",
    "\n",
    "We can now construct the matrix $\\partial \\mathrm{NLLM} / \\partial A^L$. This matrix is actually really easy to create. No fancy `np.einsum` is necessary; we can simply using broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdA_ = -Y / A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5. , -0. ],\n",
       "       [-0. , -0. ],\n",
       "       [-0. , -2.5]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdA_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything Together\n",
    "\n",
    "Now that we have both components of our gradient $\\partial \\mathrm{NLLM} / \\partial Z^L$, we need to combine them. However, since we are trying to multiply a $3$-dimensional matrix with a $2$-dimensional matrix, we need to come up with a specialized method.\n",
    "\n",
    "First, we can accomplish the desired behavior in a series of nested `for`-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdZ_ = np.zeros((n, b))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(b):\n",
    "        \n",
    "        total = 0\n",
    "        \n",
    "        for k in range(n):\n",
    "            total += dAdZ[i, k, j] * dLdA[k, j]\n",
    "        \n",
    "        dLdZ_[i, j] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8,  0.5],\n",
       "       [ 0.5,  0.1],\n",
       "       [ 0.3, -0.6]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdZ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if we can encode this using `np.einsum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdZ_ = np.einsum('ikj,kj->ij', dAdZ, dLdA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8,  0.5],\n",
       "       [ 0.5,  0.1],\n",
       "       [ 0.3, -0.6]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdZ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue the performance discussion, let's compare the loop version with the `np.einsum` version. Let's first consider the loop version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dLdZ_loop(dAdZ, dLdA):\n",
    "    \"\"\"Constructs the gradient of negative log-likelihood multi-class loss \n",
    "    with respect to the inputs of softmax activation for a single batch\n",
    "    using nested loops.\n",
    "        \n",
    "    Args:\n",
    "        dAdZ (ndarray): An n by n by b tensor representing the gradient of\n",
    "            softmax activations with respect to softmax inputs for a batch\n",
    "            of size b.\n",
    "        dLdA (ndarray): An n by b matrix representing the gradient of\n",
    "            negative log-likelihood multi-class loss with respect to the\n",
    "            inputs of the loss for a batch of size b.\n",
    "            \n",
    "    Returns:\n",
    "        ndarray: An n by b matrix representing the gradient of NLLM loss\n",
    "            with respect to inputs of softmax for a batch of size b.\n",
    "            \n",
    "    \"\"\"\n",
    "    n, b = dLdA.shape\n",
    "    \n",
    "    dLdZ = np.zeros((n, b))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(b):\n",
    "\n",
    "            total = 0\n",
    "\n",
    "            for k in range(n):\n",
    "                total += dAdZ[i, k, j] * dLdA[k, j]\n",
    "\n",
    "            dLdZ[i, j] = total\n",
    "    \n",
    "    return dLdZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.6 µs ± 6.53 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dLdZ_loop(dAdZ, dLdA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's consider the `np.einsum` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dLdZ_einsum(dAdZ, dLdA):\n",
    "    \"\"\"Constructs the gradient of negative log-likelihood multi-class loss \n",
    "    with respect to the inputs of softmax activation for a single batch\n",
    "    using np.einsum.\n",
    "        \n",
    "    Args:\n",
    "        dAdZ (ndarray): An n by n by b tensor representing the gradient of\n",
    "            softmax activations with respect to softmax inputs for a batch\n",
    "            of size b.\n",
    "        dLdA (ndarray): An n by b matrix representing the gradient of\n",
    "            negative log-likelihood multi-class loss with respect to the\n",
    "            inputs of the loss for a batch of size b.\n",
    "            \n",
    "    Returns:\n",
    "        ndarray: An n by b matrix representing the gradient of NLLM loss\n",
    "            with respect to inputs of softmax for a batch of size b.\n",
    "            \n",
    "    \"\"\"\n",
    "    return np.einsum('ikj,kj->ij', dAdZ, dLdA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.22 µs ± 50.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dLdZ_einsum(dAdZ, dLdA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is where `np.einsum` really shines. This is clearly the better method to construct this weird matrix product."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
