{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy Feed-Forward Neural Network\n",
    "\n",
    "This notebook walks through the process of constructing a feed-forward neural network for multi-class classification solely using NumPy.\n",
    "\n",
    "## Layers\n",
    "\n",
    "For our neural network, we want to abstract away from individual neurons and focus on layers. Each element of the network will be defined by a certain layer.\n",
    "\n",
    "### Base Layer\n",
    "\n",
    "The abstract base layer ensures that all called methods by the `Sequential` model exist on the layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Base class for neural network layers.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we only need to worry about the `sgd_step` method as some layers won't need to update any weights because they don't have any (e.g. activation layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, lrate):\n",
    "        \"\"\"Some layers do not have weights to update on gradient descent steps.\"\"\"\n",
    "\n",
    "# Add this method to the Layer class\n",
    "Layer.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "This is the simplest layer that makes up the majority of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"A simple, fully-connected linear layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up this layer, we need to know the input and output dimensions ahead of time. Using this information, we randomly initialize the weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, m, n):\n",
    "        \"\"\"Initializes the layer based on input and output dimensions. \n",
    "\n",
    "        Note: Kernel is initialized using normal distribution with mean 0 and \n",
    "        variance 1 / m. All biases are initialized to zero.\n",
    "\n",
    "        Args:\n",
    "            m (int): Number of inputs to the layer.\n",
    "            n (int): Number of outputs from the layer.\n",
    "\n",
    "        \"\"\"\n",
    "        self.m, self.n = m, n\n",
    "\n",
    "        self.W0 = np.zeros((n, 1))\n",
    "        self.W = np.random.normal(0, np.sqrt(1 / m), (m, n))\n",
    "\n",
    "# Add this method to the Linear layer class\n",
    "Linear.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method will compute the output of the layer given a set of $m$ inputs from the previous layer for a batch of size $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, A):\n",
    "        \"\"\"Computes the forward pass through the linear network for a batch.\n",
    "\n",
    "        Args:\n",
    "            A (ndarray): An m by b matrix representing the m activations from the\n",
    "                previous layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the result of passing the \n",
    "                activations through the network layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "\n",
    "        return self.W.T @ self.A + self.W0\n",
    "\n",
    "# Add this method to the Linear layer class\n",
    "Linear.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method will compute the gradient of the loss with respect to the inputs to the layer for a batch of size $b$. Note: There is an implicit sum over all $b$ in the `dLdW` calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdZ):\n",
    "        \"\"\"Uses the gradient of loss with respect to outputs of the layer for a \n",
    "        batch to update the sum of gradients of the loss with respect to the \n",
    "        weights for the entire batch. Also returns the gradient of the loss with \n",
    "        respect to the inputs to the layer for a batch.\n",
    "\n",
    "        Args:\n",
    "            dLdZ (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the layer outputs for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An m by b matrix representing the gradient of the loss with \n",
    "                respect to the inputs to the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.dLdW = self.A @ dLdZ.T  # Implicit sum over all b\n",
    "        self.dLdW0 = np.sum(dLdZ, axis=1, keepdims=True)\n",
    "\n",
    "        return self.W @ dLdZ\n",
    "\n",
    "# Add this method to the Linear layer class\n",
    "Linear.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need a method to update the weight matrices using the current weight gradients for a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, lrate):\n",
    "        \"\"\"Performs a single step of gradient descent to update the weights for a \n",
    "        single batch of points.\n",
    "\n",
    "        Args:\n",
    "            lrate (float): A learning rate to scale the gradient for the update.\n",
    "\n",
    "        \"\"\"\n",
    "        self.W = self.W - lrate * self.dLdW\n",
    "        self.W0 = self.W0 - lrate * self.dLdW0\n",
    "        \n",
    "# Add this method to the Linear layer class\n",
    "Linear.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent Activation Layer\n",
    "\n",
    "This layer encapsulates the hyperbolic tangent activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    \"\"\"Hyperbolic tangent activation layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method take a preactivation from the previous layer and computes the activation using the hyperbolic tangent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the output of the hyperbolic tangent activation layer.\n",
    "\n",
    "        Args:\n",
    "            Z (ndarray): An n by b matrix representing the input pre-activations\n",
    "                of the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the output of the layer after\n",
    "                using the hyperbolic tangent activation on all inputs for a batch\n",
    "                of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = np.tanh(Z)\n",
    "\n",
    "        return self.A\n",
    "\n",
    "# Add this method to the Tanh layer class\n",
    "Tanh.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method computes the gradient of the loss with respect to the inputs to the activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return (1 - self.A ** 2) * dLdA\n",
    "\n",
    "# Add this method to the Tanh layer class\n",
    "Tanh.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has no weights to update. Therefore, no `sgd_step` function is required.\n",
    "\n",
    "### Rectified Linear Unit Activation Layer\n",
    "\n",
    "This layer encapsulates the rectified linear unit activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"Rectified linear unit layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method take a preactivation from the previous layer and computes the activation using the relu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the output of the rectified linear unit layer.\n",
    "        \n",
    "        Args:\n",
    "            Z (ndarray): An n by b matrix representing the input pre-activations\n",
    "                of the layer for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the output of the layer after\n",
    "                using the rectified linear activation on all inputs for a batch\n",
    "                of size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.A = np.maximum(0, Z)\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "# Add this method to the ReLU layer class\n",
    "ReLU.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method computes the gradient of the loss with respect to the inputs to the activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.sign(self.A) * dLdA\n",
    "    \n",
    "# Add this method to the ReLU layer class\n",
    "ReLU.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has no weights to update. Therefore, no `sgd_step` function is required.\n",
    "\n",
    "### Softmax Activation Layer\n",
    "\n",
    "This layer encapsulates the softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Layer):\n",
    "    \"\"\"Softmax activation layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method take a preactivation from the previous layer and computes the activation using the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the softmax activation given the inputs from the previous\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            Z (ndarray): An n by b matrix representing the inputs to the softmax\n",
    "                layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix of outputs from softmax for a batch of \n",
    "                size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = np.exp(Z) / np.sum(np.exp(Z), axis=0, keepdims=True)\n",
    "        \n",
    "        return self.A\n",
    "\n",
    "# Add this method to the SoftMax layer class\n",
    "SoftMax.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method computes the gradient of the loss with respect to the inputs to the activation layer. Note that I *do not* assume that $\\partial \\mathrm{Loss} / \\partial Z^L$ is passed in directly. More information on how this works can be found in the 'Einstein Summation' notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "                \n",
    "        \"\"\"\n",
    "        n, _ = dLdA.shape\n",
    "        \n",
    "        dAdZ = np.einsum('jk,jk,ji->ijk', self.A, 1 - self.A, np.eye(n)) \\\n",
    "                + np.einsum('jk,ik,ji->ijk', -self.A, self.A, 1 - np.eye(n))\n",
    "        \n",
    "        return np.einsum('ikj,kj->ij', dAdZ, dLdA)\n",
    "\n",
    "# Add this method to the SoftMax layer class\n",
    "SoftMax.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make prediction a little easier, we also provide a method that will determine the classes of highest probability as returned from a softmax prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        \"\"\"Computes the index of maximum value given the softmax outputs from a\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            Ypred (ndarray): An n by b matrix representing the softmax outputs of a\n",
    "                layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: A 1 by b row vectors representing the indices of maximum value\n",
    "                for each output from a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.argmax(Ypred, axis=0)\n",
    "\n",
    "# Add this method to the SoftMax layer class\n",
    "SoftMax.class_fun = class_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has no weights to update. Therefore, no `sgd_step` function is required.\n",
    "\n",
    "### Negative Log-Likelihood Multi-Class Loss Layer\n",
    "\n",
    "This isn't really a layer, but it functions quite similarly to one. It will take predictions and actual labels and compute the categorical cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLL(Layer):\n",
    "    \"\"\"Negative log-likelihood loss layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method will compute the loss between predicted and actual labels using categorical cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Ypred, Y):\n",
    "        \"\"\"Computes the loss given the predicted and actual results.\n",
    "\n",
    "        Args:\n",
    "            Ypred (ndarray): An n by b matrix representing the predicted results\n",
    "                from the network for a batch of size b.\n",
    "            Y (ndarray): An n by b matrix representing the actual expected results\n",
    "                for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            float: A scalar representing the total loss for each of the outputs\n",
    "                in a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "\n",
    "        return -np.sum(self.Y * np.log(self.Ypred))\n",
    "\n",
    "# Add this method to the NLL layer class\n",
    "NLL.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method will compute the gradient of the loss with respect to the predicted outputs from the network. (Note: this is *not* in terms of the pre-activations, but the actual activations. To learn more about this, look at the 'Einstein Summation' notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self):\n",
    "        \"\"\"Computes the gradient of the loss with respect to predicted targets for\n",
    "        a single batch.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of loss with\n",
    "                respect to predicted targets for a batch of size b.\n",
    "                \n",
    "        \"\"\"\n",
    "        return -self.Y / self.Ypred\n",
    "\n",
    "# Add this method to the NLL layer class\n",
    "NLL.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has no weights to update. Therefore, no `sgd_step` function is required.\n",
    "\n",
    "## Model\n",
    "\n",
    "Now that we have all the components to make up a simple neural network, we can combine them together into a model.\n",
    "\n",
    "### Sequential Model\n",
    "\n",
    "This is the simplest type of model which just linearly stacks each layer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"A standard neural network model with linear stacked layers.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do anything with the model, we need to know what layers should be included and what loss should be used to compute gradient updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, layers, loss):\n",
    "        \"\"\"Initialize the layers and the loss for the network.\n",
    "        \n",
    "        Args:\n",
    "            layers (list of Layer): A list of layers to make up the linear\n",
    "                neural network.\n",
    "            loss (Layer): A final layer to use to compute the loss of the\n",
    "                neural network.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "\n",
    "# Add this method to the Sequential model class\n",
    "Sequential.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions with the network, we use the `forward` method. This passes the data through every layer and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Xt):\n",
    "        \"\"\"Predicts the output for a training input batch.\n",
    "        \n",
    "        Args:\n",
    "            Xt (ndarray): A d by b matrix of points to predict\n",
    "                with dimension d and batch size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: A 1 by b matrix representing the predicted\n",
    "                outputs of the neural network for a batch size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            Xt = layer.forward(Xt)\n",
    "            \n",
    "        return Xt\n",
    "\n",
    "# Add this method to the Sequential model class\n",
    "Sequential.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we will use stochastic gradient descent. Before we define the stochastic gradient descent training loop, we have to back-propogate the error throughout the layers of the network. To do this, we use the `backward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradients of the loss with respect to each weight\n",
    "        in the neural network to prepare for stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the\n",
    "                loss with respect to the outputs of the neural network for a\n",
    "                batch of size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers[::-1]:\n",
    "            dLdA = layer.backward(dLdA)\n",
    "\n",
    "# Add this method to the Sequential model class\n",
    "Sequential.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the error is propogated through all the layers, each layer can update their weight matrices. For a single step, this is achieved through the `sgd_step` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, lrate):\n",
    "        \"\"\"Runs a single update step on the weight matrices throughout the\n",
    "        neural network using stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            lrate (float): Learning rate for the update step.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.sgd_step(lrate)\n",
    "\n",
    "# Add this method to the Sequential model class\n",
    "Sequential.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop over the data applying many stochastic gradient descent update steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd(self, X, Y, iters=100, lrate=0.005):\n",
    "        \"\"\"Trains the neural network by running stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): A d by n matrix representing n training data points\n",
    "                each with d dimensions.\n",
    "            Y (ndarray): A 1 by n matrix representing n training labels.\n",
    "            iters (int): The number of iterations to run stochastic graident\n",
    "                descent.\n",
    "            lrate (float): The step size for stochastic gradient descent.\n",
    "        \n",
    "        \"\"\"\n",
    "        _, n = X.shape\n",
    "        \n",
    "        for it in range(iters):\n",
    "            \n",
    "            t = np.random.randint(n)\n",
    "            \n",
    "            Xt = X[:, t:t + 1]\n",
    "            Yt = Y[:, t:t + 1]\n",
    "            \n",
    "            loss = self.loss.forward(self.forward(Xt), Yt)\n",
    "            self.backward(self.loss.backward())      \n",
    "            \n",
    "            self.print_accuracy(it, X, Y, loss)\n",
    "            \n",
    "            self.sgd_step(lrate)\n",
    "            \n",
    "# Add this method to the Sequential model class\n",
    "Sequential.sgd = sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a helper method `print_accuracy` to display our progress as we train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
    "        \"\"\"Displays current prediction statistics.\n",
    "        \n",
    "        Args:\n",
    "            it (int): Current iteration.\n",
    "            X (ndarray): A d by n matrix of n points to evaluate, each with\n",
    "                d dimensions.\n",
    "            Y (ndarray): A 1 by n vector of n labels.\n",
    "            cur_loss (float): Current loss.\n",
    "            every (int): Frequency to output statistics.\n",
    "        \n",
    "        \"\"\"\n",
    "        if it % every == 1:\n",
    "            \n",
    "            cf = self.layers[-1].class_fun\n",
    "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "            \n",
    "            print('Iteration =', it, '\\tAcc =', acc, '\\tLoss =', cur_loss, flush=True)\n",
    "\n",
    "# Add this method to the Sequential model class\n",
    "Sequential.print_accuracy = print_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that we have a model, let's train it on some data and see how well it can classify. We will use the standard 'hard' data set used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[-0.23390341,  1.18151883, -2.46493986,  1.55322202,  1.27621763,\n",
    "                2.39710997, -1.34403040, -0.46903436, -0.64673502, -1.44029872,\n",
    "               -1.37537243,  1.05994811, -0.93311512,  1.02735575, -0.84138778,\n",
    "               -2.22585412, -0.42591102,  1.03561105,  0.91125595, -2.26550369],\n",
    "              [-0.92254932, -1.10309630, -2.41956036, -1.15509002, -1.04805327,\n",
    "                0.08717325,  0.81847250, -0.75171045,  0.60664705,  0.80410947,\n",
    "               -0.11600488,  1.03747218, -0.67210575,  0.99944446, -0.65559838,\n",
    "               -0.40744784, -0.58367642,  1.05972780, -0.95991874, -1.41720255]])\n",
    "\n",
    "Y = np.array([[0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
    "              [1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.scatter(X[0,:], X[1,:], c=Y[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct a neural network we think might be able to classify these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Linear(2, 10), ReLU(), \n",
    "                    Linear(10, 10), ReLU(), \n",
    "                    Linear(10, 2), SoftMax()], NLL())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training the model on the data for a few thousand iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.3 \tLoss = 0.6967746256845104\n",
      "Iteration = 251 \tAcc = 0.9 \tLoss = 0.2012862772530442\n",
      "Iteration = 501 \tAcc = 0.95 \tLoss = 0.5679379260695926\n",
      "Iteration = 751 \tAcc = 0.95 \tLoss = 0.4373239217384215\n",
      "Iteration = 1001 \tAcc = 0.95 \tLoss = 0.024026488753531423\n",
      "Iteration = 1251 \tAcc = 0.95 \tLoss = 1.904378003497613\n",
      "Iteration = 1501 \tAcc = 0.95 \tLoss = 0.005133088856219545\n",
      "Iteration = 1751 \tAcc = 0.95 \tLoss = 0.004345906816530288\n",
      "Iteration = 2001 \tAcc = 0.95 \tLoss = 0.00272317975794343\n",
      "Iteration = 2251 \tAcc = 0.95 \tLoss = 0.021620762376501575\n",
      "Iteration = 2501 \tAcc = 0.95 \tLoss = 0.0019492818042490996\n",
      "Iteration = 2751 \tAcc = 0.95 \tLoss = 1.9827765471307788\n",
      "Iteration = 3001 \tAcc = 0.95 \tLoss = 0.4033759731920736\n",
      "Iteration = 3251 \tAcc = 0.95 \tLoss = 0.002149037875191919\n",
      "Iteration = 3501 \tAcc = 0.95 \tLoss = 0.027012474954833762\n",
      "Iteration = 3751 \tAcc = 0.95 \tLoss = 0.0005521004269898281\n",
      "Iteration = 4001 \tAcc = 0.95 \tLoss = 1.6794768533600912\n",
      "Iteration = 4251 \tAcc = 0.95 \tLoss = 0.04625543640579628\n",
      "Iteration = 4501 \tAcc = 0.95 \tLoss = 0.02024381729344012\n",
      "Iteration = 4751 \tAcc = 0.95 \tLoss = 1.6799469984686484\n",
      "Iteration = 5001 \tAcc = 0.95 \tLoss = 0.013185372613443479\n",
      "Iteration = 5251 \tAcc = 0.95 \tLoss = 0.03511438460570974\n",
      "Iteration = 5501 \tAcc = 0.95 \tLoss = 0.00019027326236159946\n",
      "Iteration = 5751 \tAcc = 0.95 \tLoss = 0.3034480410536073\n",
      "Iteration = 6001 \tAcc = 0.95 \tLoss = 0.10048836408252436\n",
      "Iteration = 6251 \tAcc = 0.95 \tLoss = 0.014142939164253143\n",
      "Iteration = 6501 \tAcc = 0.95 \tLoss = 0.12124434936260367\n",
      "Iteration = 6751 \tAcc = 0.95 \tLoss = 0.08141044984197547\n",
      "Iteration = 7001 \tAcc = 0.95 \tLoss = 0.015319562478349223\n",
      "Iteration = 7251 \tAcc = 0.95 \tLoss = 0.007378124794206351\n",
      "Iteration = 7501 \tAcc = 0.95 \tLoss = 0.18930370673349134\n",
      "Iteration = 7751 \tAcc = 0.95 \tLoss = 0.009160740143707227\n",
      "Iteration = 8001 \tAcc = 0.95 \tLoss = 0.17377043358046707\n",
      "Iteration = 8251 \tAcc = 0.95 \tLoss = 0.05690417471844952\n",
      "Iteration = 8501 \tAcc = 0.95 \tLoss = 0.004197771738687373\n",
      "Iteration = 8751 \tAcc = 0.95 \tLoss = 0.004946861389637699\n",
      "Iteration = 9001 \tAcc = 0.95 \tLoss = 0.01105906995862277\n",
      "Iteration = 9251 \tAcc = 0.95 \tLoss = 0.1621112280211836\n",
      "Iteration = 9501 \tAcc = 0.95 \tLoss = 0.007579913254047852\n",
      "Iteration = 9751 \tAcc = 0.95 \tLoss = 1.601895582802248\n",
      "Iteration = 10001 \tAcc = 0.95 \tLoss = 0.22317240345064013\n",
      "Iteration = 10251 \tAcc = 0.95 \tLoss = 7.732952138742346e-06\n",
      "Iteration = 10501 \tAcc = 0.95 \tLoss = 0.012538119664222034\n",
      "Iteration = 10751 \tAcc = 0.95 \tLoss = 0.0030575537978023057\n",
      "Iteration = 11001 \tAcc = 0.95 \tLoss = 0.004944238589330267\n",
      "Iteration = 11251 \tAcc = 0.95 \tLoss = 4.020903536083152e-06\n",
      "Iteration = 11501 \tAcc = 0.95 \tLoss = 0.004129113930110758\n",
      "Iteration = 11751 \tAcc = 0.95 \tLoss = 2.758380636940635e-06\n",
      "Iteration = 12001 \tAcc = 0.95 \tLoss = 0.002215765257057173\n",
      "Iteration = 12251 \tAcc = 0.95 \tLoss = 2.2778332351445356e-06\n",
      "Iteration = 12501 \tAcc = 0.95 \tLoss = 0.010026050299307006\n",
      "Iteration = 12751 \tAcc = 0.95 \tLoss = 0.023516657774377763\n",
      "Iteration = 13001 \tAcc = 0.95 \tLoss = 0.0014917129461984166\n",
      "Iteration = 13251 \tAcc = 0.95 \tLoss = 0.07164025544115571\n",
      "Iteration = 13501 \tAcc = 0.95 \tLoss = 0.0013156173757143468\n",
      "Iteration = 13751 \tAcc = 0.95 \tLoss = 0.08887653810494746\n",
      "Iteration = 14001 \tAcc = 0.95 \tLoss = 0.006838158558373953\n",
      "Iteration = 14251 \tAcc = 0.95 \tLoss = 0.3928036248212447\n",
      "Iteration = 14501 \tAcc = 0.95 \tLoss = 0.22457269217248788\n",
      "Iteration = 14751 \tAcc = 0.95 \tLoss = 3.2775914324092114e-06\n",
      "Iteration = 15001 \tAcc = 0.95 \tLoss = 0.005462519908481029\n",
      "Iteration = 15251 \tAcc = 0.95 \tLoss = 0.000597575953428314\n",
      "Iteration = 15501 \tAcc = 0.95 \tLoss = 7.313065221011448e-08\n",
      "Iteration = 15751 \tAcc = 0.95 \tLoss = 8.328699806982149e-06\n",
      "Iteration = 16001 \tAcc = 0.95 \tLoss = 0.02135657012202454\n",
      "Iteration = 16251 \tAcc = 0.95 \tLoss = 8.100117936280317e-06\n",
      "Iteration = 16501 \tAcc = 0.95 \tLoss = 0.0003629602770073347\n",
      "Iteration = 16751 \tAcc = 0.95 \tLoss = 0.024930239228844277\n",
      "Iteration = 17001 \tAcc = 0.95 \tLoss = 0.005151236496238283\n",
      "Iteration = 17251 \tAcc = 0.95 \tLoss = 0.24619591957341766\n",
      "Iteration = 17501 \tAcc = 0.95 \tLoss = 0.12287209716693416\n",
      "Iteration = 17751 \tAcc = 0.95 \tLoss = 1.3514848114884548\n",
      "Iteration = 18001 \tAcc = 0.95 \tLoss = 0.04899867395751998\n",
      "Iteration = 18251 \tAcc = 0.95 \tLoss = 0.2137129097458514\n",
      "Iteration = 18501 \tAcc = 0.95 \tLoss = 0.008490836021563567\n",
      "Iteration = 18751 \tAcc = 0.95 \tLoss = 0.007051377826754408\n",
      "Iteration = 19001 \tAcc = 0.95 \tLoss = 0.013335789076425287\n",
      "Iteration = 19251 \tAcc = 0.95 \tLoss = 0.4212602310126846\n",
      "Iteration = 19501 \tAcc = 0.95 \tLoss = 0.09709686580736142\n",
      "Iteration = 19751 \tAcc = 0.95 \tLoss = 0.22618289929243907\n",
      "Iteration = 20001 \tAcc = 0.95 \tLoss = 0.028597817035128863\n",
      "Iteration = 20251 \tAcc = 0.95 \tLoss = 0.008381308388797765\n",
      "Iteration = 20501 \tAcc = 0.95 \tLoss = 5.030874710137695e-08\n",
      "Iteration = 20751 \tAcc = 0.95 \tLoss = 6.42428324803006e-05\n",
      "Iteration = 21001 \tAcc = 0.95 \tLoss = 0.02826781609789287\n",
      "Iteration = 21251 \tAcc = 0.95 \tLoss = 0.04330931888866736\n",
      "Iteration = 21501 \tAcc = 0.95 \tLoss = 0.005444290819282853\n",
      "Iteration = 21751 \tAcc = 0.95 \tLoss = 2.1473367203998453e-08\n",
      "Iteration = 22001 \tAcc = 0.95 \tLoss = 0.22399127137361555\n",
      "Iteration = 22251 \tAcc = 0.95 \tLoss = 3.7441487257789255e-05\n",
      "Iteration = 22501 \tAcc = 0.95 \tLoss = 0.0461697510599014\n",
      "Iteration = 22751 \tAcc = 0.95 \tLoss = 0.19867168245262842\n",
      "Iteration = 23001 \tAcc = 0.95 \tLoss = 8.307387003978544e-10\n",
      "Iteration = 23251 \tAcc = 0.95 \tLoss = 0.03348047975915451\n",
      "Iteration = 23501 \tAcc = 0.95 \tLoss = 0.04133128947841773\n",
      "Iteration = 23751 \tAcc = 0.95 \tLoss = 0.0025446196139986377\n",
      "Iteration = 24001 \tAcc = 0.95 \tLoss = 0.029623682251916323\n",
      "Iteration = 24251 \tAcc = 0.95 \tLoss = 0.0020111602021559284\n",
      "Iteration = 24501 \tAcc = 0.95 \tLoss = 7.267519919198915e-13\n",
      "Iteration = 24751 \tAcc = 0.95 \tLoss = 0.07148862002288127\n",
      "Iteration = 25001 \tAcc = 0.95 \tLoss = 0.19299591971006094\n",
      "Iteration = 25251 \tAcc = 0.95 \tLoss = 0.43524109214274653\n",
      "Iteration = 25501 \tAcc = 0.95 \tLoss = 7.2492779470117136e-06\n",
      "Iteration = 25751 \tAcc = 0.95 \tLoss = 0.07523047855119824\n",
      "Iteration = 26001 \tAcc = 0.95 \tLoss = 8.615330671091586e-14\n",
      "Iteration = 26251 \tAcc = 0.95 \tLoss = 0.0033029417499416593\n",
      "Iteration = 26501 \tAcc = 0.95 \tLoss = 3.7713883568639614e-06\n",
      "Iteration = 26751 \tAcc = 0.95 \tLoss = 0.23844246908852798\n",
      "Iteration = 27001 \tAcc = 0.95 \tLoss = 0.0007147086934936316\n",
      "Iteration = 27251 \tAcc = 0.95 \tLoss = 0.24094035216854742\n",
      "Iteration = 27501 \tAcc = 0.95 \tLoss = 0.02288910854047414\n",
      "Iteration = 27751 \tAcc = 0.95 \tLoss = 0.02275415445398897\n",
      "Iteration = 28001 \tAcc = 1.0 \tLoss = 0.11623182453426244\n",
      "Iteration = 28251 \tAcc = 0.95 \tLoss = 8.07922132461582e-09\n",
      "Iteration = 28501 \tAcc = 0.95 \tLoss = 0.8373546991818803\n",
      "Iteration = 28751 \tAcc = 0.95 \tLoss = 0.12265279339423425\n",
      "Iteration = 29001 \tAcc = 1.0 \tLoss = 9.86393494759093e-07\n",
      "Iteration = 29251 \tAcc = 0.95 \tLoss = 0.0173051064892171\n",
      "Iteration = 29501 \tAcc = 0.95 \tLoss = 0.052511484331749514\n",
      "Iteration = 29751 \tAcc = 0.95 \tLoss = 0.030437923559440253\n",
      "Iteration = 30001 \tAcc = 0.95 \tLoss = 1.0641320343530453e-06\n",
      "Iteration = 30251 \tAcc = 0.95 \tLoss = 0.06680541194683359\n",
      "Iteration = 30501 \tAcc = 0.95 \tLoss = 0.0006449507625101042\n",
      "Iteration = 30751 \tAcc = 0.95 \tLoss = 1.4528644954826116e-10\n",
      "Iteration = 31001 \tAcc = 0.95 \tLoss = 0.00046501447412760337\n",
      "Iteration = 31251 \tAcc = 1.0 \tLoss = 3.8613735616920935e-09\n",
      "Iteration = 31501 \tAcc = 0.95 \tLoss = 3.1662782446095793e-09\n",
      "Iteration = 31751 \tAcc = 0.95 \tLoss = 0.00046074140996240796\n",
      "Iteration = 32001 \tAcc = 0.95 \tLoss = 0.08129362475411071\n",
      "Iteration = 32251 \tAcc = 0.95 \tLoss = 0.12795334831629035\n",
      "Iteration = 32501 \tAcc = 0.95 \tLoss = 0.0197998022925804\n",
      "Iteration = 32751 \tAcc = 1.0 \tLoss = 0.028235452952291865\n",
      "Iteration = 33001 \tAcc = 0.95 \tLoss = 6.883382752678339e-13\n",
      "Iteration = 33251 \tAcc = 1.0 \tLoss = 0.03318616287726641\n",
      "Iteration = 33501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 33751 \tAcc = 0.95 \tLoss = 2.4720261107125515e-07\n",
      "Iteration = 34001 \tAcc = 0.95 \tLoss = 1.3338558681205775e-07\n",
      "Iteration = 34251 \tAcc = 0.95 \tLoss = 0.02595480177209375\n",
      "Iteration = 34501 \tAcc = 0.95 \tLoss = 0.022158970724967773\n",
      "Iteration = 34751 \tAcc = 1.0 \tLoss = 8.215650382226495e-14\n",
      "Iteration = 35001 \tAcc = 0.95 \tLoss = 1.7616864786640084e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 35251 \tAcc = 1.0 \tLoss = 0.029815692437236217\n",
      "Iteration = 35501 \tAcc = 0.95 \tLoss = 1.4703575261405799e-07\n",
      "Iteration = 35751 \tAcc = 1.0 \tLoss = 1.0379239118359347e-07\n",
      "Iteration = 36001 \tAcc = 1.0 \tLoss = 0.023577490499858522\n",
      "Iteration = 36251 \tAcc = 0.95 \tLoss = 1.7607815207428017e-10\n",
      "Iteration = 36501 \tAcc = 1.0 \tLoss = 0.006320864861631617\n",
      "Iteration = 36751 \tAcc = 1.0 \tLoss = 1.2478906796794547e-12\n",
      "Iteration = 37001 \tAcc = 1.0 \tLoss = 0.0001250790586971862\n",
      "Iteration = 37251 \tAcc = 0.95 \tLoss = 0.00023030349329126277\n",
      "Iteration = 37501 \tAcc = 0.95 \tLoss = 0.061393573110742755\n",
      "Iteration = 37751 \tAcc = 0.95 \tLoss = 0.0035543300242155845\n",
      "Iteration = 38001 \tAcc = 1.0 \tLoss = 5.5414185221519185e-08\n",
      "Iteration = 38251 \tAcc = 1.0 \tLoss = 0.022331825060002624\n",
      "Iteration = 38501 \tAcc = 1.0 \tLoss = 0.019505326038170095\n",
      "Iteration = 38751 \tAcc = 1.0 \tLoss = 0.01920876857624924\n",
      "Iteration = 39001 \tAcc = 1.0 \tLoss = 0.013208348967508258\n",
      "Iteration = 39251 \tAcc = 0.95 \tLoss = 0.0104161077442022\n",
      "Iteration = 39501 \tAcc = 1.0 \tLoss = 0.00553162025590256\n",
      "Iteration = 39751 \tAcc = 0.95 \tLoss = 0.0001131904004776479\n",
      "Iteration = 40001 \tAcc = 1.0 \tLoss = 3.301803275235761e-13\n",
      "Iteration = 40251 \tAcc = 1.0 \tLoss = 0.022092980610499195\n",
      "Iteration = 40501 \tAcc = 1.0 \tLoss = 0.0045653850720474565\n",
      "Iteration = 40751 \tAcc = 1.0 \tLoss = 0.0001429868394439066\n",
      "Iteration = 41001 \tAcc = 1.0 \tLoss = 5.440092820664746e-13\n",
      "Iteration = 41251 \tAcc = 0.95 \tLoss = 6.188383139262537e-13\n",
      "Iteration = 41501 \tAcc = 1.0 \tLoss = 9.913782811043074e-05\n",
      "Iteration = 41751 \tAcc = 0.95 \tLoss = 0.017031671831196842\n",
      "Iteration = 42001 \tAcc = 0.95 \tLoss = 0.00356836310856263\n",
      "Iteration = 42251 \tAcc = 0.95 \tLoss = 0.3260202641757516\n",
      "Iteration = 42501 \tAcc = 1.0 \tLoss = 6.772871153034142e-11\n",
      "Iteration = 42751 \tAcc = 0.95 \tLoss = 0.042998583268305496\n",
      "Iteration = 43001 \tAcc = 0.95 \tLoss = 6.309830799417404e-05\n",
      "Iteration = 43251 \tAcc = 0.95 \tLoss = 0.23267172601803277\n",
      "Iteration = 43501 \tAcc = 0.95 \tLoss = 0.021898737471911605\n",
      "Iteration = 43751 \tAcc = 0.95 \tLoss = 0.00707801647819149\n",
      "Iteration = 44001 \tAcc = 1.0 \tLoss = 7.478428854711416e-05\n",
      "Iteration = 44251 \tAcc = 1.0 \tLoss = 0.006102280887443793\n",
      "Iteration = 44501 \tAcc = 0.95 \tLoss = 0.7259564507349343\n",
      "Iteration = 44751 \tAcc = 0.95 \tLoss = 1.221245327087673e-15\n",
      "Iteration = 45001 \tAcc = 1.0 \tLoss = 0.007044219522393141\n",
      "Iteration = 45251 \tAcc = 1.0 \tLoss = 8.993241080597388e-05\n",
      "Iteration = 45501 \tAcc = 0.95 \tLoss = 0.00013487764605494434\n",
      "Iteration = 45751 \tAcc = 0.95 \tLoss = 0.031166719204358283\n",
      "Iteration = 46001 \tAcc = 1.0 \tLoss = 0.01142969626885068\n",
      "Iteration = 46251 \tAcc = 1.0 \tLoss = 7.43394626399995e-09\n",
      "Iteration = 46501 \tAcc = 1.0 \tLoss = 0.6453092740729324\n",
      "Iteration = 46751 \tAcc = 0.95 \tLoss = 0.04264011291283849\n",
      "Iteration = 47001 \tAcc = 1.0 \tLoss = 0.016086698803425857\n",
      "Iteration = 47251 \tAcc = 0.95 \tLoss = 0.1826253510477215\n",
      "Iteration = 47501 \tAcc = 0.95 \tLoss = 8.420042870205931e-05\n",
      "Iteration = 47751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 48001 \tAcc = 0.95 \tLoss = 3.614129203937078e-05\n",
      "Iteration = 48251 \tAcc = 1.0 \tLoss = 3.3315350478901727e-11\n",
      "Iteration = 48501 \tAcc = 0.95 \tLoss = 1.5898393712633506e-13\n",
      "Iteration = 48751 \tAcc = 1.0 \tLoss = 0.6261138465753927\n",
      "Iteration = 49001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 49251 \tAcc = 0.95 \tLoss = 3.5445655362601443e-09\n",
      "Iteration = 49501 \tAcc = 0.95 \tLoss = 4.6406520955986025e-09\n",
      "Iteration = 49751 \tAcc = 1.0 \tLoss = 0.576666060402267\n",
      "Iteration = 50001 \tAcc = 0.95 \tLoss = 0.001767338465009837\n",
      "Iteration = 50251 \tAcc = 1.0 \tLoss = 0.011512669186849175\n",
      "Iteration = 50501 \tAcc = 1.0 \tLoss = 3.0291547048936484e-11\n",
      "Iteration = 50751 \tAcc = 0.95 \tLoss = 0.00657923865062174\n",
      "Iteration = 51001 \tAcc = 1.0 \tLoss = 6.80344087388449e-05\n",
      "Iteration = 51251 \tAcc = 0.95 \tLoss = 8.616136298697563e-05\n",
      "Iteration = 51501 \tAcc = 1.0 \tLoss = 0.011643292327866723\n",
      "Iteration = 51751 \tAcc = 0.95 \tLoss = 3.0664359940151524e-13\n",
      "Iteration = 52001 \tAcc = 1.0 \tLoss = 0.42574116966608505\n",
      "Iteration = 52251 \tAcc = 1.0 \tLoss = 4.896417987265102e-05\n",
      "Iteration = 52501 \tAcc = 1.0 \tLoss = 0.0008237074713480179\n",
      "Iteration = 52751 \tAcc = 1.0 \tLoss = 0.001582059844154567\n",
      "Iteration = 53001 \tAcc = 1.0 \tLoss = 2.218784713977767e-09\n",
      "Iteration = 53251 \tAcc = 0.95 \tLoss = 4.7132741418130505e-05\n",
      "Iteration = 53501 \tAcc = 1.0 \tLoss = 9.126426285534087e-10\n",
      "Iteration = 53751 \tAcc = 0.95 \tLoss = 7.632705578977518e-11\n",
      "Iteration = 54001 \tAcc = 1.0 \tLoss = 0.004380284286075857\n",
      "Iteration = 54251 \tAcc = 0.95 \tLoss = 0.00859948837669593\n",
      "Iteration = 54501 \tAcc = 0.95 \tLoss = 2.5157653738009213e-13\n",
      "Iteration = 54751 \tAcc = 0.95 \tLoss = 0.0028978025254545875\n",
      "Iteration = 55001 \tAcc = 0.95 \tLoss = 0.755802325542279\n",
      "Iteration = 55251 \tAcc = 1.0 \tLoss = 0.1908124030529688\n",
      "Iteration = 55501 \tAcc = 1.0 \tLoss = 5.551115123125785e-16\n",
      "Iteration = 55751 \tAcc = 1.0 \tLoss = 0.011850730215060487\n",
      "Iteration = 56001 \tAcc = 1.0 \tLoss = 0.0008275714471330305\n",
      "Iteration = 56251 \tAcc = 1.0 \tLoss = 6.016676046453097e-11\n",
      "Iteration = 56501 \tAcc = 1.0 \tLoss = 0.00935016333942706\n",
      "Iteration = 56751 \tAcc = 1.0 \tLoss = 0.005622232849405282\n",
      "Iteration = 57001 \tAcc = 1.0 \tLoss = 0.47018071902257086\n",
      "Iteration = 57251 \tAcc = 0.95 \tLoss = 3.7441882934434257e-10\n",
      "Iteration = 57501 \tAcc = 0.95 \tLoss = 0.0028874410280180876\n",
      "Iteration = 57751 \tAcc = 0.95 \tLoss = 0.12034269541230978\n",
      "Iteration = 58001 \tAcc = 1.0 \tLoss = 0.49233553337042457\n",
      "Iteration = 58251 \tAcc = 0.95 \tLoss = 0.022269068953531472\n",
      "Iteration = 58501 \tAcc = 0.95 \tLoss = 4.0345504714886327e-13\n",
      "Iteration = 58751 \tAcc = 1.0 \tLoss = 0.18725618769685837\n",
      "Iteration = 59001 \tAcc = 1.0 \tLoss = 4.045522945495038e-05\n",
      "Iteration = 59251 \tAcc = 1.0 \tLoss = 0.013452547140829522\n",
      "Iteration = 59501 \tAcc = 1.0 \tLoss = 0.01371204838777222\n",
      "Iteration = 59751 \tAcc = 0.95 \tLoss = 8.881784197001256e-16\n",
      "Iteration = 60001 \tAcc = 1.0 \tLoss = 0.007738119065025908\n",
      "Iteration = 60251 \tAcc = 1.0 \tLoss = 0.016057344145403968\n",
      "Iteration = 60501 \tAcc = 1.0 \tLoss = 0.00031670235406826936\n",
      "Iteration = 60751 \tAcc = 0.95 \tLoss = 0.03047728652187428\n",
      "Iteration = 61001 \tAcc = 0.95 \tLoss = 1.633219115637773e-10\n",
      "Iteration = 61251 \tAcc = 1.0 \tLoss = 0.010732128706373002\n",
      "Iteration = 61501 \tAcc = 1.0 \tLoss = 2.2204460492503136e-16\n",
      "Iteration = 61751 \tAcc = 1.0 \tLoss = 1.436524233003818e-10\n",
      "Iteration = 62001 \tAcc = 1.0 \tLoss = 2.842170943040441e-14\n",
      "Iteration = 62251 \tAcc = 1.0 \tLoss = 0.0045465387378199485\n",
      "Iteration = 62501 \tAcc = 1.0 \tLoss = 0.005970253022841913\n",
      "Iteration = 62751 \tAcc = 0.95 \tLoss = 0.002141567956757078\n",
      "Iteration = 63001 \tAcc = 1.0 \tLoss = 0.4519467755344497\n",
      "Iteration = 63251 \tAcc = 1.0 \tLoss = 2.3193614852216222e-05\n",
      "Iteration = 63501 \tAcc = 1.0 \tLoss = 2.2263557666582257e-10\n",
      "Iteration = 63751 \tAcc = 1.0 \tLoss = 1.6131762592538566e-11\n",
      "Iteration = 64001 \tAcc = 1.0 \tLoss = 2.5607649337625873e-10\n",
      "Iteration = 64251 \tAcc = 1.0 \tLoss = 0.018500737562917383\n",
      "Iteration = 64501 \tAcc = 0.95 \tLoss = 1.2290168882601238e-13\n",
      "Iteration = 64751 \tAcc = 1.0 \tLoss = 2.0701662608513312e-10\n",
      "Iteration = 65001 \tAcc = 0.95 \tLoss = 0.014730519765263565\n",
      "Iteration = 65251 \tAcc = 1.0 \tLoss = 0.002801409775981354\n",
      "Iteration = 65501 \tAcc = 1.0 \tLoss = 0.012077872396561783\n",
      "Iteration = 65751 \tAcc = 1.0 \tLoss = 0.010259995427740802\n",
      "Iteration = 66001 \tAcc = 0.95 \tLoss = 1.0980105713543402e-13\n",
      "Iteration = 66251 \tAcc = 1.0 \tLoss = 0.00040166461022033133\n",
      "Iteration = 66501 \tAcc = 1.0 \tLoss = 0.37738433162669754\n",
      "Iteration = 66751 \tAcc = 1.0 \tLoss = 1.0980105713543402e-13\n",
      "Iteration = 67001 \tAcc = 1.0 \tLoss = 0.0035304911582363834\n",
      "Iteration = 67251 \tAcc = 0.95 \tLoss = 0.00228203984511925\n",
      "Iteration = 67501 \tAcc = 1.0 \tLoss = 0.0011144847028605755\n",
      "Iteration = 67751 \tAcc = 1.0 \tLoss = 3.33066907387547e-16\n",
      "Iteration = 68001 \tAcc = 0.95 \tLoss = 0.0020615327567867986\n",
      "Iteration = 68251 \tAcc = 1.0 \tLoss = 3.3642288960297405e-10\n",
      "Iteration = 68501 \tAcc = 1.0 \tLoss = 3.227530465631665e-10\n",
      "Iteration = 68751 \tAcc = 1.0 \tLoss = 0.002460531386816571\n",
      "Iteration = 69001 \tAcc = 1.0 \tLoss = 0.12164035416658864\n",
      "Iteration = 69251 \tAcc = 1.0 \tLoss = 1.5883383498600719e-10\n",
      "Iteration = 69501 \tAcc = 1.0 \tLoss = 0.0042623494829734255\n",
      "Iteration = 69751 \tAcc = 1.0 \tLoss = 1.468686283808856e-10\n",
      "Iteration = 70001 \tAcc = 1.0 \tLoss = 0.005587309729272092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 70251 \tAcc = 1.0 \tLoss = 0.00959457812227244\n",
      "Iteration = 70501 \tAcc = 1.0 \tLoss = 0.0022304695122480358\n",
      "Iteration = 70751 \tAcc = 1.0 \tLoss = 0.006874377255674435\n",
      "Iteration = 71001 \tAcc = 1.0 \tLoss = 2.6334050768006202e-05\n",
      "Iteration = 71251 \tAcc = 1.0 \tLoss = 0.10799614837179769\n",
      "Iteration = 71501 \tAcc = 1.0 \tLoss = 4.440892098500627e-16\n",
      "Iteration = 71751 \tAcc = 1.0 \tLoss = 2.1159529686194807e-10\n",
      "Iteration = 72001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 72251 \tAcc = 1.0 \tLoss = 6.2362972938987605e-06\n",
      "Iteration = 72501 \tAcc = 1.0 \tLoss = 6.578666688153901e-05\n",
      "Iteration = 72751 \tAcc = 1.0 \tLoss = 1.6253665080513612e-13\n",
      "Iteration = 73001 \tAcc = 1.0 \tLoss = 0.0035673951011952598\n",
      "Iteration = 73251 \tAcc = 1.0 \tLoss = 3.0974001142194473e-11\n",
      "Iteration = 73501 \tAcc = 1.0 \tLoss = 2.52575738102255e-13\n",
      "Iteration = 73751 \tAcc = 1.0 \tLoss = 4.692566336607863e-10\n",
      "Iteration = 74001 \tAcc = 1.0 \tLoss = 0.007800906280097605\n",
      "Iteration = 74251 \tAcc = 1.0 \tLoss = 0.0030745626720548296\n",
      "Iteration = 74501 \tAcc = 1.0 \tLoss = 2.9930380400816023e-10\n",
      "Iteration = 74751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 75001 \tAcc = 1.0 \tLoss = 0.012852447505705647\n",
      "Iteration = 75251 \tAcc = 1.0 \tLoss = 7.76095464793838e-06\n",
      "Iteration = 75501 \tAcc = 1.0 \tLoss = 5.06301667270775e-11\n",
      "Iteration = 75751 \tAcc = 1.0 \tLoss = 0.003285862935927074\n",
      "Iteration = 76001 \tAcc = 1.0 \tLoss = 1.5356486847094828e-06\n",
      "Iteration = 76251 \tAcc = 1.0 \tLoss = 0.00391224315058839\n",
      "Iteration = 76501 \tAcc = 1.0 \tLoss = 3.381079094446972e-06\n",
      "Iteration = 76751 \tAcc = 1.0 \tLoss = 6.122048425636268e-10\n",
      "Iteration = 77001 \tAcc = 1.0 \tLoss = 5.241587165680051e-10\n",
      "Iteration = 77251 \tAcc = 1.0 \tLoss = 4.607547676833256e-11\n",
      "Iteration = 77501 \tAcc = 1.0 \tLoss = 0.29174970502033354\n",
      "Iteration = 77751 \tAcc = 1.0 \tLoss = 0.005857604296236746\n",
      "Iteration = 78001 \tAcc = 1.0 \tLoss = 0.0011810829917623582\n",
      "Iteration = 78251 \tAcc = 1.0 \tLoss = 9.281464485866739e-14\n",
      "Iteration = 78501 \tAcc = 1.0 \tLoss = 0.004816233474479088\n",
      "Iteration = 78751 \tAcc = 1.0 \tLoss = 4.08376887944936e-10\n",
      "Iteration = 79001 \tAcc = 1.0 \tLoss = 0.0023382941533485048\n",
      "Iteration = 79251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 79501 \tAcc = 1.0 \tLoss = 2.1658176374913443e-06\n",
      "Iteration = 79751 \tAcc = 1.0 \tLoss = 0.0067073004327643255\n",
      "Iteration = 80001 \tAcc = 1.0 \tLoss = 1.7708057242772814e-13\n",
      "Iteration = 80251 \tAcc = 1.0 \tLoss = 0.003596993969823587\n",
      "Iteration = 80501 \tAcc = 1.0 \tLoss = 0.007471792486971088\n",
      "Iteration = 80751 \tAcc = 1.0 \tLoss = 4.462836767801213e-10\n",
      "Iteration = 81001 \tAcc = 1.0 \tLoss = 5.551115123125785e-16\n",
      "Iteration = 81251 \tAcc = 1.0 \tLoss = 6.423409584075605e-10\n",
      "Iteration = 81501 \tAcc = 1.0 \tLoss = 8.002033483482684e-10\n",
      "Iteration = 81751 \tAcc = 1.0 \tLoss = 1.334033662205282e-09\n",
      "Iteration = 82001 \tAcc = 1.0 \tLoss = 0.003818226447638203\n",
      "Iteration = 82251 \tAcc = 1.0 \tLoss = 5.825070427709785e-10\n",
      "Iteration = 82501 \tAcc = 1.0 \tLoss = 4.346391820105596e-05\n",
      "Iteration = 82751 \tAcc = 1.0 \tLoss = 0.000992898620312292\n",
      "Iteration = 83001 \tAcc = 1.0 \tLoss = 2.4835131829324628e-06\n",
      "Iteration = 83251 \tAcc = 1.0 \tLoss = 0.0018636366784472664\n",
      "Iteration = 83501 \tAcc = 1.0 \tLoss = 3.730252134240224e-06\n",
      "Iteration = 83751 \tAcc = 1.0 \tLoss = 0.006918793942938958\n",
      "Iteration = 84001 \tAcc = 1.0 \tLoss = 0.0020745234365742867\n",
      "Iteration = 84251 \tAcc = 1.0 \tLoss = 3.863545704705303e-06\n",
      "Iteration = 84501 \tAcc = 1.0 \tLoss = 0.004751780748088005\n",
      "Iteration = 84751 \tAcc = 1.0 \tLoss = 2.873257187730318e-13\n",
      "Iteration = 85001 \tAcc = 1.0 \tLoss = 0.13564048082364963\n",
      "Iteration = 85251 \tAcc = 1.0 \tLoss = 0.004723727204964968\n",
      "Iteration = 85501 \tAcc = 1.0 \tLoss = 2.6467716907067236e-13\n",
      "Iteration = 85751 \tAcc = 1.0 \tLoss = 1.0165958080515038e-09\n",
      "Iteration = 86001 \tAcc = 1.0 \tLoss = 0.005268104605871147\n",
      "Iteration = 86251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 86501 \tAcc = 1.0 \tLoss = 8.982240510972443e-10\n",
      "Iteration = 86751 \tAcc = 1.0 \tLoss = 0.006824923722076429\n",
      "Iteration = 87001 \tAcc = 1.0 \tLoss = 9.127392179566392e-10\n",
      "Iteration = 87251 \tAcc = 1.0 \tLoss = 1.8736111568672965e-09\n",
      "Iteration = 87501 \tAcc = 1.0 \tLoss = 6.934441909818914e-11\n",
      "Iteration = 87751 \tAcc = 1.0 \tLoss = 8.17124146124449e-13\n",
      "Iteration = 88001 \tAcc = 1.0 \tLoss = 0.0031127872246343876\n",
      "Iteration = 88251 \tAcc = 1.0 \tLoss = 2.422944070539123e-09\n",
      "Iteration = 88501 \tAcc = 1.0 \tLoss = 8.082423619271467e-14\n",
      "Iteration = 88751 \tAcc = 1.0 \tLoss = 8.626432901337839e-14\n",
      "Iteration = 89001 \tAcc = 1.0 \tLoss = 1.8755196302502047e-09\n",
      "Iteration = 89251 \tAcc = 1.0 \tLoss = 0.0011388425775884115\n",
      "Iteration = 89501 \tAcc = 1.0 \tLoss = 3.103073353827794e-13\n",
      "Iteration = 89751 \tAcc = 1.0 \tLoss = 1.925339556307308e-09\n",
      "Iteration = 90001 \tAcc = 1.0 \tLoss = 0.0009158808159326022\n",
      "Iteration = 90251 \tAcc = 1.0 \tLoss = 9.463093361115413e-07\n",
      "Iteration = 90501 \tAcc = 1.0 \tLoss = 0.052652913531675644\n",
      "Iteration = 90751 \tAcc = 1.0 \tLoss = 0.0008385473472030362\n",
      "Iteration = 91001 \tAcc = 1.0 \tLoss = 2.234372578345119e-09\n",
      "Iteration = 91251 \tAcc = 1.0 \tLoss = 1.7551171642563648e-09\n",
      "Iteration = 91501 \tAcc = 1.0 \tLoss = 0.0027913321353556673\n",
      "Iteration = 91751 \tAcc = 1.0 \tLoss = 7.771561172376099e-16\n",
      "Iteration = 92001 \tAcc = 1.0 \tLoss = 0.0005891325102249925\n",
      "Iteration = 92251 \tAcc = 1.0 \tLoss = 0.0005812823585606387\n",
      "Iteration = 92501 \tAcc = 1.0 \tLoss = 0.0028315966096300093\n",
      "Iteration = 92751 \tAcc = 1.0 \tLoss = 0.002598362879252983\n",
      "Iteration = 93001 \tAcc = 1.0 \tLoss = 2.721156633356629e-13\n",
      "Iteration = 93251 \tAcc = 1.0 \tLoss = 5.463673957855564e-11\n",
      "Iteration = 93501 \tAcc = 1.0 \tLoss = 0.0031842162913922794\n",
      "Iteration = 93751 \tAcc = 1.0 \tLoss = 6.01958751324959e-09\n",
      "Iteration = 94001 \tAcc = 1.0 \tLoss = 0.00156310343268751\n",
      "Iteration = 94251 \tAcc = 1.0 \tLoss = 3.3243741093811026e-11\n",
      "Iteration = 94501 \tAcc = 1.0 \tLoss = 5.551115123125785e-16\n",
      "Iteration = 94751 \tAcc = 1.0 \tLoss = 1.3606491252379933e-06\n",
      "Iteration = 95001 \tAcc = 1.0 \tLoss = 0.0482062662104168\n",
      "Iteration = 95251 \tAcc = 1.0 \tLoss = 3.3454237669389343e-07\n",
      "Iteration = 95501 \tAcc = 1.0 \tLoss = 6.354850665898797e-09\n",
      "Iteration = 95751 \tAcc = 1.0 \tLoss = 0.0006127567491619355\n",
      "Iteration = 96001 \tAcc = 1.0 \tLoss = 0.002737949711233688\n",
      "Iteration = 96251 \tAcc = 1.0 \tLoss = 3.6201708298422537e-11\n",
      "Iteration = 96501 \tAcc = 1.0 \tLoss = 9.219482145502457e-07\n",
      "Iteration = 96751 \tAcc = 1.0 \tLoss = 0.003837245547598318\n",
      "Iteration = 97001 \tAcc = 1.0 \tLoss = 4.440892098500627e-16\n",
      "Iteration = 97251 \tAcc = 1.0 \tLoss = 4.440892098500627e-16\n",
      "Iteration = 97501 \tAcc = 1.0 \tLoss = 1.6087131626819813e-13\n",
      "Iteration = 97751 \tAcc = 1.0 \tLoss = 0.0025783954130958043\n",
      "Iteration = 98001 \tAcc = 1.0 \tLoss = 3.2861287078836687e-09\n",
      "Iteration = 98251 \tAcc = 1.0 \tLoss = 0.00046352037023104323\n",
      "Iteration = 98501 \tAcc = 1.0 \tLoss = 3.833095308599298e-07\n",
      "Iteration = 98751 \tAcc = 1.0 \tLoss = 3.998903505198875e-07\n",
      "Iteration = 99001 \tAcc = 1.0 \tLoss = 0.0027012371278380435\n",
      "Iteration = 99251 \tAcc = 1.0 \tLoss = 0.0024766085816871238\n",
      "Iteration = 99501 \tAcc = 1.0 \tLoss = 4.0447712854570716e-07\n",
      "Iteration = 99751 \tAcc = 1.0 \tLoss = 5.861977570020998e-14\n"
     ]
    }
   ],
   "source": [
    "model.sgd(X, Y, 100000, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like it has fitted the data 100\\%. Let's see what the decision boundary looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a grid of points to classify\n",
    "xx1, xx2 = np.meshgrid(np.arange(-3, 3, 0.005), np.arange(-3, 3, 0.005))\n",
    "\n",
    "# Flatten the grid to pass into model\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()].T\n",
    "\n",
    "# Predict classification at every point on the grid\n",
    "Z = model.forward(grid)[1,:].reshape(xx1.shape)\n",
    "\n",
    "# Plot the prediction regions.\n",
    "plt.imshow(Z, interpolation='bicubic', origin='lower', extent=[-3, 3, -3, 3], \n",
    "           cmap=ListedColormap(['#1f77b4', '#ff7f0e']), alpha=0.55, aspect='auto')\n",
    "\n",
    "# Plot the original points.\n",
    "_ = plt.scatter(X[0,:], X[1,:], c=Y[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
