{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Network in NumPy\n",
    "\n",
    "This notebook walks through the process of constructing a feed-forward neural network for multi-class classification solely using NumPy.\n",
    "\n",
    "## Layers\n",
    "\n",
    "For our neural network, we want to abstract away from individual neurons and focus on layers. Each element of the network will be defined by a certain layer.\n",
    "\n",
    "### Base Layer\n",
    "\n",
    "The abstract base layer ensures that all called methods by the `Sequential` model exist on the layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Base class for neural network layers.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we only need to worry about the `sgd_step` method as some layers won't need to update any weights because they don't have any (e.g. activation layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self):\n",
    "        \"\"\"Some layers do not have weights to update on gradient descent steps.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Add this method to the Layer class\n",
    "Layer.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "This is the simplest layer that makes up the majority of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"A simple, fully-connected linear layer.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up this layer, we need to know the input and output dimensions ahead of time. Using this information, we randomly initialize the weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, m, n):\n",
    "        \"\"\"Initializes the layer based on input and output dimensions. \n",
    "\n",
    "        Note: Kernel is initialized using normal distribution with mean 0 and \n",
    "        variance 1 / m. All biases are initialized to zero.\n",
    "\n",
    "        Args:\n",
    "            m (int): Number of inputs to the layer.\n",
    "            n (int): Number of outputs from the layer.\n",
    "\n",
    "        \"\"\"\n",
    "        self.m, self.n = m, n\n",
    "\n",
    "        self.W0 = np.zeros((n, 1))\n",
    "        self.W = np.random.normal(0, np.sqrt(1 / m), (m, n))\n",
    "\n",
    "# Add this method to the Linear layer class\n",
    "Linear.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method will compute the output of the layer given a set of $m$ inputs from the previous layer for a batch of size $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, A):\n",
    "        \"\"\"Computes the forward pass through the linear network for a batch.\n",
    "\n",
    "        Args:\n",
    "            A (ndarray): An m by b matrix representing the m activations from the\n",
    "                previous layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the result of passing the \n",
    "                activations through the network layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "\n",
    "        return self.W.T @ self.A + self.W0\n",
    "\n",
    "# Add this method to the Linear layer class\n",
    "Linear.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method will compute the gradient of the loss with respect to the inputs to the layer for a batch of size $b$. Note: There is an implicit sum over all $b$ in the `dLdW` calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdZ):\n",
    "        \"\"\"Uses the gradient of loss with respect to outputs of the layer for a \n",
    "        batch to update the sum of gradients of the loss with respect to the \n",
    "        weights for the entire batch. Also returns the gradient of the loss with \n",
    "        respect to the inputs to the layer for a batch.\n",
    "\n",
    "        Args:\n",
    "            dLdZ (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the layer outputs for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An m by b matrix representing the gradient of the loss with \n",
    "                respect to the inputs to the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.dLdW = self.A @ dLdZ.T  # Implicit sum over all b\n",
    "        self.dLdW0 = np.sum(dLdZ, axis=1, keepdims=True)\n",
    "\n",
    "        return self.W @ dLdZ\n",
    "\n",
    "# Add this method to the Linear layer class\n",
    "Linear.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"A simple, fully-connected linear layer.\"\"\"\n",
    "\n",
    "    def __init__(self, m, n):\n",
    "        \"\"\"Initializes the layer based on input and output dimensions. \n",
    "\n",
    "        Note: Kernel is initialized using normal distribution with mean 0 and \n",
    "        variance 1 / m. All biases are initialized to zero.\n",
    "\n",
    "        Args:\n",
    "            m (int): Number of inputs to the layer.\n",
    "            n (int): Number of outputs from the layer.\n",
    "\n",
    "        \"\"\"\n",
    "        self.m, self.n = m, n\n",
    "\n",
    "        self.W0 = np.zeros((n, 1))\n",
    "        self.W = np.random.normal(0, np.sqrt(1 / m), (m, n))\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"Computes the forward pass through the linear network for a batch.\n",
    "\n",
    "        Args:\n",
    "            A (ndarray): An m by b matrix representing the m activations from the\n",
    "                previous layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the result of passing the \n",
    "                activations through the network layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "\n",
    "        return self.W.T @ self.A + self.W0\n",
    "\n",
    "    def backward(self, dLdZ):\n",
    "        \"\"\"Uses the gradient of loss with respect to outputs of the layer for a \n",
    "        batch to update the sum of gradients of the loss with respect to the \n",
    "        weights for the entire batch. Also returns the gradient of the loss with \n",
    "        respect to the inputs to the layer for a batch.\n",
    "\n",
    "        Args:\n",
    "            dLdZ (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the layer outputs for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An m by b matrix representing the gradient of the loss with \n",
    "                respect to the inputs to the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.dLdW = self.A @ dLdZ.T  # Implicit sum over all b\n",
    "        self.dLdW0 = np.sum(dLdZ, axis=1, keepdims=True)\n",
    "\n",
    "        return self.W @ dLdZ\n",
    "\n",
    "    def sgd_step(self, lrate):\n",
    "        \"\"\"Performs a single step of gradient descent to update the weights for a \n",
    "        single batch of points.\n",
    "\n",
    "        Args:\n",
    "            lrate (float): A learning rate to scale the gradient for the update.\n",
    "\n",
    "        \"\"\"\n",
    "        self.W = self.W - lrate * self.dLdW\n",
    "        self.W0 = self.W0 - lrate * self.dLdW0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    \"\"\"Hyperbolic tangent activation layer.\"\"\"\n",
    "\n",
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the output of the hyperbolic tangent activation layer.\n",
    "\n",
    "        Args:\n",
    "            Z (ndarray): An n by b matrix representing the input pre-activations\n",
    "                of the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the output of the layer after\n",
    "                using the hyperbolic tangent activation on all inputs for a batch\n",
    "                of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = np.tanh(Z)\n",
    "\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return (1 - self.A ** 2) * dLdA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"Rectified linear unit layer.\"\"\"\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the output of the rectified linear unit layer.\n",
    "        \n",
    "        Args:\n",
    "            Z (ndarray): An n by b matrix representing the input pre-activations\n",
    "                of the layer for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the output of the layer after\n",
    "                using the rectified linear activation on all inputs for a batch\n",
    "                of size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.A = np.maximum(0, Z)\n",
    "        \n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return dLdA * (self.A != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Layer):\n",
    "    \"\"\"Softmax activation layer.\"\"\"\n",
    "\n",
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the softmax activation given the inputs from the previous\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            Z (ndarray): An n by b matrix representing the inputs to the softmax\n",
    "                layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix of outputs from softmax for a batch of \n",
    "                size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = np.exp(Z) / np.sum(np.exp(Z), axis=0, keepdims=True)\n",
    "        \n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "                \n",
    "        \"\"\"\n",
    "        n, _ = dLdA.shape\n",
    "        \n",
    "        dAdZ = np.einsum('jk,jk,ji->ijk', self.A, 1 - self.A, np.eye(n)) \\\n",
    "                + np.einsum('jk,ik,ji->ijk', -self.A, self.A, 1 - np.eye(n))\n",
    "        \n",
    "        return np.einsum('ikj,kj->ij', dAdZ, dLdA)\n",
    "\n",
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        \"\"\"Computes the index of maximum value given the softmax outputs from a\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            Ypred (ndarray): An n by b matrix representing the softmax outputs of a\n",
    "                layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: A 1 by b row vectors representing the indices of maximum value\n",
    "                for each output from a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.argmax(Ypred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([[1, 0],\n",
    "              [0, 0],\n",
    "              [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[5, 4],\n",
    "              [7, 9],\n",
    "              [3, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAdZ0 = np.array([[A[0,0]*(1-A[0,0]),    -A[0,0]*A[1,0],    -A[0,0]*A[2,0]],\n",
    "                  [   -A[1,0]*A[0,0], A[1,0]*(1-A[1,0]),    -A[1,0]*A[2,0]],\n",
    "                  [   -A[2,0]*A[0,0],    -A[2,0]*A[1,0], A[2,0]*(1-A[2,0])]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-20, -35, -15],\n",
       "       [-35, -42, -21],\n",
       "       [-15, -21,  -6]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAdZ0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAdZ = np.einsum('ji,jk->ijk', A * (1 - A), np.eye(3)) \\\n",
    "       + np.einsum('ki,ji,jk->ijk', -A, A, np.ones((3, 3)) - np.eye(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-20., -35., -15.],\n",
       "        [-35., -42., -21.],\n",
       "        [-15., -21.,  -6.]],\n",
       "\n",
       "       [[-12., -36.,  -8.],\n",
       "        [-36., -72., -18.],\n",
       "        [ -8., -18.,  -2.]]])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAdZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdA = -Y / A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2,  0. ],\n",
       "       [ 0. ,  0. ],\n",
       "       [ 0. , -0.5]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdZ0 = dAdZ0 @ dLdA[:, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.],\n",
       "       [7.],\n",
       "       [3.]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdZ0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdZ = np.einsum('jik,kj->ij', dAdZ, dLdA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 4.],\n",
       "       [7., 9.],\n",
       "       [3., 1.]])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLL(Layer):\n",
    "    \"\"\"Negative log-likelihood loss layer.\"\"\"\n",
    "\n",
    "    def forward(self, Ypred, Y):\n",
    "        \"\"\"Computes the loss given the predicted and actual results.\n",
    "\n",
    "        Args:\n",
    "            Ypred (ndarray): An n by b matrix representing the predicted results\n",
    "                from the network for a batch of size b.\n",
    "            Y (ndarray): An n by b matrix representing the actual expected results\n",
    "                for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            float: A scalar representing the total loss for each of the outputs\n",
    "                in a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "\n",
    "        return -np.sum(self.Y * np.log(self.Ypred))\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"Computes the gradient of the loss with respect to predicted targets for\n",
    "        a single batch.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of loss with\n",
    "                respect to predicted targets for a batch of size b.\n",
    "                \n",
    "        \"\"\"\n",
    "        return -self.Y / self.Ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"A standard neural network model with linear stacked layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, modules, loss):\n",
    "        \"\"\"Initialize the modules and the loss for the network.\n",
    "        \n",
    "        Args:\n",
    "            modules (list of Module): A list of modules to make up the linear\n",
    "                neural network.\n",
    "            loss (Module): A final module to use to compute the loss of the\n",
    "                neural network.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "\n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):\n",
    "        \"\"\"Trains the neural network by running stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): A d by n matrix representing n training data points\n",
    "                each with d dimensions.\n",
    "            Y (ndarray): A 1 by n matrix representing n training labels.\n",
    "            iters (int): The number of iterations to run stochastic graident\n",
    "                descent.\n",
    "            lrate (float): The step size for stochastic gradient descent.\n",
    "        \n",
    "        \"\"\"\n",
    "        d, n = X.shape\n",
    "        \n",
    "        for it in range(iters):\n",
    "            \n",
    "            t = np.random.randint(n)\n",
    "            \n",
    "            Xt = X[:, t:t + 1]\n",
    "            Yt = Y[:, t:t + 1]\n",
    "            \n",
    "            loss = self.loss.forward(self.forward(Xt), Yt)\n",
    "            self.backward(self.loss.backward())      \n",
    "            \n",
    "            self.print_accuracy(it, X, Y, loss)\n",
    "            \n",
    "            self.sgd_step(lrate)\n",
    "\n",
    "    def forward(self, Xt):\n",
    "        \"\"\"Predicts the output for a training input batch.\n",
    "        \n",
    "        Args:\n",
    "            Xt (ndarray): A d by b matrix of points to predict\n",
    "                with dimension d and batch size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: A 1 by b matrix representing the predicted\n",
    "                outputs of the neural network for a batch size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        for m in self.modules:\n",
    "            Xt = m.forward(Xt)\n",
    "            \n",
    "        return Xt\n",
    "\n",
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradients of the loss with respect to each weight\n",
    "        in the neural network to prepare for stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the\n",
    "                loss with respect to the outputs of the neural network for a\n",
    "                batch of size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        for m in self.modules[::-1]:\n",
    "            dLdA = m.backward(dLdA)\n",
    "\n",
    "    def sgd_step(self, lrate):\n",
    "        \"\"\"Runs a single update step on the weight matrices throughout the\n",
    "        neural network using stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            lrate (float): Learning rate for the update step.\n",
    "        \n",
    "        \"\"\"\n",
    "        for m in self.modules:\n",
    "            m.sgd_step(lrate)\n",
    "\n",
    "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
    "        \"\"\"Displays current prediction statistics.\n",
    "        \n",
    "        Args:\n",
    "            it (int): Current iteration.\n",
    "            X (ndarray): A d by n matrix of n points to evaluate, each with\n",
    "                d dimensions.\n",
    "            Y (ndarray): A 1 by n vector of n labels.\n",
    "            cur_loss (float): Current loss.\n",
    "            every (int): Frequency to output statistics.\n",
    "        \n",
    "        \"\"\"\n",
    "        if it % every == 1:\n",
    "            \n",
    "            cf = self.modules[-1].class_fun\n",
    "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "            \n",
    "            print('Iteration =', it, '\\tAcc =', acc, '\\tLoss =', cur_loss, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, Y = hard()\n",
    "nn = Sequential([Linear(2, 10), ReLU(), \n",
    "                 Linear(10, 10), ReLU(), \n",
    "                 Linear(10, 2), SoftMax()], NLL())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
