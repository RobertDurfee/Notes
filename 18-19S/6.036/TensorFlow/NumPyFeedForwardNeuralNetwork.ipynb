{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy Feed-Forward Neural Network\n",
    "\n",
    "This notebook walks through the process of constructing a feed-forward neural network for multi-class classification solely using NumPy.\n",
    "\n",
    "## Layers\n",
    "\n",
    "For our neural network, we want to abstract away from individual neurons and focus on layers. Each element of the network will be defined by a certain layer.\n",
    "\n",
    "### Base Layer\n",
    "\n",
    "The abstract base layer ensures that all called methods by the `Sequential` model exist on the layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Base class for neural network layers.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we only need to worry about the `sgd_step` method as some layers won't need to update any weights because they don't have any (e.g. activation layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, lrate):\n",
    "        \"\"\"Some layers do not have weights to update on gradient descent steps.\"\"\"\n",
    "\n",
    "# Add this method to the Layer class\n",
    "Layer.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "This is the simplest layer that makes up the majority of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"A simple, fully-connected linear layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up this layer, we need to know the input and output dimensions ahead of time. Using this information, we randomly initialize the weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, m, n):\n",
    "        \"\"\"Initializes the layer based on input and output dimensions. \n",
    "\n",
    "        Note: Kernel is initialized using normal distribution with mean 0 and \n",
    "        variance 1 / m. All biases are initialized to zero.\n",
    "\n",
    "        Args:\n",
    "            m (int): Number of inputs to the layer.\n",
    "            n (int): Number of outputs from the layer.\n",
    "\n",
    "        \"\"\"\n",
    "        self.m, self.n = m, n\n",
    "\n",
    "        self.W0 = np.zeros((n, 1))\n",
    "        self.W = np.random.normal(0, np.sqrt(1 / m), (m, n))\n",
    "\n",
    "# Add this method to the Linear layer class\n",
    "Linear.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method will compute the output of the layer given a set of $m$ inputs from the previous layer for a batch of size $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, A):\n",
    "        \"\"\"Computes the forward pass through the linear network for a batch.\n",
    "\n",
    "        Args:\n",
    "            A (ndarray): An m by b matrix representing the m activations from the\n",
    "                previous layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the result of passing the \n",
    "                activations through the network layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "\n",
    "        return self.W.T @ self.A + self.W0\n",
    "\n",
    "# Add this method to the Linear layer class\n",
    "Linear.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method will compute the gradient of the loss with respect to the inputs to the layer for a batch of size $b$. Note: There is an implicit sum over all $b$ in the `dLdW` calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdZ):\n",
    "        \"\"\"Uses the gradient of loss with respect to outputs of the layer for a \n",
    "        batch to update the sum of gradients of the loss with respect to the \n",
    "        weights for the entire batch. Also returns the gradient of the loss with \n",
    "        respect to the inputs to the layer for a batch.\n",
    "\n",
    "        Args:\n",
    "            dLdZ (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the layer outputs for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An m by b matrix representing the gradient of the loss with \n",
    "                respect to the inputs to the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.dLdW = self.A @ dLdZ.T  # Implicit sum over all b\n",
    "        self.dLdW0 = np.sum(dLdZ, axis=1, keepdims=True)\n",
    "\n",
    "        return self.W @ dLdZ\n",
    "\n",
    "# Add this method to the Linear layer class\n",
    "Linear.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need a method to update the weight matrices using the current weight gradients for a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, lrate):\n",
    "        \"\"\"Performs a single step of gradient descent to update the weights for a \n",
    "        single batch of points.\n",
    "\n",
    "        Args:\n",
    "            lrate (float): A learning rate to scale the gradient for the update.\n",
    "\n",
    "        \"\"\"\n",
    "        self.W = self.W - lrate * self.dLdW\n",
    "        self.W0 = self.W0 - lrate * self.dLdW0\n",
    "        \n",
    "# Add this method to the Linear layer class\n",
    "Linear.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent Activation Layer\n",
    "\n",
    "This layer encapsulates the hyperbolic tangent activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    \"\"\"Hyperbolic tangent activation layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method take a preactivation from the previous layer and computes the activation using the hyperbolic tangent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the output of the hyperbolic tangent activation layer.\n",
    "\n",
    "        Args:\n",
    "            Z (ndarray): An n by b matrix representing the input pre-activations\n",
    "                of the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the output of the layer after\n",
    "                using the hyperbolic tangent activation on all inputs for a batch\n",
    "                of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = np.tanh(Z)\n",
    "\n",
    "        return self.A\n",
    "\n",
    "# Add this method to the Tanh layer class\n",
    "Tanh.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method computes the gradient of the loss with respect to the inputs to the activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return (1 - self.A ** 2) * dLdA\n",
    "\n",
    "# Add this method to the Tanh layer class\n",
    "Tanh.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has no weights to update. Therefore, no `sgd_step` function is required.\n",
    "\n",
    "### Rectified Linear Unit Activation Layer\n",
    "\n",
    "This layer encapsulates the rectified linear unit activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"Rectified linear unit layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method take a preactivation from the previous layer and computes the activation using the relu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the output of the rectified linear unit layer.\n",
    "        \n",
    "        Args:\n",
    "            Z (ndarray): An n by b matrix representing the input pre-activations\n",
    "                of the layer for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the output of the layer after\n",
    "                using the rectified linear activation on all inputs for a batch\n",
    "                of size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.A = np.maximum(0, Z)\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "# Add this method to the ReLU layer class\n",
    "ReLU.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method computes the gradient of the loss with respect to the inputs to the activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.sign(self.A) * dLdA\n",
    "    \n",
    "# Add this method to the ReLU layer class\n",
    "ReLU.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has no weights to update. Therefore, no `sgd_step` function is required.\n",
    "\n",
    "### Softmax Activation Layer\n",
    "\n",
    "This layer encapsulates the softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Layer):\n",
    "    \"\"\"Softmax activation layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method take a preactivation from the previous layer and computes the activation using the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the softmax activation given the inputs from the previous\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            Z (ndarray): An n by b matrix representing the inputs to the softmax\n",
    "                layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix of outputs from softmax for a batch of \n",
    "                size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = np.exp(Z) / np.sum(np.exp(Z), axis=0, keepdims=True)\n",
    "        \n",
    "        return self.A\n",
    "\n",
    "# Add this method to the SoftMax layer class\n",
    "SoftMax.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method computes the gradient of the loss with respect to the inputs to the activation layer. Note that I *do not* assume that $\\partial \\mathrm{Loss} / \\partial Z^L$ is passed in directly. More information on how this works can be found in the 'Einstein Summation' notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "                \n",
    "        \"\"\"\n",
    "        n, _ = dLdA.shape\n",
    "        \n",
    "        dAdZ = np.einsum('jk,jk,ji->ijk', self.A, 1 - self.A, np.eye(n)) \\\n",
    "                + np.einsum('jk,ik,ji->ijk', -self.A, self.A, 1 - np.eye(n))\n",
    "        \n",
    "        return np.einsum('ikj,kj->ij', dAdZ, dLdA)\n",
    "\n",
    "# Add this method to the SoftMax layer class\n",
    "SoftMax.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make prediction a little easier, we also provide a method that will determine the classes of highest probability as returned from a softmax prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        \"\"\"Computes the index of maximum value given the softmax outputs from a\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            Ypred (ndarray): An n by b matrix representing the softmax outputs of a\n",
    "                layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: A 1 by b row vectors representing the indices of maximum value\n",
    "                for each output from a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.argmax(Ypred, axis=0)\n",
    "\n",
    "# Add this method to the SoftMax layer class\n",
    "SoftMax.class_fun = class_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has no weights to update. Therefore, no `sgd_step` function is required.\n",
    "\n",
    "### Negative Log-Likelihood Multi-Class Loss Layer\n",
    "\n",
    "This isn't really a layer, but it functions quite similarly to one. It will take predictions and actual labels and compute the categorical cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLL(Layer):\n",
    "    \"\"\"Negative log-likelihood loss layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method will compute the loss between predicted and actual labels using categorical cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Ypred, Y):\n",
    "        \"\"\"Computes the loss given the predicted and actual results.\n",
    "\n",
    "        Args:\n",
    "            Ypred (ndarray): An n by b matrix representing the predicted results\n",
    "                from the network for a batch of size b.\n",
    "            Y (ndarray): An n by b matrix representing the actual expected results\n",
    "                for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            float: A scalar representing the total loss for each of the outputs\n",
    "                in a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "\n",
    "        return -np.sum(self.Y * np.log(self.Ypred))\n",
    "\n",
    "# Add this method to the NLL layer class\n",
    "NLL.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method will compute the gradient of the loss with respect to the predicted outputs from the network. (Note: this is *not* in terms of the pre-activations, but the actual activations. To learn more about this, look at the 'Einstein Summation' notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self):\n",
    "        \"\"\"Computes the gradient of the loss with respect to predicted targets for\n",
    "        a single batch.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of loss with\n",
    "                respect to predicted targets for a batch of size b.\n",
    "                \n",
    "        \"\"\"\n",
    "        return -self.Y / self.Ypred\n",
    "\n",
    "# Add this method to the NLL layer class\n",
    "NLL.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer has no weights to update. Therefore, no `sgd_step` function is required.\n",
    "\n",
    "## Model\n",
    "\n",
    "Now that we have all the components to make up a simple neural network, we can combine them together into a model.\n",
    "\n",
    "### Sequential Model\n",
    "\n",
    "This is the simplest type of model which just linearly stacks each layer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"A standard neural network model with linear stacked layers.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do anything with the model, we need to know what layers should be included and what loss should be used to compute gradient updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, layers, loss):\n",
    "        \"\"\"Initialize the layers and the loss for the network.\n",
    "        \n",
    "        Args:\n",
    "            layers (list of Layer): A list of layers to make up the linear\n",
    "                neural network.\n",
    "            loss (Layer): A final layer to use to compute the loss of the\n",
    "                neural network.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "\n",
    "# Add this method to the Sequential model class\n",
    "Sequential.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions with the network, we use the `forward` method. This passes the data through every layer and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Xt):\n",
    "        \"\"\"Predicts the output for a training input batch.\n",
    "        \n",
    "        Args:\n",
    "            Xt (ndarray): A d by b matrix of points to predict\n",
    "                with dimension d and batch size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: A 1 by b matrix representing the predicted\n",
    "                outputs of the neural network for a batch size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            Xt = layer.forward(Xt)\n",
    "            \n",
    "        return Xt\n",
    "\n",
    "# Add this method to the Sequential model class\n",
    "Sequential.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we will use stochastic gradient descent. Before we define the stochastic gradient descent training loop, we have to back-propogate the error throughout the layers of the network. To do this, we use the `backward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradients of the loss with respect to each weight\n",
    "        in the neural network to prepare for stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the\n",
    "                loss with respect to the outputs of the neural network for a\n",
    "                batch of size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers[::-1]:\n",
    "            dLdA = layer.backward(dLdA)\n",
    "\n",
    "# Add this method to the Sequential model class\n",
    "Sequential.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the error is propogated through all the layers, each layer can update their weight matrices. For a single step, this is achieved through the `sgd_step` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, lrate):\n",
    "        \"\"\"Runs a single update step on the weight matrices throughout the\n",
    "        neural network using stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            lrate (float): Learning rate for the update step.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.sgd_step(lrate)\n",
    "\n",
    "# Add this method to the Sequential model class\n",
    "Sequential.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop over the data applying many stochastic gradient descent update steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd(self, X, Y, iters=100, lrate=0.005):\n",
    "        \"\"\"Trains the neural network by running stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): A d by n matrix representing n training data points\n",
    "                each with d dimensions.\n",
    "            Y (ndarray): A 1 by n matrix representing n training labels.\n",
    "            iters (int): The number of iterations to run stochastic graident\n",
    "                descent.\n",
    "            lrate (float): The step size for stochastic gradient descent.\n",
    "        \n",
    "        \"\"\"\n",
    "        _, n = X.shape\n",
    "        \n",
    "        for it in range(iters):\n",
    "            \n",
    "            t = np.random.randint(n)\n",
    "            \n",
    "            Xt = X[:, t:t + 1]\n",
    "            Yt = Y[:, t:t + 1]\n",
    "            \n",
    "            loss = self.loss.forward(self.forward(Xt), Yt)\n",
    "            self.backward(self.loss.backward())      \n",
    "            \n",
    "            self.print_accuracy(it, X, Y, loss)\n",
    "            \n",
    "            self.sgd_step(lrate)\n",
    "            \n",
    "# Add this method to the Sequential model class\n",
    "Sequential.sgd = sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a helper method `print_accuracy` to display our progress as we train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
    "        \"\"\"Displays current prediction statistics.\n",
    "        \n",
    "        Args:\n",
    "            it (int): Current iteration.\n",
    "            X (ndarray): A d by n matrix of n points to evaluate, each with\n",
    "                d dimensions.\n",
    "            Y (ndarray): A 1 by n vector of n labels.\n",
    "            cur_loss (float): Current loss.\n",
    "            every (int): Frequency to output statistics.\n",
    "        \n",
    "        \"\"\"\n",
    "        if it % every == 1:\n",
    "            \n",
    "            cf = self.layers[-1].class_fun\n",
    "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "            \n",
    "            print('Iteration =', it, '\\tAcc =', acc, '\\tLoss =', cur_loss, flush=True)\n",
    "\n",
    "# Add this method to the Sequential model class\n",
    "Sequential.print_accuracy = print_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that we have a model, let's train it on some data and see how well it can classify. We will use the standard 'hard' data set used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[-0.23390341,  1.18151883, -2.46493986,  1.55322202,  1.27621763,\n",
    "                2.39710997, -1.34403040, -0.46903436, -0.64673502, -1.44029872,\n",
    "               -1.37537243,  1.05994811, -0.93311512,  1.02735575, -0.84138778,\n",
    "               -2.22585412, -0.42591102,  1.03561105,  0.91125595, -2.26550369],\n",
    "              [-0.92254932, -1.10309630, -2.41956036, -1.15509002, -1.04805327,\n",
    "                0.08717325,  0.81847250, -0.75171045,  0.60664705,  0.80410947,\n",
    "               -0.11600488,  1.03747218, -0.67210575,  0.99944446, -0.65559838,\n",
    "               -0.40744784, -0.58367642,  1.05972780, -0.95991874, -1.41720255]])\n",
    "\n",
    "Y = np.array([[0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
    "              [1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.scatter(X[0,:], X[1,:], c=Y[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct a neural network we think might be able to classify these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Linear(2, 10), ReLU(), \n",
    "                    Linear(10, 10), ReLU(), \n",
    "                    Linear(10, 2), SoftMax()], NLL())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training the model on the data for a few thousand iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.7 \tLoss = 0.3934914193303726\n",
      "Iteration = 251 \tAcc = 0.9 \tLoss = 0.12411735563525321\n",
      "Iteration = 501 \tAcc = 0.95 \tLoss = 0.44936215058337015\n",
      "Iteration = 751 \tAcc = 0.95 \tLoss = 0.023703123508501126\n",
      "Iteration = 1001 \tAcc = 0.95 \tLoss = 0.03389847921897066\n",
      "Iteration = 1251 \tAcc = 0.95 \tLoss = 0.040771498401040975\n",
      "Iteration = 1501 \tAcc = 0.95 \tLoss = 0.04041583067233898\n",
      "Iteration = 1751 \tAcc = 0.95 \tLoss = 0.005750435344733054\n",
      "Iteration = 2001 \tAcc = 0.95 \tLoss = 0.22655461864745374\n",
      "Iteration = 2251 \tAcc = 0.95 \tLoss = 0.0010722377951524911\n",
      "Iteration = 2501 \tAcc = 0.95 \tLoss = 0.05172183453643268\n",
      "Iteration = 2751 \tAcc = 0.95 \tLoss = 0.022242247645096684\n",
      "Iteration = 3001 \tAcc = 0.95 \tLoss = 0.03157322213905229\n",
      "Iteration = 3251 \tAcc = 0.95 \tLoss = 0.07051094667061675\n",
      "Iteration = 3501 \tAcc = 0.95 \tLoss = 0.37745435765886864\n",
      "Iteration = 3751 \tAcc = 0.95 \tLoss = 1.878718234366643e-05\n",
      "Iteration = 4001 \tAcc = 0.95 \tLoss = 0.0009934308898585354\n",
      "Iteration = 4251 \tAcc = 0.95 \tLoss = 0.024471157611222035\n",
      "Iteration = 4501 \tAcc = 0.95 \tLoss = 1.8575603218133008\n",
      "Iteration = 4751 \tAcc = 0.95 \tLoss = 0.03700767043415097\n",
      "Iteration = 5001 \tAcc = 0.95 \tLoss = 0.0003637526769248513\n",
      "Iteration = 5251 \tAcc = 0.95 \tLoss = 0.0016443918592251529\n",
      "Iteration = 5501 \tAcc = 0.95 \tLoss = 0.016315097498260225\n",
      "Iteration = 5751 \tAcc = 0.95 \tLoss = 0.005520203096089413\n",
      "Iteration = 6001 \tAcc = 0.95 \tLoss = 0.21678804870759608\n",
      "Iteration = 6251 \tAcc = 0.95 \tLoss = 0.005905096402422633\n",
      "Iteration = 6501 \tAcc = 0.95 \tLoss = 0.2274420308802165\n",
      "Iteration = 6751 \tAcc = 0.95 \tLoss = 0.006146658603942413\n",
      "Iteration = 7001 \tAcc = 0.95 \tLoss = 8.388903693246218e-05\n",
      "Iteration = 7251 \tAcc = 0.95 \tLoss = 0.14282110077083526\n",
      "Iteration = 7501 \tAcc = 0.95 \tLoss = 0.008909254308614273\n",
      "Iteration = 7751 \tAcc = 0.95 \tLoss = 0.0006578489613627756\n",
      "Iteration = 8001 \tAcc = 0.95 \tLoss = 0.36659338779338\n",
      "Iteration = 8251 \tAcc = 0.95 \tLoss = 5.2301852444303974e-05\n",
      "Iteration = 8501 \tAcc = 0.95 \tLoss = 0.12122549269248557\n",
      "Iteration = 8751 \tAcc = 0.95 \tLoss = 0.003907392710635276\n",
      "Iteration = 9001 \tAcc = 0.95 \tLoss = 1.1447667991540584e-07\n",
      "Iteration = 9251 \tAcc = 0.95 \tLoss = 0.3109134351847943\n",
      "Iteration = 9501 \tAcc = 0.95 \tLoss = 0.04491861277223132\n",
      "Iteration = 9751 \tAcc = 0.95 \tLoss = 1.670809796435294e-06\n",
      "Iteration = 10001 \tAcc = 0.95 \tLoss = 0.023123408614060113\n",
      "Iteration = 10251 \tAcc = 0.95 \tLoss = 0.10802438458168738\n",
      "Iteration = 10501 \tAcc = 0.95 \tLoss = 0.0024395282364348043\n",
      "Iteration = 10751 \tAcc = 0.95 \tLoss = 0.0003404127472996712\n",
      "Iteration = 11001 \tAcc = 0.95 \tLoss = 1.885885898575243e-08\n",
      "Iteration = 11251 \tAcc = 0.95 \tLoss = 1.471182008711023\n",
      "Iteration = 11501 \tAcc = 0.95 \tLoss = 0.002016508527984569\n",
      "Iteration = 11751 \tAcc = 0.95 \tLoss = 0.08919078398558873\n",
      "Iteration = 12001 \tAcc = 0.95 \tLoss = 0.09182961072662747\n",
      "Iteration = 12251 \tAcc = 0.95 \tLoss = 1.8389813253361964e-07\n",
      "Iteration = 12501 \tAcc = 0.95 \tLoss = 0.22117675965425213\n",
      "Iteration = 12751 \tAcc = 0.95 \tLoss = 0.00018746357700072621\n",
      "Iteration = 13001 \tAcc = 0.95 \tLoss = 0.05970545401333278\n",
      "Iteration = 13251 \tAcc = 0.95 \tLoss = 0.08122096671588885\n",
      "Iteration = 13501 \tAcc = 0.95 \tLoss = 1.75815866691336e-05\n",
      "Iteration = 13751 \tAcc = 0.95 \tLoss = 5.871966518893402e-08\n",
      "Iteration = 14001 \tAcc = 0.95 \tLoss = 0.06255766917989933\n",
      "Iteration = 14251 \tAcc = 0.95 \tLoss = 4.4093819413714643e-07\n",
      "Iteration = 14501 \tAcc = 0.95 \tLoss = 7.62635177522436e-10\n",
      "Iteration = 14751 \tAcc = 0.95 \tLoss = 1.038991015833111e-05\n",
      "Iteration = 15001 \tAcc = 0.95 \tLoss = 2.736052821765566e-08\n",
      "Iteration = 15251 \tAcc = 0.95 \tLoss = 0.22499875375866335\n",
      "Iteration = 15501 \tAcc = 0.95 \tLoss = 0.2945129976797181\n",
      "Iteration = 15751 \tAcc = 0.95 \tLoss = 1.749194671085233e-08\n",
      "Iteration = 16001 \tAcc = 0.95 \tLoss = 2.093059224048725e-07\n",
      "Iteration = 16251 \tAcc = 0.95 \tLoss = 0.042052455763472205\n",
      "Iteration = 16501 \tAcc = 0.95 \tLoss = 0.21347137422751009\n",
      "Iteration = 16751 \tAcc = 0.95 \tLoss = 0.07612743922247481\n",
      "Iteration = 17001 \tAcc = 0.95 \tLoss = 0.06838861721535214\n",
      "Iteration = 17251 \tAcc = 0.95 \tLoss = 2.5664700105583724e-07\n",
      "Iteration = 17501 \tAcc = 0.95 \tLoss = 0.057134589510469076\n",
      "Iteration = 17751 \tAcc = 0.95 \tLoss = 1.0087041190587642\n",
      "Iteration = 18001 \tAcc = 0.95 \tLoss = 0.3810845860630388\n",
      "Iteration = 18251 \tAcc = 0.95 \tLoss = 0.09156134527787897\n",
      "Iteration = 18501 \tAcc = 0.95 \tLoss = 6.828119081228342e-08\n",
      "Iteration = 18751 \tAcc = 0.95 \tLoss = 0.035472509064088574\n",
      "Iteration = 19001 \tAcc = 0.95 \tLoss = 1.0659193396777618\n",
      "Iteration = 19251 \tAcc = 0.95 \tLoss = 0.00033005968623555537\n",
      "Iteration = 19501 \tAcc = 1.0 \tLoss = 2.583826530585253e-08\n",
      "Iteration = 19751 \tAcc = 0.95 \tLoss = 7.852961888435339e-05\n",
      "Iteration = 20001 \tAcc = 0.95 \tLoss = 1.317567973142155\n",
      "Iteration = 20251 \tAcc = 1.0 \tLoss = 0.02766151505308017\n",
      "Iteration = 20501 \tAcc = 0.95 \tLoss = 9.960496735280706e-07\n",
      "Iteration = 20751 \tAcc = 0.95 \tLoss = 4.1805551349134185e-05\n",
      "Iteration = 21001 \tAcc = 0.95 \tLoss = 0.06414696900650756\n",
      "Iteration = 21251 \tAcc = 0.95 \tLoss = 1.2157221636679781e-08\n",
      "Iteration = 21501 \tAcc = 0.95 \tLoss = 0.0013076460887208813\n",
      "Iteration = 21751 \tAcc = 0.95 \tLoss = 3.9267370215665495e-07\n",
      "Iteration = 22001 \tAcc = 1.0 \tLoss = 0.07595080837431376\n",
      "Iteration = 22251 \tAcc = 0.95 \tLoss = 0.013386410885680823\n",
      "Iteration = 22501 \tAcc = 1.0 \tLoss = 2.4783066993148692e-05\n",
      "Iteration = 22751 \tAcc = 1.0 \tLoss = 0.09934522140564246\n",
      "Iteration = 23001 \tAcc = 1.0 \tLoss = 0.0579718263284432\n",
      "Iteration = 23251 \tAcc = 0.95 \tLoss = 0.0041144144411106144\n",
      "Iteration = 23501 \tAcc = 0.95 \tLoss = 6.905184115029016e-09\n",
      "Iteration = 23751 \tAcc = 1.0 \tLoss = 0.6836741670282984\n",
      "Iteration = 24001 \tAcc = 0.95 \tLoss = 0.0060707345340905395\n",
      "Iteration = 24251 \tAcc = 1.0 \tLoss = 2.8133051444005425e-13\n",
      "Iteration = 24501 \tAcc = 0.95 \tLoss = 1.3917419616065288e-05\n",
      "Iteration = 24751 \tAcc = 1.0 \tLoss = 1.1471228418187535e-09\n",
      "Iteration = 25001 \tAcc = 0.95 \tLoss = 1.245177941674306e-07\n",
      "Iteration = 25251 \tAcc = 1.0 \tLoss = 0.0003787526171783812\n",
      "Iteration = 25501 \tAcc = 0.95 \tLoss = 0.012681246513480217\n",
      "Iteration = 25751 \tAcc = 0.95 \tLoss = 9.47321221915943e-10\n",
      "Iteration = 26001 \tAcc = 1.0 \tLoss = 9.121441268795761e-06\n",
      "Iteration = 26251 \tAcc = 0.95 \tLoss = 0.004402865878884308\n",
      "Iteration = 26501 \tAcc = 0.95 \tLoss = 0.03532531107148847\n",
      "Iteration = 26751 \tAcc = 0.95 \tLoss = 1.1783860006365048\n",
      "Iteration = 27001 \tAcc = 1.0 \tLoss = 0.032707534992321145\n",
      "Iteration = 27251 \tAcc = 1.0 \tLoss = 5.340034215624308e-06\n",
      "Iteration = 27501 \tAcc = 1.0 \tLoss = 5.568657961768283e-06\n",
      "Iteration = 27751 \tAcc = 0.95 \tLoss = 2.3663186505511353e-06\n",
      "Iteration = 28001 \tAcc = 0.95 \tLoss = 1.012471850582188e-05\n",
      "Iteration = 28251 \tAcc = 0.95 \tLoss = 0.010517402470713462\n",
      "Iteration = 28501 \tAcc = 1.0 \tLoss = 4.672769853158493e-06\n",
      "Iteration = 28751 \tAcc = 0.95 \tLoss = 0.024861064877329215\n",
      "Iteration = 29001 \tAcc = 1.0 \tLoss = 2.5103188168521037e-06\n",
      "Iteration = 29251 \tAcc = 1.0 \tLoss = 1.0214051826551492e-14\n",
      "Iteration = 29501 \tAcc = 1.0 \tLoss = 0.003955088241459176\n",
      "Iteration = 29751 \tAcc = 0.95 \tLoss = 9.699736580970828e-06\n",
      "Iteration = 30001 \tAcc = 0.95 \tLoss = 0.01772088369910615\n",
      "Iteration = 30251 \tAcc = 1.0 \tLoss = 0.023310736535270868\n",
      "Iteration = 30501 \tAcc = 0.95 \tLoss = 4.591255025502915e-06\n",
      "Iteration = 30751 \tAcc = 1.0 \tLoss = 0.021656995368061076\n",
      "Iteration = 31001 \tAcc = 1.0 \tLoss = 1.2013723349476035e-12\n",
      "Iteration = 31251 \tAcc = 1.0 \tLoss = 6.40573122334162e-05\n",
      "Iteration = 31501 \tAcc = 0.95 \tLoss = 0.0017932204784147456\n",
      "Iteration = 31751 \tAcc = 1.0 \tLoss = 0.003926802347462158\n",
      "Iteration = 32001 \tAcc = 1.0 \tLoss = 1.887379141862768e-15\n",
      "Iteration = 32251 \tAcc = 1.0 \tLoss = 2.0911170261143633e-06\n",
      "Iteration = 32501 \tAcc = 1.0 \tLoss = 0.46287514310112204\n",
      "Iteration = 32751 \tAcc = 1.0 \tLoss = 0.0023608138123673915\n",
      "Iteration = 33001 \tAcc = 0.95 \tLoss = 0.011627492309372885\n",
      "Iteration = 33251 \tAcc = 1.0 \tLoss = 6.205226458888957e-06\n",
      "Iteration = 33501 \tAcc = 0.95 \tLoss = 0.019975100986406858\n",
      "Iteration = 33751 \tAcc = 0.95 \tLoss = 0.0010853395503855868\n",
      "Iteration = 34001 \tAcc = 1.0 \tLoss = 4.1757042269258435e-11\n",
      "Iteration = 34251 \tAcc = 1.0 \tLoss = 5.6372771723040975e-06\n",
      "Iteration = 34501 \tAcc = 0.95 \tLoss = 0.0015155736554504126\n",
      "Iteration = 34751 \tAcc = 1.0 \tLoss = 0.02056975229631056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 35001 \tAcc = 0.95 \tLoss = 2.5307850291916825e-09\n",
      "Iteration = 35251 \tAcc = 1.0 \tLoss = 1.2328471577026046e-11\n",
      "Iteration = 35501 \tAcc = 0.95 \tLoss = 5.046543867981812e-06\n",
      "Iteration = 35751 \tAcc = 0.95 \tLoss = 1.4032338062493788e-06\n",
      "Iteration = 36001 \tAcc = 1.0 \tLoss = 0.36628446991454305\n",
      "Iteration = 36251 \tAcc = 0.95 \tLoss = 0.005117248166227754\n",
      "Iteration = 36501 \tAcc = 0.95 \tLoss = 4.850334341761312e-06\n",
      "Iteration = 36751 \tAcc = 1.0 \tLoss = 0.006711691691118829\n",
      "Iteration = 37001 \tAcc = 1.0 \tLoss = 7.527402231332379e-07\n",
      "Iteration = 37251 \tAcc = 1.0 \tLoss = 0.00164209628353335\n",
      "Iteration = 37501 \tAcc = 1.0 \tLoss = 1.9573898058147855e-11\n",
      "Iteration = 37751 \tAcc = 1.0 \tLoss = 1.884660651540642e-09\n",
      "Iteration = 38001 \tAcc = 1.0 \tLoss = 0.6122466958115572\n",
      "Iteration = 38251 \tAcc = 0.95 \tLoss = 0.00039516640914568805\n",
      "Iteration = 38501 \tAcc = 1.0 \tLoss = 3.022500024070885e-07\n",
      "Iteration = 38751 \tAcc = 1.0 \tLoss = 0.01229536755774261\n",
      "Iteration = 39001 \tAcc = 0.95 \tLoss = 4.408331968695247e-06\n",
      "Iteration = 39251 \tAcc = 0.95 \tLoss = 5.939693181744764e-14\n",
      "Iteration = 39501 \tAcc = 1.0 \tLoss = 4.159858348793186e-06\n",
      "Iteration = 39751 \tAcc = 1.0 \tLoss = 1.3275736074832986e-09\n",
      "Iteration = 40001 \tAcc = 0.95 \tLoss = 1.382893799482657e-11\n",
      "Iteration = 40251 \tAcc = 1.0 \tLoss = 0.016949832861158\n",
      "Iteration = 40501 \tAcc = 1.0 \tLoss = 0.011344294196807393\n",
      "Iteration = 40751 \tAcc = 1.0 \tLoss = 4.0867905374690795e-06\n",
      "Iteration = 41001 \tAcc = 1.0 \tLoss = 0.002658496782794299\n",
      "Iteration = 41251 \tAcc = 1.0 \tLoss = 0.004176699375675386\n",
      "Iteration = 41501 \tAcc = 1.0 \tLoss = 6.897080304598236e-06\n",
      "Iteration = 41751 \tAcc = 1.0 \tLoss = 9.144561778657907e-10\n",
      "Iteration = 42001 \tAcc = 1.0 \tLoss = 4.99139929280816e-07\n",
      "Iteration = 42251 \tAcc = 0.95 \tLoss = 4.0134313666041784e-07\n",
      "Iteration = 42501 \tAcc = 1.0 \tLoss = 0.011460235507196662\n",
      "Iteration = 42751 \tAcc = 1.0 \tLoss = 0.0019676641003080184\n",
      "Iteration = 43001 \tAcc = 0.95 \tLoss = 0.026220940038076103\n",
      "Iteration = 43251 \tAcc = 1.0 \tLoss = 0.0038077427503913287\n",
      "Iteration = 43501 \tAcc = 1.0 \tLoss = 0.013239124392049002\n",
      "Iteration = 43751 \tAcc = 1.0 \tLoss = 0.2660273107895684\n",
      "Iteration = 44001 \tAcc = 1.0 \tLoss = 0.5716575360145593\n",
      "Iteration = 44251 \tAcc = 1.0 \tLoss = 8.048898217835772e-10\n",
      "Iteration = 44501 \tAcc = 0.95 \tLoss = 3.0182933553810824e-06\n",
      "Iteration = 44751 \tAcc = 1.0 \tLoss = 0.0035366642819532113\n",
      "Iteration = 45001 \tAcc = 0.95 \tLoss = 1.0111865707710124\n",
      "Iteration = 45251 \tAcc = 0.95 \tLoss = 0.0035738422031461967\n",
      "Iteration = 45501 \tAcc = 1.0 \tLoss = 0.3167424358468553\n",
      "Iteration = 45751 \tAcc = 1.0 \tLoss = 0.0019947420688018512\n",
      "Iteration = 46001 \tAcc = 1.0 \tLoss = 0.5354207585014057\n",
      "Iteration = 46251 \tAcc = 1.0 \tLoss = 0.008966221184279654\n",
      "Iteration = 46501 \tAcc = 1.0 \tLoss = 0.0030111096437103048\n",
      "Iteration = 46751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 47001 \tAcc = 1.0 \tLoss = 0.002830444983547576\n",
      "Iteration = 47251 \tAcc = 0.95 \tLoss = 0.0029955132847871296\n",
      "Iteration = 47501 \tAcc = 1.0 \tLoss = 0.00909699603686445\n",
      "Iteration = 47751 \tAcc = 1.0 \tLoss = 0.24728801110150908\n",
      "Iteration = 48001 \tAcc = 1.0 \tLoss = 0.002494293699675775\n",
      "Iteration = 48251 \tAcc = 0.95 \tLoss = 0.0031482097250973852\n",
      "Iteration = 48501 \tAcc = 1.0 \tLoss = 3.4143798899380355e-12\n",
      "Iteration = 48751 \tAcc = 1.0 \tLoss = 3.3969196779073757e-06\n",
      "Iteration = 49001 \tAcc = 1.0 \tLoss = 0.36392562409533613\n",
      "Iteration = 49251 \tAcc = 1.0 \tLoss = 4.921988637889371e-08\n",
      "Iteration = 49501 \tAcc = 1.0 \tLoss = 3.6859613459483603e-06\n",
      "Iteration = 49751 \tAcc = 1.0 \tLoss = 0.26761096819676305\n",
      "Iteration = 50001 \tAcc = 1.0 \tLoss = 3.927176091281717e-06\n",
      "Iteration = 50251 \tAcc = 1.0 \tLoss = 0.012283112949703459\n",
      "Iteration = 50501 \tAcc = 1.0 \tLoss = 2.4842136468050583e-06\n",
      "Iteration = 50751 \tAcc = 1.0 \tLoss = 8.659739592076259e-15\n",
      "Iteration = 51001 \tAcc = 1.0 \tLoss = 0.0007796367637666282\n",
      "Iteration = 51251 \tAcc = 1.0 \tLoss = 0.4528568220586623\n",
      "Iteration = 51501 \tAcc = 0.95 \tLoss = 0.002323021077062115\n",
      "Iteration = 51751 \tAcc = 1.0 \tLoss = 0.008322028136511079\n",
      "Iteration = 52001 \tAcc = 1.0 \tLoss = 5.004228144231742e-10\n",
      "Iteration = 52251 \tAcc = 1.0 \tLoss = 3.6481113332494798e-06\n",
      "Iteration = 52501 \tAcc = 1.0 \tLoss = 3.7710877420851857e-06\n",
      "Iteration = 52751 \tAcc = 1.0 \tLoss = 0.008929214166181556\n",
      "Iteration = 53001 \tAcc = 1.0 \tLoss = 5.240134993963079e-10\n",
      "Iteration = 53251 \tAcc = 1.0 \tLoss = 3.612216377388819e-06\n",
      "Iteration = 53501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 53751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 54001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 54251 \tAcc = 1.0 \tLoss = 7.362391810029809e-10\n",
      "Iteration = 54501 \tAcc = 1.0 \tLoss = 1.3678478700003459e-06\n",
      "Iteration = 54751 \tAcc = 1.0 \tLoss = 4.654193479463974e-06\n",
      "Iteration = 55001 \tAcc = 1.0 \tLoss = 0.35599203436281246\n",
      "Iteration = 55251 \tAcc = 1.0 \tLoss = 0.0020547337131352245\n",
      "Iteration = 55501 \tAcc = 1.0 \tLoss = 0.001923262610756725\n",
      "Iteration = 55751 \tAcc = 1.0 \tLoss = 0.006239957766621967\n",
      "Iteration = 56001 \tAcc = 1.0 \tLoss = 0.09250225161492993\n",
      "Iteration = 56251 \tAcc = 1.0 \tLoss = 0.0021359471435089352\n",
      "Iteration = 56501 \tAcc = 1.0 \tLoss = 0.00920098726170305\n",
      "Iteration = 56751 \tAcc = 1.0 \tLoss = 1.6431300764452452e-14\n",
      "Iteration = 57001 \tAcc = 1.0 \tLoss = 0.0014139780455551313\n",
      "Iteration = 57251 \tAcc = 1.0 \tLoss = 4.894993089464006e-06\n",
      "Iteration = 57501 \tAcc = 1.0 \tLoss = 0.20936390981312208\n",
      "Iteration = 57751 \tAcc = 1.0 \tLoss = 2.6157964683227523e-12\n",
      "Iteration = 58001 \tAcc = 1.0 \tLoss = 2.375877272697863e-14\n",
      "Iteration = 58251 \tAcc = 1.0 \tLoss = 7.337019880564725e-12\n",
      "Iteration = 58501 \tAcc = 1.0 \tLoss = 0.17895529358691403\n",
      "Iteration = 58751 \tAcc = 1.0 \tLoss = 5.799017550875773e-06\n",
      "Iteration = 59001 \tAcc = 1.0 \tLoss = 0.0015596846799375143\n",
      "Iteration = 59251 \tAcc = 1.0 \tLoss = 3.5958651511494137e-06\n",
      "Iteration = 59501 \tAcc = 1.0 \tLoss = 0.0010136897818500779\n",
      "Iteration = 59751 \tAcc = 1.0 \tLoss = 0.003631114667831408\n",
      "Iteration = 60001 \tAcc = 1.0 \tLoss = 1.1842733467566717e-09\n",
      "Iteration = 60251 \tAcc = 1.0 \tLoss = 0.0011860267886063297\n",
      "Iteration = 60501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 60751 \tAcc = 1.0 \tLoss = 3.8837116388732056e-08\n",
      "Iteration = 61001 \tAcc = 1.0 \tLoss = 0.007148017461185393\n",
      "Iteration = 61251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 61501 \tAcc = 1.0 \tLoss = 6.875774611218384e-06\n",
      "Iteration = 61751 \tAcc = 1.0 \tLoss = 7.011794963004175e-06\n",
      "Iteration = 62001 \tAcc = 1.0 \tLoss = 0.001733490003375887\n",
      "Iteration = 62251 \tAcc = 1.0 \tLoss = 0.007717114409052395\n",
      "Iteration = 62501 \tAcc = 1.0 \tLoss = 2.981694124980784e-08\n",
      "Iteration = 62751 \tAcc = 1.0 \tLoss = 3.8283030590106343e-08\n",
      "Iteration = 63001 \tAcc = 1.0 \tLoss = 0.0018583191804469098\n",
      "Iteration = 63251 \tAcc = 1.0 \tLoss = 3.4950040917044566e-08\n",
      "Iteration = 63501 \tAcc = 1.0 \tLoss = 0.0016133694278114015\n",
      "Iteration = 63751 \tAcc = 1.0 \tLoss = 5.127091188407102e-06\n",
      "Iteration = 64001 \tAcc = 1.0 \tLoss = 0.12436464587960498\n",
      "Iteration = 64251 \tAcc = 1.0 \tLoss = 3.5478286974984438e-12\n",
      "Iteration = 64501 \tAcc = 1.0 \tLoss = 0.0014636725242433\n",
      "Iteration = 64751 \tAcc = 1.0 \tLoss = 0.005214131118563326\n",
      "Iteration = 65001 \tAcc = 1.0 \tLoss = 1.2518839306372543e-09\n",
      "Iteration = 65251 \tAcc = 1.0 \tLoss = 0.1387809356776524\n",
      "Iteration = 65501 \tAcc = 1.0 \tLoss = 0.004952215339884132\n",
      "Iteration = 65751 \tAcc = 1.0 \tLoss = 1.2381620181250133e-09\n",
      "Iteration = 66001 \tAcc = 1.0 \tLoss = 8.157947257749021e-06\n",
      "Iteration = 66251 \tAcc = 1.0 \tLoss = 2.5671686998440445e-12\n",
      "Iteration = 66501 \tAcc = 1.0 \tLoss = 0.0007871992615118115\n",
      "Iteration = 66751 \tAcc = 1.0 \tLoss = 0.001348673524996489\n",
      "Iteration = 67001 \tAcc = 1.0 \tLoss = 0.0007880216494693643\n",
      "Iteration = 67251 \tAcc = 1.0 \tLoss = 0.0013448456642589366\n",
      "Iteration = 67501 \tAcc = 1.0 \tLoss = 0.004464539477039491\n",
      "Iteration = 67751 \tAcc = 1.0 \tLoss = 7.70561392474305e-12\n",
      "Iteration = 68001 \tAcc = 1.0 \tLoss = 9.833772496807205e-06\n",
      "Iteration = 68251 \tAcc = 1.0 \tLoss = 0.001202092340980168\n",
      "Iteration = 68501 \tAcc = 1.0 \tLoss = 0.0006426401590977124\n",
      "Iteration = 68751 \tAcc = 1.0 \tLoss = 5.513791281742474e-06\n",
      "Iteration = 69001 \tAcc = 1.0 \tLoss = 5.9455364138920374e-06\n",
      "Iteration = 69251 \tAcc = 1.0 \tLoss = 0.004115279935925176\n",
      "Iteration = 69501 \tAcc = 1.0 \tLoss = 6.175273968728562e-06\n",
      "Iteration = 69751 \tAcc = 1.0 \tLoss = 1.8207657603852735e-14\n",
      "Iteration = 70001 \tAcc = 1.0 \tLoss = 0.0064160857312008275\n",
      "Iteration = 70251 \tAcc = 1.0 \tLoss = 0.003944996097748947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 70501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 70751 \tAcc = 1.0 \tLoss = 2.0600188221940996e-12\n",
      "Iteration = 71001 \tAcc = 1.0 \tLoss = 6.596426733285705e-06\n",
      "Iteration = 71251 \tAcc = 1.0 \tLoss = 0.003733373890669672\n",
      "Iteration = 71501 \tAcc = 1.0 \tLoss = 6.941447416887958e-12\n",
      "Iteration = 71751 \tAcc = 1.0 \tLoss = 9.453379195028758e-10\n",
      "Iteration = 72001 \tAcc = 1.0 \tLoss = 7.002609285257464e-06\n",
      "Iteration = 72251 \tAcc = 1.0 \tLoss = 5.5079713164462104e-09\n",
      "Iteration = 72501 \tAcc = 1.0 \tLoss = 0.0007428180872959329\n",
      "Iteration = 72751 \tAcc = 1.0 \tLoss = 0.09587592815344274\n",
      "Iteration = 73001 \tAcc = 1.0 \tLoss = 1.672461361176425e-08\n",
      "Iteration = 73251 \tAcc = 1.0 \tLoss = 0.0006195831985393855\n",
      "Iteration = 73501 \tAcc = 1.0 \tLoss = 0.05874640457468738\n",
      "Iteration = 73751 \tAcc = 1.0 \tLoss = 1.0122488232229309e-08\n",
      "Iteration = 74001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 74251 \tAcc = 1.0 \tLoss = 4.663047725738992e-12\n",
      "Iteration = 74501 \tAcc = 1.0 \tLoss = 1.1170146314789952e-08\n",
      "Iteration = 74751 \tAcc = 1.0 \tLoss = 4.028951069779345e-09\n",
      "Iteration = 75001 \tAcc = 1.0 \tLoss = 0.0005923349442536828\n",
      "Iteration = 75251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 75501 \tAcc = 1.0 \tLoss = 3.5481617644058325e-12\n",
      "Iteration = 75751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 76001 \tAcc = 1.0 \tLoss = 0.0007251757215684801\n",
      "Iteration = 76251 \tAcc = 1.0 \tLoss = 2.5004000121120866e-07\n",
      "Iteration = 76501 \tAcc = 1.0 \tLoss = 0.0008747260571131515\n",
      "Iteration = 76751 \tAcc = 1.0 \tLoss = 2.3556788718734505e-07\n",
      "Iteration = 77001 \tAcc = 1.0 \tLoss = 0.0007487116316142986\n",
      "Iteration = 77251 \tAcc = 1.0 \tLoss = 0.0007518830036485561\n",
      "Iteration = 77501 \tAcc = 1.0 \tLoss = 1.1366912586951466e-08\n",
      "Iteration = 77751 \tAcc = 1.0 \tLoss = 0.06457363458166773\n",
      "Iteration = 78001 \tAcc = 1.0 \tLoss = 8.771687595856105e-06\n",
      "Iteration = 78251 \tAcc = 1.0 \tLoss = 0.0008088770668430066\n",
      "Iteration = 78501 \tAcc = 1.0 \tLoss = 0.0030116140030186462\n",
      "Iteration = 78751 \tAcc = 1.0 \tLoss = 1.0419443086112523e-12\n",
      "Iteration = 79001 \tAcc = 1.0 \tLoss = 1.2511772803661172e-05\n",
      "Iteration = 79251 \tAcc = 1.0 \tLoss = 0.00083350962733348\n",
      "Iteration = 79501 \tAcc = 1.0 \tLoss = 0.02727153984394517\n",
      "Iteration = 79751 \tAcc = 1.0 \tLoss = 7.4153670299536955e-06\n",
      "Iteration = 80001 \tAcc = 1.0 \tLoss = 2.0175522178697824e-07\n",
      "Iteration = 80251 \tAcc = 1.0 \tLoss = 7.359708922438192e-06\n",
      "Iteration = 80501 \tAcc = 1.0 \tLoss = 2.755711215154389e-10\n",
      "Iteration = 80751 \tAcc = 1.0 \tLoss = 0.003904721030957022\n",
      "Iteration = 81001 \tAcc = 1.0 \tLoss = 7.2787779704509426e-06\n",
      "Iteration = 81251 \tAcc = 1.0 \tLoss = 7.608298039874327e-06\n",
      "Iteration = 81501 \tAcc = 1.0 \tLoss = 0.0004131635194136025\n",
      "Iteration = 81751 \tAcc = 1.0 \tLoss = 5.544379149461141e-06\n",
      "Iteration = 82001 \tAcc = 1.0 \tLoss = 0.0040048768953245455\n",
      "Iteration = 82251 \tAcc = 1.0 \tLoss = 0.04577234326101734\n",
      "Iteration = 82501 \tAcc = 1.0 \tLoss = 6.028842534172224e-06\n",
      "Iteration = 82751 \tAcc = 1.0 \tLoss = 1.0649913556639936e-05\n",
      "Iteration = 83001 \tAcc = 1.0 \tLoss = 0.00048072291515282564\n",
      "Iteration = 83251 \tAcc = 1.0 \tLoss = 0.0004620225281354859\n",
      "Iteration = 83501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 83751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 84001 \tAcc = 1.0 \tLoss = 0.0004648138564287825\n",
      "Iteration = 84251 \tAcc = 1.0 \tLoss = 0.0004469325823388258\n",
      "Iteration = 84501 \tAcc = 1.0 \tLoss = 1.575373055470956e-09\n",
      "Iteration = 84751 \tAcc = 1.0 \tLoss = 1.031656685351301e-05\n",
      "Iteration = 85001 \tAcc = 1.0 \tLoss = 0.05189223705437829\n",
      "Iteration = 85251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 85501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 85751 \tAcc = 1.0 \tLoss = 2.1094237467877998e-15\n",
      "Iteration = 86001 \tAcc = 1.0 \tLoss = 5.362729830151563e-09\n",
      "Iteration = 86251 \tAcc = 1.0 \tLoss = 2.3314683517128315e-15\n",
      "Iteration = 86501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 86751 \tAcc = 1.0 \tLoss = 0.004211669191741737\n",
      "Iteration = 87001 \tAcc = 1.0 \tLoss = 2.990940828340619e-13\n",
      "Iteration = 87251 \tAcc = 1.0 \tLoss = 2.0991164364474919e-10\n",
      "Iteration = 87501 \tAcc = 1.0 \tLoss = 1.776356839400252e-15\n",
      "Iteration = 87751 \tAcc = 1.0 \tLoss = 0.00038677946016835965\n",
      "Iteration = 88001 \tAcc = 1.0 \tLoss = 1.9091450644427842e-10\n",
      "Iteration = 88251 \tAcc = 1.0 \tLoss = 1.2869669782599447e-09\n",
      "Iteration = 88501 \tAcc = 1.0 \tLoss = 0.0003985655389068474\n",
      "Iteration = 88751 \tAcc = 1.0 \tLoss = 0.027904328030218438\n",
      "Iteration = 89001 \tAcc = 1.0 \tLoss = 0.0004125700363393077\n",
      "Iteration = 89251 \tAcc = 1.0 \tLoss = 0.00041917288086607116\n",
      "Iteration = 89501 \tAcc = 1.0 \tLoss = 9.323682430389495e-06\n",
      "Iteration = 89751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 90001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 90251 \tAcc = 1.0 \tLoss = 1.850741782050307e-13\n",
      "Iteration = 90501 \tAcc = 1.0 \tLoss = 0.000700033348883845\n",
      "Iteration = 90751 \tAcc = 1.0 \tLoss = 8.809335027558722e-08\n",
      "Iteration = 91001 \tAcc = 1.0 \tLoss = 0.0005768281153116615\n",
      "Iteration = 91251 \tAcc = 1.0 \tLoss = 6.010747455322404e-13\n",
      "Iteration = 91501 \tAcc = 1.0 \tLoss = 5.077975377840221e-06\n",
      "Iteration = 91751 \tAcc = 1.0 \tLoss = 0.00039512278130399446\n",
      "Iteration = 92001 \tAcc = 1.0 \tLoss = 0.0004566130256385683\n",
      "Iteration = 92251 \tAcc = 1.0 \tLoss = 0.0005442315070821984\n",
      "Iteration = 92501 \tAcc = 1.0 \tLoss = 7.771561172376099e-16\n",
      "Iteration = 92751 \tAcc = 1.0 \tLoss = 0.0003233957091008223\n",
      "Iteration = 93001 \tAcc = 1.0 \tLoss = 4.7963859816733245e-06\n",
      "Iteration = 93251 \tAcc = 1.0 \tLoss = 1.2452372467273527e-10\n",
      "Iteration = 93501 \tAcc = 1.0 \tLoss = 0.002143428751360091\n",
      "Iteration = 93751 \tAcc = 1.0 \tLoss = 0.00038170260282032045\n",
      "Iteration = 94001 \tAcc = 1.0 \tLoss = 1.4610535004068128e-13\n",
      "Iteration = 94251 \tAcc = 1.0 \tLoss = 4.958256027977178e-13\n",
      "Iteration = 94501 \tAcc = 1.0 \tLoss = 8.622491613317415e-10\n",
      "Iteration = 94751 \tAcc = 1.0 \tLoss = 5.097033906055392e-13\n",
      "Iteration = 95001 \tAcc = 1.0 \tLoss = 8.516896080908775e-10\n",
      "Iteration = 95251 \tAcc = 1.0 \tLoss = 7.767794188670474e-10\n",
      "Iteration = 95501 \tAcc = 1.0 \tLoss = 0.00029633710706471883\n",
      "Iteration = 95751 \tAcc = 1.0 \tLoss = 0.03481230108671698\n",
      "Iteration = 96001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 96251 \tAcc = 1.0 \tLoss = 5.362125797829273e-06\n",
      "Iteration = 96501 \tAcc = 1.0 \tLoss = 0.0031258404535509246\n",
      "Iteration = 96751 \tAcc = 1.0 \tLoss = 0.0002624166557700875\n",
      "Iteration = 97001 \tAcc = 1.0 \tLoss = 7.092425997591755e-06\n",
      "Iteration = 97251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 97501 \tAcc = 1.0 \tLoss = 3.945732629518585e-13\n",
      "Iteration = 97751 \tAcc = 1.0 \tLoss = 9.692247004978086e-14\n",
      "Iteration = 98001 \tAcc = 1.0 \tLoss = 0.0002484867571085359\n",
      "Iteration = 98251 \tAcc = 1.0 \tLoss = 0.0004332218610165163\n",
      "Iteration = 98501 \tAcc = 1.0 \tLoss = 0.002884681833310442\n",
      "Iteration = 98751 \tAcc = 1.0 \tLoss = 8.535494533754693e-11\n",
      "Iteration = 99001 \tAcc = 1.0 \tLoss = 8.809950495445773e-06\n",
      "Iteration = 99251 \tAcc = 1.0 \tLoss = 7.158551531891558e-10\n",
      "Iteration = 99501 \tAcc = 1.0 \tLoss = 0.0004768015909036533\n",
      "Iteration = 99751 \tAcc = 1.0 \tLoss = 8.30562600571789e-06\n"
     ]
    }
   ],
   "source": [
    "model.sgd(X, Y, 100000, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like it has fitted the data 100\\%. Let's see what the decision boundary looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a grid of points to classify\n",
    "xx1, xx2 = np.meshgrid(np.arange(-3, 3, 0.005), np.arange(-3, 3, 0.005))\n",
    "\n",
    "# Flatten the grid to pass into model\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()].T\n",
    "\n",
    "# Predict classification at every point on the grid\n",
    "Z = model.forward(grid)[1,:].reshape(xx1.shape)\n",
    "\n",
    "# Plot the prediction regions.\n",
    "plt.imshow(Z, interpolation='bicubic', origin='lower', extent=[-3, 3, -3, 3], \n",
    "           cmap=ListedColormap(['#1f77b4', '#ff7f0e']), alpha=0.55, aspect='auto')\n",
    "\n",
    "# Plot the original points.\n",
    "_ = plt.scatter(X[0,:], X[1,:], c=Y[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
