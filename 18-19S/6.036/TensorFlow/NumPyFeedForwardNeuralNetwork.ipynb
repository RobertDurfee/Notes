{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy Feed-Forward Neural Network\n",
    "\n",
    "This notebook walks through the process of constructing a feed-forward neural network for multi-class classification solely using NumPy.\n",
    "\n",
    "## Layers\n",
    "\n",
    "For our neural network, we want to abstract away from individual neurons and focus on layers. Each element of the network will be defined by a certain layer.\n",
    "\n",
    "### Base Layer\n",
    "\n",
    "This layer provides the virtual methods that each layer has to implement. If the layer doesn't implement the method, we default to one of these empty methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Abstract base layer for our neural network.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__init__` method sets up constant information about the layer that is necessary to build the graph later. Typically, only dimensions of input/output data is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self):\n",
    "        \"\"\"Initializes layer constants necessary to construct the graph\n",
    "            for training. Likely: just dimension information or nothing\n",
    "            at all.\"\"\"\n",
    "\n",
    "Layer.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward method computes the forward pass from the network. All layers should implement this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, X):\n",
    "        \"\"\"Executes the forward pass through the layer\n",
    "            \n",
    "        Args:\n",
    "            X (ndarray): A matrix representing the inputs to the layer.\n",
    "                Likely: A or Z depending on the layer.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: A tensor representing the outputs of the layer. \n",
    "                Likely: Z or A depending on the layer.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "Layer.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward method computes the backward pass step for a single layer. This should be implemented for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dL):\n",
    "        \"\"\"Computes the backward pass for the layer.\n",
    "            \n",
    "        Args:\n",
    "            dL (ndarray): A matrix representing the gradient of the loss\n",
    "                of the network with respect to the outputs of the current\n",
    "                layer. Likely: dLdA or dLdZ depending on the layer.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: A matrix representing the gradient of the loss of the\n",
    "                network will respect to the inputs of the current layer.\n",
    "                Likely: dLdZ or dLdA depending on the layer.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "Layer.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any layer that has any variables needs to update these variables during stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, eta):\n",
    "        \"\"\"Updates trainable variables based off the results from the\n",
    "            backward pass.\n",
    "            \n",
    "        Args:\n",
    "            eta (float): The learning rate to use for the stochastic\n",
    "                gradient descent update step.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "Layer.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "This is the simplest possible layer where all inputs are connected to all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"Simple layer fully-connecting inputs to outputs linearly.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up this layer, we need to know the input and output dimensions ahead of time. Using this information, we randomly initialize the weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, m, n):\n",
    "        \"\"\"Initializes the layer based on input and output dimensions. \n",
    "\n",
    "        Note: Kernel is initialized using normal distribution with mean 0 \n",
    "        and variance 1 / m. All biases are initialized to zero.\n",
    "\n",
    "        Args:\n",
    "            m (int): Number of input features to the layer.\n",
    "            n (int): Number of output features of the layer.\n",
    "\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "\n",
    "        self.W0 = np.zeros((self.n, 1))\n",
    "        self.W = np.random.normal(0, np.sqrt(1 / self.m), (self.m, self.n))\n",
    "\n",
    "Linear.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our variables defined, we can execute the forward pass. We take the activation $A$ from the previous layer and produce the current $Z$ pre-activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, A):\n",
    "        \"\"\"Computes the forward pass of the layer.\n",
    "            \n",
    "        Args:\n",
    "            A (ndarray): An m by b matrix representing the activations from\n",
    "                the previous layer with a batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix, Z, representing the pre-activations\n",
    "                as the output from this linear layer.\n",
    "        \n",
    "        \"\"\"\n",
    "        # We need this input later when computing the backward path.\n",
    "        self.A = A\n",
    "\n",
    "        return np.transpose(self.W) @ self.A + self.W0\n",
    "\n",
    "Linear.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will compute the gradients by hand using back-propogation. We take the gradient of the loss with respect to the pre-activations of the layer $\\partial \\mathrm{Loss} / \\partial Z$ and compute the gradient of the loss with respect to the activations of the previous layer $\\partial \\mathrm{Loss} / \\partial A$. \n",
    "\n",
    "In addition, we save gradients of the loss with respect to the weights ($\\partial \\mathrm{Loss} / \\partial W$ and $\\partial \\mathrm{Loss} / \\partial W_0$) for the stochastic gradient descent update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdZ):\n",
    "        \"\"\"Computes the backward pass for the layer. Also records gradients\n",
    "            of the loss with respect to weights for later stochastic \n",
    "            gradient descent updates.\n",
    "        \n",
    "        Args:\n",
    "            dLdZ (ndarray): An n by b matrix representing the gradient of\n",
    "                the loss with respect to the current layer's \n",
    "                pre-activations for a batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            ndarray: An m by b matrix, dLdA, representing the gradient of \n",
    "                the loss with respect to the previous layer's activations.\n",
    "        \n",
    "        \"\"\"\n",
    "        # We store these gradients for use later in the sgd_step\n",
    "        self.dLdW = self.A @ np.transpose(dLdZ)\n",
    "        self.dLdW0 = np.sum(dLdZ, axis=1, keepdims=True)\n",
    "\n",
    "        return self.W @ dLdZ\n",
    "\n",
    "Linear.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear layer has some trainable parameters to update. Using the specified learning rate $\\eta$, we can re-assign our variable values using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, eta):\n",
    "        \"\"\"Updates the layer's variables using stochastic gradient\n",
    "            descent.\n",
    "            \n",
    "        Args:\n",
    "            eta (float): The learning rate to use for the stochastic\n",
    "                gradient descent update step.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.W -= eta * self.dLdW\n",
    "        self.W0 -= eta * self.dLdW0\n",
    "        \n",
    "Linear.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit Activation Layer\n",
    "\n",
    "This layer applies the relu activation function to each of the inputs element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"Applies relu activation function to all inputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have no variables to consider, we just need to construct the forward and backward passes. With the forward pass we compute the activation $A$ using the previous layer's pre-activation $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Compute the forward pass output for the layer. \n",
    "            \n",
    "        Args:\n",
    "            Z (ndarray): An m by b matrix representing the pre-activations\n",
    "                from the previous layer for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix, A, representing the activations from\n",
    "                the current layer for a batch of size b. (Note: n and m \n",
    "                are equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        # We need this activation when computing the backward step later\n",
    "        self.A = np.maximum(0.0, Z)\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "ReLU.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backward pass, we use the gradient of the loss with respect to the layer's activations $\\partial \\mathrm{Loss} / \\partial A$ to compute the gradient of the loss with respect to the previous layer's pre-activations $\\partial \\mathrm{Loss} / \\partial Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the backward pass for the layer.\n",
    "        \n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of\n",
    "                the loss with respect to the current layer's activations\n",
    "                for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An m by b matrix, dLdZ, representing the gradient of\n",
    "                the loss with respect to the previous layer's activations\n",
    "                for a batch of size b. (Note: n and m are equal.)\n",
    "        \n",
    "        \"\"\"\n",
    "        return np.sign(self.A) * dLdA\n",
    "    \n",
    "ReLU.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent Activation Layer\n",
    "\n",
    "This layer applies the hyperbolic tangent activation function to each input element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    \"\"\"Applies hyperbolic tangent activation function to all inputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no variables in this layer, thus we only need to worry about forward and backward passes. The forward pass takes the previous layer's pre-activation $Z$ and produces the activation $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the forward pass activation for the layer.\n",
    "        \n",
    "        Args:\n",
    "            Z (ndarray): An m by b matrix representing the previous layer's\n",
    "                pre-activation for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix, A, representing the layer's \n",
    "                activation for a batch of size b. (Note: m and n are\n",
    "                equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        # We need this activation when computing the backward step later\n",
    "        self.A = np.tanh(Z)\n",
    "\n",
    "        return self.A\n",
    "\n",
    "Tanh.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward pass uses the gradient of the loss with respect to the layer's activation $\\partial \\mathrm{Loss} / \\partial A$ to compute the gradient of the loss with respect to the previous layer's pre-activation $\\partial \\mathrm{Loss} / \\partial Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the backward pass for the layer.\n",
    "        \n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of\n",
    "                the loss with respect to this layer's activation for a \n",
    "                batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            ndarray: An m by b matrix, dLdZ, representing the gradient of \n",
    "                the loss with respect to the previous layer's \n",
    "                pre-activation for a batch of size b. (Note: n and m are \n",
    "                equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        return (1.0 - self.A ** 2.0) * dLdA\n",
    "\n",
    "Tanh.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Layer\n",
    "\n",
    "This layer applies the softmax activation function to the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Layer):\n",
    "    \"\"\"Applies the softmax activation function to layer inputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the other activation layers, there are no variables to consider. The forward pass takes the previous layer's pre-activations $Z$ and computes the current layer's activation $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the forward pass activations for the layer.\n",
    "            \n",
    "        Args:\n",
    "            Z (ndarray): An m by b matrix representing the previous layer's\n",
    "                pre-activation for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix, A, representing the current layer's\n",
    "                activation for a batch of size b. (Note: m and n are\n",
    "                equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        # We need this activation when computing the backward step later\n",
    "        self.A = np.exp(Z) / np.sum(np.exp(Z), axis=0, keepdims=True)\n",
    "        \n",
    "        return self.A\n",
    "\n",
    "SoftMax.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backward pass, we use the gradient of the loss with respect to the current layer's activations $\\partial \\mathrm{Loss} / \\partial A$ to compute the gradient of the loss with respect to the previous layer's pre-activations $\\partial \\mathrm{Loss} / \\partial Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the backward pass for the layer.\n",
    "            \n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of\n",
    "                the loss with respect to the current layer's activation.\n",
    "                \n",
    "        Returns:\n",
    "            ndarray: An m by b tensor, dLdZ, representing the gradient of \n",
    "                the loss with respect to the previous layer's \n",
    "                pre-activation. (Note: n and m are equal.)\n",
    "            \n",
    "        \"\"\"\n",
    "        n = dLdA.shape[0]\n",
    "        \n",
    "        # This is just a way to compute dLdZ by using the provided dLdA\n",
    "        # and softmax's dAdZ tensor. Or you can assume dLdZ is passed in.\n",
    "        return np.einsum('ikj,kj->ij', np.einsum('jk,jk,ji->ijk', self.A, 1.0 - self.A, np.eye(n)) + np.einsum('jk,ik,ji->ijk', -self.A, self.A, 1.0 - np.eye(n)), dLdA)\n",
    "\n",
    "SoftMax.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log-Likelihood Multi-Class Loss Layer\n",
    "\n",
    "This layer computes the loss of the output of the network compared with the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLM(Layer):\n",
    "    \"\"\"Computes the negative log-likelihood multi-class loss for neural\n",
    "        network outputs and expected outputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the activation layers, there are no variables to consider. The forward pass takes the neural network's final activations $A$ and the expected outputs\n",
    "$Y$ and computes the loss scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, A, Y):\n",
    "        \"\"\"Computes the loss of predictions vs expected outputs.\n",
    "            \n",
    "        Args:\n",
    "            A (ndarray): An n by b matrix representing the neural network's\n",
    "                outputs for a batch of size b.\n",
    "            Y (ndarray): An n by b matrix representing the expected outputs\n",
    "                from the neural network for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            float: A scalar, L, which represents the loss of the neural\n",
    "                network for a batch of size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        # We will need both of these later to compute the backward pass.\n",
    "        self.A = A\n",
    "        self.Y = Y\n",
    "\n",
    "        return -np.sum(self.Y * np.log(self.A))\n",
    "\n",
    "NLLM.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward pass computes the gradient of the loss with respect to the neural network's final activations $\\partial \\mathrm{Loss} / \\partial A$. Note, this is not immediately computing $\\partial \\mathrm{Loss} / \\partial Z$ by assuming softmax activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self):\n",
    "        \"\"\"Computes the backward step for the loss.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: An n by b tensor, dLdA, representing the gradient of\n",
    "                the loss with respect to the neural network's outputs.\n",
    "                \n",
    "        \"\"\"\n",
    "        return -self.Y / self.A\n",
    "\n",
    "NLLM.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now we have all the components to construct our neural network, but we need a model to connect them together.\n",
    "\n",
    "### Sequential Model\n",
    "\n",
    "The sequential model simply connects all the layer linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"A standard neural network model with linearly stacked layers.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to construct the model is to provide a list of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, layers, loss):\n",
    "        \"\"\"Initialize the layers and the loss for the network.\n",
    "        \n",
    "        Args:\n",
    "            layers (list of Layer): A list of layers in sequential order\n",
    "                to construct the model from.\n",
    "            loss (Layer): A layer used to construct the objective for\n",
    "                stochastic gradient descent.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "\n",
    "Sequential.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions with the network, we use the `forward` method. This passes the data through every layer and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, X):\n",
    "        \"\"\"Predicts the output for a training input batch.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): A d by b matrix of points to predict with \n",
    "                dimension d and batch size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: A c by b matrix representing the predicted outputs \n",
    "                with c features of the neural network for a batch size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "            \n",
    "        return X\n",
    "\n",
    "Sequential.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we will use stochastic gradient descent. Before we define the stochastic gradient descent training loop, we have to back-propogate the error throughout the layers of the network. To do this, we use the `backward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dL):\n",
    "        \"\"\"Computes the gradients of the loss with respect to each weight\n",
    "        in the neural network to prepare for stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            dL (ndarray): An n by b tensor representing the gradient of the\n",
    "                loss with respect to the output of the neural network.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers[::-1]:\n",
    "            dL = layer.backward(dL)\n",
    "\n",
    "Sequential.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the error is propogated through all the layers, each layer can update their weight matrices. For a single step, this is achieved through the `sgd_step` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, eta):\n",
    "        \"\"\"Runs a single update step on the weight matrices throughout the\n",
    "        neural network using stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            eta (float): A learning rate for stochastic gradient descent.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.sgd_step(eta)\n",
    "\n",
    "Sequential.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop over the data applying many stochastic gradient descent update steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd(self, X_train, Y_train, epochs, eta):\n",
    "        \"\"\"Trains the neural network by running stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): A d by n NumPy array representing n input\n",
    "                training points each with d features.\n",
    "            Y_train (ndarray): A c by n NumPy array representing n output\n",
    "                training points each with c features.\n",
    "            epochs (int): Number of iterations to run stochastic gradient\n",
    "                descent.\n",
    "            eta (float): A learning rate for stochastic gradient descent.\n",
    "        \n",
    "        \"\"\"\n",
    "        _, n = X.shape\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            t = np.random.randint(n)\n",
    "            \n",
    "            Xt = X[:, t:t + 1]\n",
    "            Yt = Y[:, t:t + 1]\n",
    "            \n",
    "            loss = self.loss.forward(self.forward(Xt), Yt)\n",
    "            self.backward(self.loss.backward())            \n",
    "            self.sgd_step(eta)\n",
    "            \n",
    "            if epoch % 250 == 1:\n",
    "                \n",
    "                acc = np.mean(np.argmax(self.forward(X_train), axis=0) == np.argmax(Y_train, axis=0))\n",
    "                print('Iteration =', epoch, '\\tAcc =', acc, '\\tLoss =', loss, flush=True)\n",
    "\n",
    "Sequential.sgd = sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Our model is complete! Let's train it on some data and see how will it can classify. We will use the standard 'hard' data set used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[-0.23390341,  1.18151883, -2.46493986,  1.55322202,  1.27621763,\n",
    "                2.39710997, -1.34403040, -0.46903436, -0.64673502, -1.44029872,\n",
    "               -1.37537243,  1.05994811, -0.93311512,  1.02735575, -0.84138778,\n",
    "               -2.22585412, -0.42591102,  1.03561105,  0.91125595, -2.26550369],\n",
    "              [-0.92254932, -1.10309630, -2.41956036, -1.15509002, -1.04805327,\n",
    "                0.08717325,  0.81847250, -0.75171045,  0.60664705,  0.80410947,\n",
    "               -0.11600488,  1.03747218, -0.67210575,  0.99944446, -0.65559838,\n",
    "               -0.40744784, -0.58367642,  1.05972780, -0.95991874, -1.41720255]])\n",
    "\n",
    "Y = np.array([[0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
    "              [1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGrBJREFUeJzt3XuUFPWd9/H3d7pnhgEGkPtdrqJ4wcvIGsUo6ioYIzFqlN0kGnVxd9UkzxOfrHncJ88m2ZyTHM/GXTc+UbzF5Bh1Y3TBlRXReI0iDIooIgIjCsNtAAVkrt39ff7o1gzQwwxUTddM1+d1zpzpqvpN/b59YOoz9avqX5m7IyIi8VMSdQEiIhINBYCISEwpAEREYkoBICISUwoAEZGYUgCIiMSUAkBEJKYUACIiMaUAEBGJqWTUBRzMwIEDfcyYMVGXISLSbSxbtmy7uw/qSNsuHQBjxoyhuro66jJERLoNM/uwo201BCQiElMKABGRmFIAiIjElAJARCSmFAAiIjHVpe8CEpEuyh0+eg02LIHKoXDMl6GsV9RVySFSAIjIoUk1w0OXw8alkG6CRBksuBmuehKGnxR1dXIINAQkIodm6T2w4XVo2QuZFLTUQ9MemDsd3no06urkECgAROTQvPFbSDXk2eDw5E2waXnBS5LDE0oAmNn9ZrbNzN5pY7uZ2R1mttbMVpjZyWH0KyIRyLS0vS3VDK/fVbhaJJCwzgB+Dcw4yPaZwMTc1xzgVyH1KyKF1pLvr//POOyuLVgpEkwoAeDuLwE7D9JkFvAbz1oM9DOzYWH0LSIFtHkF1B/kVz1ZARMvKFw9EkihrgGMADa0Wt6YW3cAM5tjZtVmVl1XV1eQ4kSkgz5eD4nSNjYa9B4CJ3+zkBVJAF3uIrC7z3X3KnevGjSoQzOaikihDDkW0s15NhiMOROufxF69Cl4WXJ4ChUAtcCoVssjc+tEpDsZMB6OmpEd6vmMJaBnf/jag1DRL7ra5JAVKgDmA9/M3Q10GrDL3TcXqG8RCdOl98IXb4bK4dCjLxx7Ccx5MRsC0q2E8klgM3sYOBsYaGYbgf8LlAK4+13AAuBCYC1QD3wrjH5FJAKJ0mwAfPHmqCuRgEIJAHef3c52B24Ioy8REQlHl7sILCIihaHJ4IpQSzpDY0ua3uVJzAx357FlG5n7Ug079zZz+vgB3HzBJI4coNkbReJMAVBEGlvS/OjJlTz+Ri3pjDPyiAp+esnx/Gntdh7403oaWtIAPPX2Zl58v46nv/tFhveraGevIlKsNARURL77yJs8/kYtTakMqYyzfkc91z64lHtervn84A+QcahvTnP3SzURVisiUVMAFIktuxp5fnUdTanMPuubUxncD2yfyjhLP9hRoOpEpCtSABSJ2k/qKUse+M+ZcUhnDkwAMxijawAisaYAKBLjBvameb+//gGSJcawvj0OCIceyRKuP2t8ocoTkS5IAVAkjuhVxuypo6koTXy+zoDy0gQPfOtULjh2CGWJEsqTJQyuLOeO2SczZZQ+ti8SZ7oLqIj88KLJjO7fk3tfqWFXQwtTx/Tnf194DBOHVPLvs09mb1OKT5tSDOpdTkmJRV2uiETMPN8Vwi6iqqrKq6uroy5DRKTbMLNl7l7VkbYaAhIRiSkFgIhITCkARERiSheBpctZs3UP//zUKpau30lljyTXThvLddPG6cK1SMgUANKlbNhZz1f+36vUN6VwslNW3L5oDR/uqOenlxwfdXkiRUVDQNKlzH2phqaWNK3vTWtoSfPYso3s+LQpsrpEipECQLqU5Rs+IZVn6oqyZAnr6vZGUJFI8VIASJdy1JDeJOzAsf7mVIZR/TV1tUiYFADSpVx/1vgD5i0qT5Zw9qRBDOurABAJUygBYGYzzGy1ma01s1vybL/azOrMbHnu67ow+pXic9SQSh741qmMG9SLhBnlyRIuPWUk/3blSVGXJlJ0At8FZGYJ4E7gL4GNwFIzm+/u7+7X9FF3vzFof1L8Ths3gD9+72wamtOUJoxkQieqIp0hjN+sqcBad69x92bgEWBWCPuVmKsoS+jgL9KJwvjtGgFsaLW8Mbduf5ea2Qoze8zMRoXQr4iIBFCoP6+eBMa4+wnAIuDBthqa2Rwzqzaz6rq6ugKVJyISP2EEQC3Q+i/6kbl1n3P3He7+2ad47gVOaWtn7j7X3avcvWrQoEEhlCciIvmEEQBLgYlmNtbMyoArgfmtG5jZsFaLFwOrQuhXREQCCHwXkLunzOxGYCGQAO5395Vm9mOg2t3nA982s4uBFLATuDpovyIiEoyeCCYiUkT0RDAREWmXAkBEJKYUACIiMaUAEBGJKQWAiEhMKQBERGJKASAiElMKABGRmFIAiIjElAJARCSmFAAiIjGlABARiSkFgIhITCkARERiKvDzAEREJAQNH8OyB+GjxTBwIkz9G+g3ulO7VACIiERt9ya4+yxo2g2pRlj3HCy9D775nzBqaqd1qyEgEZGoPfdjqN+ZPfgDpJuhZS/Mu6FTu1UAiIhE7f2F4KkD13+8PhsMnUQBICIStbKebW9Llndat6EEgJnNMLPVZrbWzG7Js73czB7NbX/dzMaE0a+ISFE49TpIVuy7rqQUJpwHZb06rdvAAWBmCeBOYCYwGZhtZpP3a3Yt8LG7TwBuB34etF8RkaLxhZtg0kxI9oCySijtCUOOhVl3dmq3YdwFNBVY6+41AGb2CDALeLdVm1nAP+VePwb80szM3T2E/kVEurdEEi5/AHasg63vZG//HHYimHVqt2EEwAhgQ6vljcBftNXG3VNmtgsYAGwPoX8RkeIwYHz2q0C63EVgM5tjZtVmVl1XVxd1OSIiRSuMAKgFRrVaHplbl7eNmSWBvsCOfDtz97nuXuXuVYMGDQqhPBERySeMAFgKTDSzsWZWBlwJzN+vzXzgqtzry4A/avy/cLbubmTlpl00tqSjLkVEupDA1wByY/o3AguBBHC/u680sx8D1e4+H7gP+K2ZrQV2kg0J6WS7G1u48aE3ef2DHZQmSsi48/0LJnH1GWOjLk1EuoBQ5gJy9wXAgv3W/bDV60bg8jD6ko676Xdv8lrNdlrSTlMqA8DPn17NkQN7MX3S4IirE5GodbmLwBKObXsaWVyzg5b0viNtDS1p7n5xXURViUhXogAoUjv3NpNM5L+HeMvuxgJXIyJdkQKgSI0d2AvjwABIlhhnTtDdVSKiACha5ckEP5h5NBWlic/XJUuM3j2S3DB9QoSViUhXoQfCFLG/Pu1IjhzQi7teXMeWXQ2cMXEQf3fWeIb27RF1aSLSBSgAity0iQOZNnFg1GWISBekISARkZhSAIiIxJQCQEQkphQAIiIxpQAQEYkpBYCISEwpAEREYkoBICISU/ogWCsvvV/HPS/XsG13E9OPHsTfnDmOAb3Loy5LRKRTKABy7n/lA25buJqG3FOzPti+lz+8UcvT3zlTISAiRUlDQEB9c2qfgz9AczrDrvpm7nvlgwgrExHpPAoA4L0te0iUHDh1cnPaefH9uggqEhHpfAoAYGCvclKZTN5tQ/po5kwRKU6BAsDM+pvZIjNbk/t+RBvt0ma2PPc1P0ifnWH0gJ4cO7wvyf3OAipKE1x3ph6gLiLFKegZwC3Ac+4+EXgut5xPg7ufmPu6OGCfnWLuN07hpNH9KE+W0Ls8Sc+yBP/4pWM4fbymUhaR4hT0LqBZwNm51w8CLwD/EHCfkRjQu5zf/+3pbPy4np17mzlqSCU9Wj1NS0Sk2AQ9Axji7ptzr7cAQ9po18PMqs1ssZl9JWCfnWrkET05YWQ/HfxFpOi1ewZgZs8CQ/NsurX1gru7mXkbuznS3WvNbBzwRzN7293XtdHfHGAOwOjRo9srT0REDlO7AeDu57W1zcy2mtkwd99sZsOAbW3sozb3vcbMXgBOAvIGgLvPBeYCVFVVtRUoIgdYsfET/nXRGlZt2c2Ewb35zrkTqRrTP+qyRLqsoENA84Grcq+vAubt38DMjjCz8tzrgcAZwLsB+xXZx5IPdnLF3a/x/OptbN7VyMtrtvP1+17X5zhEDiJoAPwM+EszWwOcl1vGzKrM7N5cm2OAajN7C3ge+Jm7KwAkVD/5r3dpaMnQ+pSxsSXDj+avjKwmka4u0F1A7r4DODfP+mrgutzrV4Hjg/Qj0p73tuzOu75m+17SGc/7SW+RuNNkcNLpPtpRz50vrGXZ+p2MHtCLG6aP55Qjwx2b79+zjK17mg5Y36dHUgd/kTYoACQ89Tvhpdvg3fmQLIeqa1g37hvM+tViGppTpB3W1u3l1XXb+dcrTmTGccNC6/r6s8YfMKFfRWmCa6fpk9wibVEASDhaGuCe6bB7E6Sbs+ue/2duezFJffMoMq0G5xtbMvyfeSs5f/JQSkL66/xbZ4zhk/pm7nn5A8wg487XTxvNTedMDGX/IsVIASDhePv38Om2Px/8AVoaWNLYj3zT7O1uaGH7p00MDmmyPTPjf54/ib+fPoEtuxoZ3KecnmX67y1yMJoNVMLx4Z+gpf6A1QNsT97mDlT2KA29jB6lCcYM7KWDv0gHKAAkHEeMhcSBT067vvxpKvY7FpcnS/jyCcOoKAtnuo1texqZt7yW51ZtpSmVbv8HRATQEJCE5eSr4NU7IN3qThxLcGmftWw8dgJ3vVxDsqSElnSGc48ezE8vCefO4DufX8sdz60hmTAMI1Fi/OaaqUwZ1S+U/YsUM3PvurMtVFVVeXV1ddRlSEdtWAKPz4E9m8AdRpwCl94HfUfwaVOK9dv3MqRPDwZVhvOM5er1O/nGfUv2ufMH4IiepSy59TxKEzrBlfgxs2XuXtWRtjoDkPCMmgrffhP2bM4OB/Ua8Pmm3uVJjhvRN9TuHl7yEY0tBw75tKSd12t2Mm2inuUgcjAKAAmXGfQZXpCu9jSmaOv8tb45VZAaRLoznSNLt3XRlOH0zHMhuSWd4bTxA/L8hIi0pgCQbuvC44Zy0ugjPg+BEoMepSX88KLJ9OmEW0xFio2GgKTbSiZK+M01U1n07haefmcrfXuWckXVKCYP7xN1aSLdggJAurVEiTHjuGGhziskEhcaAhIRiSmdAUh87ViX/fDa5rdg2BQ4/dswYHzUVYkUjAJA4mnTm/DAlyDVCJ6GzStgxe/h6v+CESdHXZ1IQWgISOJpwf+Clr3Zgz9kv7fsza4XyGRg9dPZT3bPuxE+Whx1RdIJdAYg8VS7LP/6TW8Uto6uyB3+cA28/0w2FDF45w/ZIbLpP4i6OglRoDMAM7vczFaaWcbM2px7wsxmmNlqM1trZrcE6VMkFGW9D219nHzwYquDP4Bnp/r+0+3wyUeRlibhCjoE9A7wVeClthqYWQK4E5gJTAZmm9nkgP2KBHPqtZCs2HddsgKqrommnq5k9YJWB/9WrATWPlf4eqTTBAoAd1/l7qvbaTYVWOvuNe7eDDwCzArSr0hg02+FY76cfXZxeZ/s5HXHXATn/GPUlUWvrBJK8owOW0JnSEWmENcARgAbWi1vBP6iAP2KtC1RCpfeA3t+kr0ddMB4qBwadVVdw5Qr4bVfQma/CfXcYdKM4Ptv2gNvPgTr/gj9RsHUOTBoUvD9yiFrNwDM7Fkg32/Gre4+L+yCzGwOMAdg9OjRYe9eZF+VQ3Xg39/AifClf4Gnvgcln82p5HDl76C8Mti+Gz6Gu8+CvdugpSF7VrH8Ibjs1+GEixySdgPA3c8L2EctMKrV8sjcurb6mwvMhewDYQL2LSKH46Svw9EXQc0LkCiD8dOhtKLdH2vXK/+WfV5Eujm77OlsEMz7e7h5DZSE85hQ6ZhCDAEtBSaa2ViyB/4rgb8qQL8iEkRFPzj2K4f/85kMrHsO1j4LPQfAlNnw3pN/Pvi3lmqE7Wtg8NGH358cskABYGaXAP8ODAKeMrPl7n6BmQ0H7nX3C909ZWY3AguBBHC/u68MXLmIdF3pFDx0GWxcAs17s2cRr/wCKtt4WFAmHXx4SQ5ZoABw9yeAJ/Ks3wRc2Gp5AbAgSF8i0o2seBQ2vJ79/ABk/+pPA7trobTnn9dD9jrAkOOg74hISo0zfRJYRMK34tF9D/KfKUnC+HNgzcLsWYFnoM8IuOK3h7Z/9+zw0tuPZa8bTJkNY6aFU3uMKABEJHyJsra3nX4TzPw51L6RvQNrxCnZZ0l3lDvMuwFW/merqSoeh1Ovg/N/Erj0ONFkcCISvlOuyg717K+0Z/aA32d49oN3I6sO7eAPsLEaVj5x4FQVS+ZmLyRLhykARCR8R18EJ1wJyR7ZKTbKekN5X/irR4Lf6vn+09lbR/fnGVizKNi+Y0ZDQCISPjP48u1w2t9lJ5fr2R+Omgllec4KDlVZ7+wnufe/nbQkCWW9gu8/RhQAItJ5Bh2V/QrT8ZfBiz/Ls8Gz8ztJh2kISES6l36jYNad2eGlssrsV2kv+Npvs2ca0mE6AxCR7uf4y2Di+VDzfHboZ9zZGv45DAoAEemeevSByZpZPggNAYmIxJQCQEQkphQAIiIxpQAQEYkpBYCISEwpAEREYir2t4Fu2dXI7c++zwurt9GnopRrzxjLFaeOwg51gioRkW4m1gGwc28zX7rjZXY1tJDKOFt3N/GjJ9/lvS17+KeLj426PBGRThXrIaBfv/oBnzalSGX+/Oz5hpY0Dy/5iLo9TRFWJiLS+WIdAItrdtKUyhywvixZwqrNuyOoSESkcGIdAGMG9CSRZ6w/lXaG9+sRQUUiIoUTKADM7HIzW2lmGTOrOki79Wb2tpktN7PqIH2G6dpp4yhN7hsApSXGMcMqmTC4MqKqREQKI+gZwDvAV4GXOtB2uruf6O5tBkWhTRpaya++fgpD+pTTo7SEskQJZ04cxP1Xnxp1aSIinS7QXUDuvgro1rdMTp80mNduOZfNuxvpXZakb8/SqEsSESmIQl0DcOAZM1tmZnMO1tDM5phZtZlV19XVFaS4khJjRL8KHfxFJFbaPQMws2eBoXk23eru8zrYzzR3rzWzwcAiM3vP3fMOG7n7XGAuQFVVledrIyIiwbUbAO5+XtBO3L02932bmT0BTKVj1w1ERKSTdPoQkJn1MrPKz14D55O9eCwiIhEKehvoJWa2EfgC8JSZLcytH25mC3LNhgCvmNlbwBLgKXd/Oki/IiISXNC7gJ4AnsizfhNwYe51DTAlSD8iIhK+WH8SWEQkzhQAIiIxpQAQEYkpBYCISEwpAEREYkoBICISUwoAEZGYUgCIiMSUAkBEJKYUACIiMaUAEBGJKQWAiEhMKQBERGJKASAiElMKABGRmFIAiIjElAJARCSmFAAiIjGlABARiamgD4W/zczeM7MVZvaEmfVro90MM1ttZmvN7JYgfYqISDiCngEsAo5z9xOA94Ef7N/AzBLAncBMYDIw28wmB+xXREQCChQA7v6Mu6dyi4uBkXmaTQXWunuNuzcDjwCzgvQrIiLBhXkN4Brgv/OsHwFsaLW8MbcuLzObY2bVZlZdV1cXYnkiItJasr0GZvYsMDTPplvdfV6uza1ACngoaEHuPheYC1BVVeVB9yciIvm1GwDuft7BtpvZ1cBFwLnunu+AXQuMarU8MrdOREQiFPQuoBnA94GL3b2+jWZLgYlmNtbMyoArgflB+hURkeCCXgP4JVAJLDKz5WZ2F4CZDTezBQC5i8Q3AguBVcB/uPvKgP2KiEhA7Q4BHYy7T2hj/SbgwlbLC4AFQfoSEZFw6ZPAIiIxpQAQEYkpBYCISEwpAEREYkoBICISUwoAEZGYUgCIiMSUAkBEJKYUACIiMaUAEBGJKQWAiEhMKQBERGJKASAiElMKABGRmAo0HXRX5O5Uf/gxmz5pYMrIfowZ2CvqkkREuqSiCoCtuxuZPXcxW3c3gkEq7cw8bhj/8rUpJEos6vJERLqUohoCuunhN/lwx172NqfZ25SmKZVh4cotPLT4w6hLExHpcoomAHZ82sTyjz4hvd9j6Rta0vxGASAicoCiCYCGljQlbbyb+uZUYYsREekGAgWAmd1mZu+Z2Qoze8LM+rXRbr2ZvZ17cHx1kD7bMqJfBf17lR2wvjRhzDh2aGd0KSLSrQU9A1gEHOfuJwDvAz84SNvp7n6iu1cF7DMvM+MXXzuRitIEpYnsBd+K0gSDK3tw0zkTO6NLEZFuLdBdQO7+TKvFxcBlwcoJ5rRxA3jmf3yR373+ER/u3MsXxg3gqyePpFd5Ud3sJCISijCPjNcAj7axzYFnzMyBu919boj97mNU/578w8yjO2v3IiJFo90AMLNngXyD6Le6+7xcm1uBFPBQG7uZ5u61ZjYYWGRm77n7S230NweYAzB69OgOvAURETkc7QaAu593sO1mdjVwEXCuu3u+Nu5em/u+zcyeAKYCeQMgd3YwF6Cqqirv/kREJLigdwHNAL4PXOzu9W206WVmlZ+9Bs4H3gnSr4iIBBf0LqBfApVkh3WWm9ldAGY23MwW5NoMAV4xs7eAJcBT7v50wH5FRCSgoHcBTWhj/SbgwtzrGmBKkH5ERCR8RfNJYBEROTTWxnXbLsHM6oDuNJHPQGB71EVEJK7vXe87frr6ez/S3Qd1pGGXDoDuxsyqO+uTzl1dXN+73nf8FNN71xCQiEhMKQBERGJKARCuTpviohuI63vX+46fonnvugYgIhJTOgMQEYkpBUDIOvqQnGJjZpeb2Uozy5hZUdwh0R4zm2Fmq81srZndEnU9hWBm95vZNjOL1XQuZjbKzJ43s3dz/8+/E3VNYVAAhO9QHpJTTN4Bvkobk/wVGzNLAHcCM4HJwGwzmxxtVQXxa2BG1EVEIAV8z90nA6cBNxTDv7cCIGTu/oy7f/YQ4sXAyCjrKRR3X+Xuq6Ouo4CmAmvdvcbdm4FHgFkR19TpctO474y6jkJz983u/kbu9R5gFTAi2qqCUwB0rmuA/466COkUI4ANrZY3UgQHBGmfmY0BTgJej7aS4PSsxMMQ0kNyup2OvG+RYmZmvYE/AN91991R1xOUAuAwhPGQnO6ovfcdM7XAqFbLI3PrpEiZWSnZg/9D7v541PWEQUNAIevIQ3KkKCwFJprZWDMrA64E5kdck3QSMzPgPmCVu/8i6nrCogAIX96H5BQ7M7vEzDYCXwCeMrOFUdfUmXIX+m8EFpK9IPgf7r4y2qo6n5k9DLwGTDKzjWZ2bdQ1FcgZwDeAc3K/18vN7MKoiwpKnwQWEYkpnQGIiMSUAkBEJKYUACIiMaUAEBGJKQWAiEhMKQBERGJKASAiElMKABGRmPr/vCSQ/fuJzDAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.scatter(X[0,:], X[1,:], c=Y[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct a neural network we think might be able to classify these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Linear(2, 10), ReLU(), \n",
    "                    Linear(10, 10), ReLU(), \n",
    "                    Linear(10, 2), SoftMax()], NLLM())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training the model on the data for a few thousand iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.9 \tLoss = 0.7291152473713108\n",
      "Iteration = 251 \tAcc = 0.95 \tLoss = 0.2973852747147087\n",
      "Iteration = 501 \tAcc = 0.95 \tLoss = 0.18839965876111237\n",
      "Iteration = 751 \tAcc = 0.95 \tLoss = 0.01352699316990441\n",
      "Iteration = 1001 \tAcc = 0.95 \tLoss = 0.021876539231356084\n",
      "Iteration = 1251 \tAcc = 0.95 \tLoss = 0.3601216501566613\n",
      "Iteration = 1501 \tAcc = 0.95 \tLoss = 0.5056405685397661\n",
      "Iteration = 1751 \tAcc = 0.95 \tLoss = 0.0043338910953799195\n",
      "Iteration = 2001 \tAcc = 0.95 \tLoss = 1.6469783097728663\n",
      "Iteration = 2251 \tAcc = 0.95 \tLoss = 0.2929479593145705\n",
      "Iteration = 2501 \tAcc = 0.95 \tLoss = 0.0121445184948526\n",
      "Iteration = 2751 \tAcc = 0.95 \tLoss = 0.024619672642208358\n",
      "Iteration = 3001 \tAcc = 0.95 \tLoss = 0.0020037896152337816\n",
      "Iteration = 3251 \tAcc = 0.95 \tLoss = 0.0005497366022968423\n",
      "Iteration = 3501 \tAcc = 0.95 \tLoss = 0.015482134711551014\n",
      "Iteration = 3751 \tAcc = 0.95 \tLoss = 0.0021990851417584755\n",
      "Iteration = 4001 \tAcc = 0.95 \tLoss = 0.01006816362788859\n",
      "Iteration = 4251 \tAcc = 0.95 \tLoss = 0.4450258838866263\n",
      "Iteration = 4501 \tAcc = 0.95 \tLoss = 0.04439183023783149\n",
      "Iteration = 4751 \tAcc = 0.95 \tLoss = 0.4608350823105593\n",
      "Iteration = 5001 \tAcc = 0.95 \tLoss = 0.002817241934111932\n",
      "Iteration = 5251 \tAcc = 0.95 \tLoss = 0.03219779773697584\n",
      "Iteration = 5501 \tAcc = 0.95 \tLoss = 0.00016096546072754994\n",
      "Iteration = 5751 \tAcc = 0.95 \tLoss = 0.05214098731268116\n",
      "Iteration = 6001 \tAcc = 0.95 \tLoss = 0.003684967962694749\n",
      "Iteration = 6251 \tAcc = 0.95 \tLoss = 0.00012196553637170549\n",
      "Iteration = 6501 \tAcc = 0.95 \tLoss = 0.00352420392157122\n",
      "Iteration = 6751 \tAcc = 0.95 \tLoss = 1.2195817557580652e-06\n",
      "Iteration = 7001 \tAcc = 0.95 \tLoss = 0.38740130868348643\n",
      "Iteration = 7251 \tAcc = 0.95 \tLoss = 0.3557225538874234\n",
      "Iteration = 7501 \tAcc = 0.95 \tLoss = 0.025436483363261948\n",
      "Iteration = 7751 \tAcc = 0.95 \tLoss = 0.004705774529632416\n",
      "Iteration = 8001 \tAcc = 0.95 \tLoss = 4.1916611601955085e-06\n",
      "Iteration = 8251 \tAcc = 0.95 \tLoss = 0.0031176512653219753\n",
      "Iteration = 8501 \tAcc = 0.95 \tLoss = 0.2702308033471532\n",
      "Iteration = 8751 \tAcc = 0.95 \tLoss = 0.0046354653382623175\n",
      "Iteration = 9001 \tAcc = 0.95 \tLoss = 0.005735304112263148\n",
      "Iteration = 9251 \tAcc = 0.95 \tLoss = 9.33946153794779e-05\n",
      "Iteration = 9501 \tAcc = 0.95 \tLoss = 0.005217156429371994\n",
      "Iteration = 9751 \tAcc = 0.95 \tLoss = 0.02664077164935997\n",
      "Iteration = 10001 \tAcc = 0.95 \tLoss = 0.003998494272783764\n",
      "Iteration = 10251 \tAcc = 0.95 \tLoss = 0.000868508015455265\n",
      "Iteration = 10501 \tAcc = 0.95 \tLoss = 4.848067533552739e-07\n",
      "Iteration = 10751 \tAcc = 0.95 \tLoss = 0.0007925932401593746\n",
      "Iteration = 11001 \tAcc = 0.95 \tLoss = 2.5792937643301933e-05\n",
      "Iteration = 11251 \tAcc = 0.95 \tLoss = 8.628121920847073e-09\n",
      "Iteration = 11501 \tAcc = 0.95 \tLoss = 0.12336733815148841\n",
      "Iteration = 11751 \tAcc = 0.95 \tLoss = 0.07323372376348038\n",
      "Iteration = 12001 \tAcc = 0.95 \tLoss = 0.021171137920624878\n",
      "Iteration = 12251 \tAcc = 0.95 \tLoss = 0.23852573717498188\n",
      "Iteration = 12501 \tAcc = 0.95 \tLoss = 0.0004337842503643591\n",
      "Iteration = 12751 \tAcc = 0.95 \tLoss = 0.088858140125882\n",
      "Iteration = 13001 \tAcc = 0.95 \tLoss = 1.1565368669446034e-09\n",
      "Iteration = 13251 \tAcc = 0.95 \tLoss = 0.0017297920251923323\n",
      "Iteration = 13501 \tAcc = 0.95 \tLoss = 0.14865989380859926\n",
      "Iteration = 13751 \tAcc = 0.95 \tLoss = 0.016821806371731567\n",
      "Iteration = 14001 \tAcc = 0.95 \tLoss = 0.34809969783659794\n",
      "Iteration = 14251 \tAcc = 0.95 \tLoss = 0.0045080759435619415\n",
      "Iteration = 14501 \tAcc = 0.95 \tLoss = 0.0014098055368666903\n",
      "Iteration = 14751 \tAcc = 0.95 \tLoss = 2.530256577472498e-06\n",
      "Iteration = 15001 \tAcc = 0.95 \tLoss = 0.2728939963404257\n",
      "Iteration = 15251 \tAcc = 0.95 \tLoss = 6.609032628628476e-08\n",
      "Iteration = 15501 \tAcc = 0.95 \tLoss = 0.06857987256722797\n",
      "Iteration = 15751 \tAcc = 0.95 \tLoss = 8.033098664728469e-08\n",
      "Iteration = 16001 \tAcc = 0.95 \tLoss = 9.64001294033402e-07\n",
      "Iteration = 16251 \tAcc = 0.95 \tLoss = 5.038221866724344e-08\n",
      "Iteration = 16501 \tAcc = 0.95 \tLoss = 0.0011727695332323087\n",
      "Iteration = 16751 \tAcc = 0.95 \tLoss = 1.408634145486216\n",
      "Iteration = 17001 \tAcc = 0.95 \tLoss = 7.075315167872513e-05\n",
      "Iteration = 17251 \tAcc = 0.95 \tLoss = 0.011035753713981057\n",
      "Iteration = 17501 \tAcc = 0.95 \tLoss = 0.001271586032727767\n",
      "Iteration = 17751 \tAcc = 0.95 \tLoss = 0.012891254514193045\n",
      "Iteration = 18001 \tAcc = 0.95 \tLoss = 0.0022916343222122225\n",
      "Iteration = 18251 \tAcc = 0.95 \tLoss = 1.6949920935791332\n",
      "Iteration = 18501 \tAcc = 0.95 \tLoss = 0.054143506692026215\n",
      "Iteration = 18751 \tAcc = 0.95 \tLoss = 1.4872799360890636\n",
      "Iteration = 19001 \tAcc = 0.95 \tLoss = 0.0022120095025991156\n",
      "Iteration = 19251 \tAcc = 0.95 \tLoss = 0.003979496283963525\n",
      "Iteration = 19501 \tAcc = 0.95 \tLoss = 0.05817107538673841\n",
      "Iteration = 19751 \tAcc = 0.95 \tLoss = 1.4103657396281775\n",
      "Iteration = 20001 \tAcc = 0.95 \tLoss = 1.4344697662224207e-09\n",
      "Iteration = 20251 \tAcc = 0.95 \tLoss = 5.226820089221781e-10\n",
      "Iteration = 20501 \tAcc = 0.95 \tLoss = 1.3611160676188179e-05\n",
      "Iteration = 20751 \tAcc = 0.95 \tLoss = 1.588201644652891e-05\n",
      "Iteration = 21001 \tAcc = 0.95 \tLoss = 0.000601399621289717\n",
      "Iteration = 21251 \tAcc = 0.95 \tLoss = 1.4929123321520907e-05\n",
      "Iteration = 21501 \tAcc = 0.95 \tLoss = 1.141095205622301e-05\n",
      "Iteration = 21751 \tAcc = 0.95 \tLoss = 0.05848537071161515\n",
      "Iteration = 22001 \tAcc = 0.95 \tLoss = 0.3492023244683011\n",
      "Iteration = 22251 \tAcc = 0.95 \tLoss = 9.088841841246537e-06\n",
      "Iteration = 22501 \tAcc = 0.95 \tLoss = 0.0007006013080652228\n",
      "Iteration = 22751 \tAcc = 0.95 \tLoss = 1.298215979246178e-10\n",
      "Iteration = 23001 \tAcc = 0.95 \tLoss = 0.0007944523191385294\n",
      "Iteration = 23251 \tAcc = 1.0 \tLoss = 0.018084534004163468\n",
      "Iteration = 23501 \tAcc = 1.0 \tLoss = 0.04928968584903637\n",
      "Iteration = 23751 \tAcc = 0.95 \tLoss = 0.49755811879366085\n",
      "Iteration = 24001 \tAcc = 0.95 \tLoss = 3.2538008146229168e-06\n",
      "Iteration = 24251 \tAcc = 0.95 \tLoss = 0.10300914026838023\n",
      "Iteration = 24501 \tAcc = 0.95 \tLoss = 1.7205740180750542e-09\n",
      "Iteration = 24751 \tAcc = 0.95 \tLoss = 4.440892098500627e-16\n",
      "Iteration = 25001 \tAcc = 0.95 \tLoss = 0.006251718855701491\n",
      "Iteration = 25251 \tAcc = 0.95 \tLoss = 0.5378776564152458\n",
      "Iteration = 25501 \tAcc = 0.95 \tLoss = 0.0002585627228071096\n",
      "Iteration = 25751 \tAcc = 0.95 \tLoss = 7.114143289757227e-05\n",
      "Iteration = 26001 \tAcc = 0.95 \tLoss = 2.0324303349567516e-06\n",
      "Iteration = 26251 \tAcc = 0.95 \tLoss = 0.033596286244764476\n",
      "Iteration = 26501 \tAcc = 0.95 \tLoss = 1.3877787807814553e-14\n",
      "Iteration = 26751 \tAcc = 0.95 \tLoss = 0.011318506432189393\n",
      "Iteration = 27001 \tAcc = 0.95 \tLoss = 3.2037480411359245e-05\n",
      "Iteration = 27251 \tAcc = 1.0 \tLoss = 1.2702259791150805e-06\n",
      "Iteration = 27501 \tAcc = 1.0 \tLoss = 0.0068881612585841865\n",
      "Iteration = 27751 \tAcc = 0.95 \tLoss = 0.1734043665741831\n",
      "Iteration = 28001 \tAcc = 0.95 \tLoss = 6.123600362096942e-07\n",
      "Iteration = 28251 \tAcc = 1.0 \tLoss = 0.03188944076436563\n",
      "Iteration = 28501 \tAcc = 1.0 \tLoss = 0.041338412012014436\n",
      "Iteration = 28751 \tAcc = 1.0 \tLoss = 4.5530135218611703e-11\n",
      "Iteration = 29001 \tAcc = 0.95 \tLoss = 3.7340769242070593e-05\n",
      "Iteration = 29251 \tAcc = 0.95 \tLoss = 3.8260394853061614e-11\n",
      "Iteration = 29501 \tAcc = 1.0 \tLoss = 1.699751450701259e-13\n",
      "Iteration = 29751 \tAcc = 0.95 \tLoss = 0.002383174973536866\n",
      "Iteration = 30001 \tAcc = 0.95 \tLoss = 4.657272133322545e-07\n",
      "Iteration = 30251 \tAcc = 0.95 \tLoss = 2.5528227123270214e-05\n",
      "Iteration = 30501 \tAcc = 1.0 \tLoss = 0.004899325662544605\n",
      "Iteration = 30751 \tAcc = 0.95 \tLoss = 0.29030809501470767\n",
      "Iteration = 31001 \tAcc = 0.95 \tLoss = 4.7951049191214574e-05\n",
      "Iteration = 31251 \tAcc = 0.95 \tLoss = 0.0036143631162131287\n",
      "Iteration = 31501 \tAcc = 0.95 \tLoss = 7.771561172376126e-15\n",
      "Iteration = 31751 \tAcc = 1.0 \tLoss = 0.023468882726393667\n",
      "Iteration = 32001 \tAcc = 1.0 \tLoss = 0.028117639552641155\n",
      "Iteration = 32251 \tAcc = 0.95 \tLoss = 4.7252202151182925e-12\n",
      "Iteration = 32501 \tAcc = 1.0 \tLoss = 1.3514666888442736e-07\n",
      "Iteration = 32751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 33001 \tAcc = 0.95 \tLoss = 1.0214051826551492e-14\n",
      "Iteration = 33251 \tAcc = 0.95 \tLoss = 8.770761894538775e-15\n",
      "Iteration = 33501 \tAcc = 0.95 \tLoss = 0.0018612877901450619\n",
      "Iteration = 33751 \tAcc = 1.0 \tLoss = 0.604000909537737\n",
      "Iteration = 34001 \tAcc = 1.0 \tLoss = 0.029331245137358065\n",
      "Iteration = 34251 \tAcc = 0.95 \tLoss = 1.908871741354942e-05\n",
      "Iteration = 34501 \tAcc = 1.0 \tLoss = 3.836654689875894e-05\n",
      "Iteration = 34751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 35001 \tAcc = 0.95 \tLoss = 0.038986744814807454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 35251 \tAcc = 0.95 \tLoss = 0.002423360938707255\n",
      "Iteration = 35501 \tAcc = 0.95 \tLoss = 2.2204460492503136e-16\n",
      "Iteration = 35751 \tAcc = 0.95 \tLoss = 4.947399024050628e-08\n",
      "Iteration = 36001 \tAcc = 1.0 \tLoss = 0.007265969019241052\n",
      "Iteration = 36251 \tAcc = 0.95 \tLoss = 0.002476767678562398\n",
      "Iteration = 36501 \tAcc = 1.0 \tLoss = 6.005889999158912e-08\n",
      "Iteration = 36751 \tAcc = 0.95 \tLoss = 5.142574154407003e-08\n",
      "Iteration = 37001 \tAcc = 0.95 \tLoss = 0.0016259637016151182\n",
      "Iteration = 37251 \tAcc = 0.95 \tLoss = 0.02087370997282972\n",
      "Iteration = 37501 \tAcc = 1.0 \tLoss = 0.634833662391953\n",
      "Iteration = 37751 \tAcc = 1.0 \tLoss = 8.726352973554112e-14\n",
      "Iteration = 38001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 38251 \tAcc = 0.95 \tLoss = 0.00659449436293096\n",
      "Iteration = 38501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 38751 \tAcc = 1.0 \tLoss = 4.5129889511276434e-08\n",
      "Iteration = 39001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 39251 \tAcc = 0.95 \tLoss = 0.001069956645868117\n",
      "Iteration = 39501 \tAcc = 0.95 \tLoss = 0.18360989794717128\n",
      "Iteration = 39751 \tAcc = 0.95 \tLoss = 0.35974660848238793\n",
      "Iteration = 40001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 40251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 40501 \tAcc = 1.0 \tLoss = 0.6419821285133498\n",
      "Iteration = 40751 \tAcc = 1.0 \tLoss = 0.015303103863764822\n",
      "Iteration = 41001 \tAcc = 1.0 \tLoss = 4.966933646883247e-06\n",
      "Iteration = 41251 \tAcc = 1.0 \tLoss = 0.024209116105749138\n",
      "Iteration = 41501 \tAcc = 1.0 \tLoss = 7.549516567451093e-15\n",
      "Iteration = 41751 \tAcc = 1.0 \tLoss = 0.014937743670637667\n",
      "Iteration = 42001 \tAcc = 1.0 \tLoss = 0.004359599121043658\n",
      "Iteration = 42251 \tAcc = 1.0 \tLoss = 2.0646341738456788e-08\n",
      "Iteration = 42501 \tAcc = 0.95 \tLoss = 9.808980319031806e-06\n",
      "Iteration = 42751 \tAcc = 0.95 \tLoss = 0.20216992537795858\n",
      "Iteration = 43001 \tAcc = 1.0 \tLoss = 0.00520612126966862\n",
      "Iteration = 43251 \tAcc = 0.95 \tLoss = 0.011493818327330315\n",
      "Iteration = 43501 \tAcc = 1.0 \tLoss = 7.46031095240667e-09\n",
      "Iteration = 43751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 44001 \tAcc = 0.95 \tLoss = 0.3626811103160668\n",
      "Iteration = 44251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 44501 \tAcc = 1.0 \tLoss = 1.0822068948870742\n",
      "Iteration = 44751 \tAcc = 1.0 \tLoss = 0.8620022301763862\n",
      "Iteration = 45001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 45251 \tAcc = 1.0 \tLoss = 8.171279074535545e-07\n",
      "Iteration = 45501 \tAcc = 1.0 \tLoss = 0.003030088796863804\n",
      "Iteration = 45751 \tAcc = 0.95 \tLoss = 0.02005851604004343\n",
      "Iteration = 46001 \tAcc = 1.0 \tLoss = 9.393007613712767e-06\n",
      "Iteration = 46251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 46501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 46751 \tAcc = 1.0 \tLoss = 0.007135081023989998\n",
      "Iteration = 47001 \tAcc = 1.0 \tLoss = 1.274689848142658e-05\n",
      "Iteration = 47251 \tAcc = 1.0 \tLoss = 0.0018960967153154175\n",
      "Iteration = 47501 \tAcc = 1.0 \tLoss = 0.0030394709792786676\n",
      "Iteration = 47751 \tAcc = 1.0 \tLoss = 4.72572748717333e-09\n",
      "Iteration = 48001 \tAcc = 1.0 \tLoss = 9.194639167158826e-07\n",
      "Iteration = 48251 \tAcc = 1.0 \tLoss = 0.001744330265524391\n",
      "Iteration = 48501 \tAcc = 0.95 \tLoss = 0.0015906555750523155\n",
      "Iteration = 48751 \tAcc = 0.95 \tLoss = 0.015896377955996747\n",
      "Iteration = 49001 \tAcc = 1.0 \tLoss = 6.389382867853768e-07\n",
      "Iteration = 49251 \tAcc = 1.0 \tLoss = 0.6781813015208907\n",
      "Iteration = 49501 \tAcc = 0.95 \tLoss = 1.1102230246251565e-16\n",
      "Iteration = 49751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 50001 \tAcc = 1.0 \tLoss = 0.8253804606296992\n",
      "Iteration = 50251 \tAcc = 0.95 \tLoss = 2.767417305008481e-06\n",
      "Iteration = 50501 \tAcc = 0.95 \tLoss = 0.009489587765401227\n",
      "Iteration = 50751 \tAcc = 0.95 \tLoss = 0.0075225684069313535\n",
      "Iteration = 51001 \tAcc = 0.95 \tLoss = 4.7525321568953985e-09\n",
      "Iteration = 51251 \tAcc = 0.95 \tLoss = 3.6707382220827594e-07\n",
      "Iteration = 51501 \tAcc = 1.0 \tLoss = 0.45811630168534967\n",
      "Iteration = 51751 \tAcc = 0.95 \tLoss = 3.611518535200447e-09\n",
      "Iteration = 52001 \tAcc = 1.0 \tLoss = 0.0049910515395525335\n",
      "Iteration = 52251 \tAcc = 1.0 \tLoss = 0.005200362660548117\n",
      "Iteration = 52501 \tAcc = 1.0 \tLoss = 0.004383954911809319\n",
      "Iteration = 52751 \tAcc = 0.95 \tLoss = 0.005285568425819914\n",
      "Iteration = 53001 \tAcc = 0.95 \tLoss = 3.6234791899326066e-09\n",
      "Iteration = 53251 \tAcc = 1.0 \tLoss = 8.850121822532184e-07\n",
      "Iteration = 53501 \tAcc = 1.0 \tLoss = 0.006882527500269117\n",
      "Iteration = 53751 \tAcc = 1.0 \tLoss = 1.2751516517498402e-09\n",
      "Iteration = 54001 \tAcc = 1.0 \tLoss = 8.944850993403134e-06\n",
      "Iteration = 54251 \tAcc = 1.0 \tLoss = 1.3378152989408552\n",
      "Iteration = 54501 \tAcc = 1.0 \tLoss = 2.281538019510782e-05\n",
      "Iteration = 54751 \tAcc = 1.0 \tLoss = 0.0028725229052337133\n",
      "Iteration = 55001 \tAcc = 1.0 \tLoss = 0.006091486509051952\n",
      "Iteration = 55251 \tAcc = 0.95 \tLoss = 3.3366663780855137e-07\n",
      "Iteration = 55501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 55751 \tAcc = 1.0 \tLoss = 2.7409711579524506e-05\n",
      "Iteration = 56001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 56251 \tAcc = 1.0 \tLoss = 1.1666814188215973e-09\n",
      "Iteration = 56501 \tAcc = 0.95 \tLoss = 1.0916322296513362e-09\n",
      "Iteration = 56751 \tAcc = 0.95 \tLoss = 6.854392969524192e-06\n",
      "Iteration = 57001 \tAcc = 0.95 \tLoss = 2.0039966373104864e-09\n",
      "Iteration = 57251 \tAcc = 0.95 \tLoss = 0.003551365410465972\n",
      "Iteration = 57501 \tAcc = 1.0 \tLoss = 1.6258966408672576e-09\n",
      "Iteration = 57751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 58001 \tAcc = 1.0 \tLoss = 0.009027972140961309\n",
      "Iteration = 58251 \tAcc = 0.95 \tLoss = 0.005334648515137329\n",
      "Iteration = 58501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 58751 \tAcc = 1.0 \tLoss = 0.0036580316590973604\n",
      "Iteration = 59001 \tAcc = 0.95 \tLoss = 0.005363150601619082\n",
      "Iteration = 59251 \tAcc = 1.0 \tLoss = 1.3038871102440583e-09\n",
      "Iteration = 59501 \tAcc = 1.0 \tLoss = 7.383297309598914e-10\n",
      "Iteration = 59751 \tAcc = 0.95 \tLoss = 2.464194054885195e-07\n",
      "Iteration = 60001 \tAcc = 1.0 \tLoss = 0.006409311205678021\n",
      "Iteration = 60251 \tAcc = 1.0 \tLoss = 1.1646318360934477e-09\n",
      "Iteration = 60501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 60751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 61001 \tAcc = 1.0 \tLoss = 7.883081481663273e-07\n",
      "Iteration = 61251 \tAcc = 1.0 \tLoss = 0.007834063211557989\n",
      "Iteration = 61501 \tAcc = 1.0 \tLoss = 0.003322725981400193\n",
      "Iteration = 61751 \tAcc = 0.95 \tLoss = 0.0051284863404697685\n",
      "Iteration = 62001 \tAcc = 1.0 \tLoss = 4.764399986781126e-10\n",
      "Iteration = 62251 \tAcc = 1.0 \tLoss = 0.0023591007567273555\n",
      "Iteration = 62501 \tAcc = 1.0 \tLoss = 0.0064955034911772175\n",
      "Iteration = 62751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 63001 \tAcc = 1.0 \tLoss = 0.007845873515067322\n",
      "Iteration = 63251 \tAcc = 1.0 \tLoss = 0.0032652668446772515\n",
      "Iteration = 63501 \tAcc = 1.0 \tLoss = 0.0017313055044640175\n",
      "Iteration = 63751 \tAcc = 1.0 \tLoss = 0.008677891394553095\n",
      "Iteration = 64001 \tAcc = 1.0 \tLoss = 0.2175775807779262\n",
      "Iteration = 64251 \tAcc = 0.95 \tLoss = 3.896462911486459e-06\n",
      "Iteration = 64501 \tAcc = 1.0 \tLoss = 0.004101962206025886\n",
      "Iteration = 64751 \tAcc = 1.0 \tLoss = 6.184506242371037e-10\n",
      "Iteration = 65001 \tAcc = 1.0 \tLoss = 0.00445456653993478\n",
      "Iteration = 65251 \tAcc = 0.95 \tLoss = 2.216510308873663e-10\n",
      "Iteration = 65501 \tAcc = 1.0 \tLoss = 5.835617546449873e-10\n",
      "Iteration = 65751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 66001 \tAcc = 1.0 \tLoss = 0.0008468018115995931\n",
      "Iteration = 66251 \tAcc = 1.0 \tLoss = 7.44525750154614e-06\n",
      "Iteration = 66501 \tAcc = 1.0 \tLoss = 0.0019139307971074541\n",
      "Iteration = 66751 \tAcc = 1.0 \tLoss = 2.560504031351734e-10\n",
      "Iteration = 67001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 67251 \tAcc = 1.0 \tLoss = 0.005114649919876852\n",
      "Iteration = 67501 \tAcc = 1.0 \tLoss = 0.21527914886048377\n",
      "Iteration = 67751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 68001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 68251 \tAcc = 1.0 \tLoss = 2.184520342635074e-10\n",
      "Iteration = 68501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 68751 \tAcc = 1.0 \tLoss = 0.0025972276447362584\n",
      "Iteration = 69001 \tAcc = 1.0 \tLoss = 1.8524526359026623e-10\n",
      "Iteration = 69251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 69501 \tAcc = 0.95 \tLoss = 3.6786851349662306e-10\n",
      "Iteration = 69751 \tAcc = 1.0 \tLoss = 0.5136040654452239\n",
      "Iteration = 70001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 70251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 70501 \tAcc = 1.0 \tLoss = 0.012515241708486035\n",
      "Iteration = 70751 \tAcc = 1.0 \tLoss = 0.12529002458257182\n",
      "Iteration = 71001 \tAcc = 0.95 \tLoss = 0.0018318944399335498\n",
      "Iteration = 71251 \tAcc = 1.0 \tLoss = 0.006386258724188847\n",
      "Iteration = 71501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 71751 \tAcc = 1.0 \tLoss = 8.685252517559276e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 72001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 72251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 72501 \tAcc = 1.0 \tLoss = 0.002499051098635531\n",
      "Iteration = 72751 \tAcc = 1.0 \tLoss = 0.005750729880868507\n",
      "Iteration = 73001 \tAcc = 1.0 \tLoss = 0.005004032992514144\n",
      "Iteration = 73251 \tAcc = 1.0 \tLoss = 1.0252287908045077e-10\n",
      "Iteration = 73501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 73751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 74001 \tAcc = 1.0 \tLoss = 0.005089206793688874\n",
      "Iteration = 74251 \tAcc = 1.0 \tLoss = 0.00526856011259056\n",
      "Iteration = 74501 \tAcc = 1.0 \tLoss = 3.143435017284196e-08\n",
      "Iteration = 74751 \tAcc = 0.95 \tLoss = 0.0024442865227678715\n",
      "Iteration = 75001 \tAcc = 1.0 \tLoss = 7.104069585136298e-08\n",
      "Iteration = 75251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 75501 \tAcc = 1.0 \tLoss = 0.005086314764488435\n",
      "Iteration = 75751 \tAcc = 1.0 \tLoss = 0.27267781107263694\n",
      "Iteration = 76001 \tAcc = 1.0 \tLoss = 7.578482286450702e-11\n",
      "Iteration = 76251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 76501 \tAcc = 1.0 \tLoss = 7.095784950249833e-07\n",
      "Iteration = 76751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 77001 \tAcc = 1.0 \tLoss = 0.004878823629294295\n",
      "Iteration = 77251 \tAcc = 1.0 \tLoss = 0.005744016627316616\n",
      "Iteration = 77501 \tAcc = 1.0 \tLoss = 0.002858324984974765\n",
      "Iteration = 77751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 78001 \tAcc = 1.0 \tLoss = 5.960687499317898e-11\n",
      "Iteration = 78251 \tAcc = 1.0 \tLoss = 1.2409784312043612e-10\n",
      "Iteration = 78501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 78751 \tAcc = 1.0 \tLoss = 2.0198043552942837e-08\n",
      "Iteration = 79001 \tAcc = 1.0 \tLoss = 0.0007458511608509221\n",
      "Iteration = 79251 \tAcc = 1.0 \tLoss = 0.1949763325340146\n",
      "Iteration = 79501 \tAcc = 1.0 \tLoss = 3.2140623496507404e-11\n",
      "Iteration = 79751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 80001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 80251 \tAcc = 1.0 \tLoss = 0.0034784552800874113\n",
      "Iteration = 80501 \tAcc = 1.0 \tLoss = 0.00428686770412676\n",
      "Iteration = 80751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 81001 \tAcc = 1.0 \tLoss = 0.006868513993427178\n",
      "Iteration = 81251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 81501 \tAcc = 1.0 \tLoss = 0.0011937867448252248\n",
      "Iteration = 81751 \tAcc = 1.0 \tLoss = 0.0066081913708615865\n",
      "Iteration = 82001 \tAcc = 1.0 \tLoss = 0.0026684756703423225\n",
      "Iteration = 82251 \tAcc = 1.0 \tLoss = 2.2361223983830215e-11\n",
      "Iteration = 82501 \tAcc = 1.0 \tLoss = 0.003917833917034765\n",
      "Iteration = 82751 \tAcc = 1.0 \tLoss = 1.9823062277180734e-08\n",
      "Iteration = 83001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 83251 \tAcc = 1.0 \tLoss = 0.00398272337429651\n",
      "Iteration = 83501 \tAcc = 1.0 \tLoss = 0.004250968717841489\n",
      "Iteration = 83751 \tAcc = 1.0 \tLoss = 0.000565111927353652\n",
      "Iteration = 84001 \tAcc = 1.0 \tLoss = 2.8392177498352035e-11\n",
      "Iteration = 84251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 84501 \tAcc = 1.0 \tLoss = 8.734446204481616e-09\n",
      "Iteration = 84751 \tAcc = 1.0 \tLoss = 0.003383710516247268\n",
      "Iteration = 85001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 85251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 85501 \tAcc = 1.0 \tLoss = 0.002091494214687082\n",
      "Iteration = 85751 \tAcc = 1.0 \tLoss = 0.00381931134188241\n",
      "Iteration = 86001 \tAcc = 1.0 \tLoss = 0.002088312510639374\n",
      "Iteration = 86251 \tAcc = 1.0 \tLoss = 0.0006875769597002842\n",
      "Iteration = 86501 \tAcc = 1.0 \tLoss = 0.0038779437750932626\n",
      "Iteration = 86751 \tAcc = 1.0 \tLoss = 0.004250311595762237\n",
      "Iteration = 87001 \tAcc = 1.0 \tLoss = 0.0020518540957990936\n",
      "Iteration = 87251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 87501 \tAcc = 1.0 \tLoss = 0.0035384799757140948\n",
      "Iteration = 87751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 88001 \tAcc = 1.0 \tLoss = 4.796418817791368e-11\n",
      "Iteration = 88251 \tAcc = 1.0 \tLoss = 1.8992807327047842e-11\n",
      "Iteration = 88501 \tAcc = 1.0 \tLoss = 0.0006300470243190999\n",
      "Iteration = 88751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 89001 \tAcc = 1.0 \tLoss = 2.613380182343415e-09\n",
      "Iteration = 89251 \tAcc = 1.0 \tLoss = 1.1339817973585644e-11\n",
      "Iteration = 89501 \tAcc = 1.0 \tLoss = 0.000622476873032333\n",
      "Iteration = 89751 \tAcc = 1.0 \tLoss = 0.004673967277651392\n",
      "Iteration = 90001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 90251 \tAcc = 1.0 \tLoss = 1.5223933225288342e-11\n",
      "Iteration = 90501 \tAcc = 1.0 \tLoss = 1.1363490217180958e-07\n",
      "Iteration = 90751 \tAcc = 1.0 \tLoss = 2.354729334534213e-07\n",
      "Iteration = 91001 \tAcc = 1.0 \tLoss = 3.2517615319994477e-09\n",
      "Iteration = 91251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 91501 \tAcc = 1.0 \tLoss = 1.7565768184028618e-07\n",
      "Iteration = 91751 \tAcc = 1.0 \tLoss = 1.0209702246904573e-08\n",
      "Iteration = 92001 \tAcc = 1.0 \tLoss = 0.0004439459324787221\n",
      "Iteration = 92251 \tAcc = 1.0 \tLoss = 0.0006108085712385536\n",
      "Iteration = 92501 \tAcc = 1.0 \tLoss = 8.331002554519415e-12\n",
      "Iteration = 92751 \tAcc = 1.0 \tLoss = 6.96239457401845e-09\n",
      "Iteration = 93001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 93251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 93501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 93751 \tAcc = 1.0 \tLoss = 0.0030031313094886487\n",
      "Iteration = 94001 \tAcc = 1.0 \tLoss = 0.11643310618163705\n",
      "Iteration = 94251 \tAcc = 1.0 \tLoss = 0.00181711322406171\n",
      "Iteration = 94501 \tAcc = 1.0 \tLoss = 1.3035926790975439e-09\n",
      "Iteration = 94751 \tAcc = 1.0 \tLoss = 0.12246953272944382\n",
      "Iteration = 95001 \tAcc = 1.0 \tLoss = 0.12512935753376345\n",
      "Iteration = 95251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 95501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 95751 \tAcc = 1.0 \tLoss = 6.000089314302197e-12\n",
      "Iteration = 96001 \tAcc = 1.0 \tLoss = 0.0029910721429790917\n",
      "Iteration = 96251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 96501 \tAcc = 1.0 \tLoss = 0.0024230984407473525\n",
      "Iteration = 96751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 97001 \tAcc = 1.0 \tLoss = 0.002272474187894024\n",
      "Iteration = 97251 \tAcc = 1.0 \tLoss = 5.211719944511453e-12\n",
      "Iteration = 97501 \tAcc = 1.0 \tLoss = 5.88365080610472e-09\n",
      "Iteration = 97751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 98001 \tAcc = 1.0 \tLoss = 2.0748514018224952e-11\n",
      "Iteration = 98251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 98501 \tAcc = 1.0 \tLoss = 0.0015371175727227765\n",
      "Iteration = 98751 \tAcc = 1.0 \tLoss = 0.0012552091420507398\n",
      "Iteration = 99001 \tAcc = 1.0 \tLoss = 1.0460902149987196e-09\n",
      "Iteration = 99251 \tAcc = 1.0 \tLoss = 4.4815262612119486e-12\n",
      "Iteration = 99501 \tAcc = 1.0 \tLoss = 0.03991702224278788\n",
      "Iteration = 99751 \tAcc = 1.0 \tLoss = 0.053212580743462544\n"
     ]
    }
   ],
   "source": [
    "model.sgd(X, Y, 100000, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like it has fitted the data 100\\%. Let's see what the decision boundary looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a grid of points to classify\n",
    "xx1, xx2 = np.meshgrid(np.arange(-3, 3, 0.005), np.arange(-3, 3, 0.005))\n",
    "\n",
    "# Flatten the grid to pass into model\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()].T\n",
    "\n",
    "# Predict classification at every point on the grid\n",
    "Z = model.forward(grid)[1,:].reshape(xx1.shape)\n",
    "\n",
    "# Plot the prediction regions.\n",
    "plt.imshow(Z, interpolation='bicubic', origin='lower', extent=[-3, 3, -3, 3], \n",
    "           cmap=ListedColormap(['#1f77b4', '#ff7f0e']), alpha=0.55, aspect='auto')\n",
    "\n",
    "# Plot the original points.\n",
    "_ = plt.scatter(X[0,:], X[1,:], c=Y[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
