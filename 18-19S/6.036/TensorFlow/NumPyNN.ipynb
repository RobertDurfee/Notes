{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy NN\n",
    "\n",
    "This notebook walks through the process of constructing a feed-forward neural network for multi-class classification solely using NumPy.\n",
    "\n",
    "## Layers\n",
    "\n",
    "For our neural network, we want to abstract away from individual neurons and focus on layers. Each element of the network will be defined by a certain layer.\n",
    "\n",
    "### Base Layer\n",
    "\n",
    "This layer provides the virtual methods that each layer has to implement. If the layer doesn't implement the method, we default to one of these empty methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Abstract base layer for our neural network.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__init__` method sets up constant information about the layer that is necessary to build the graph later. Typically, only dimensions of input/output data is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self):\n",
    "        \"\"\"Initializes layer constants necessary to construct the graph\n",
    "            for training. Likely: just dimension information or nothing\n",
    "            at all.\"\"\"\n",
    "\n",
    "Layer.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward method computes the forward pass from the network. All layers should implement this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, X):\n",
    "        \"\"\"Executes the forward pass through the layer\n",
    "            \n",
    "        Args:\n",
    "            X (ndarray): A matrix representing the inputs to the layer.\n",
    "                Likely: A or Z depending on the layer.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: A tensor representing the outputs of the layer. \n",
    "                Likely: Z or A depending on the layer.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "Layer.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward method computes the backward pass step for a single layer. This should be implemented for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dL):\n",
    "        \"\"\"Computes the backward pass for the layer.\n",
    "            \n",
    "        Args:\n",
    "            dL (ndarray): A matrix representing the gradient of the loss\n",
    "                of the network with respect to the outputs of the current\n",
    "                layer. Likely: dLdA or dLdZ depending on the layer.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: A matrix representing the gradient of the loss of the\n",
    "                network will respect to the inputs of the current layer.\n",
    "                Likely: dLdZ or dLdA depending on the layer.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "Layer.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any layer that has any variables needs to update these variables during stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, eta):\n",
    "        \"\"\"Updates trainable variables based off the results from the\n",
    "            backward pass.\n",
    "            \n",
    "        Args:\n",
    "            eta (float): The learning rate to use for the stochastic\n",
    "                gradient descent update step.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "Layer.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "This is the simplest possible layer where all inputs are connected to all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"Simple layer fully-connecting inputs to outputs linearly.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up this layer, we need to know the input and output dimensions ahead of time. Using this information, we randomly initialize the weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, m, n):\n",
    "        \"\"\"Initializes the layer based on input and output dimensions. \n",
    "\n",
    "        Note: Kernel is initialized using normal distribution with mean 0 \n",
    "        and variance 1 / m. All biases are initialized to zero.\n",
    "\n",
    "        Args:\n",
    "            m (int): Number of input features to the layer.\n",
    "            n (int): Number of output features of the layer.\n",
    "\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "\n",
    "        self.W0 = np.zeros((self.n, 1))\n",
    "        self.W = np.random.normal(0, np.sqrt(1 / self.m), (self.m, self.n))\n",
    "\n",
    "Linear.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our variables defined, we can execute the forward pass. We take the activation $A$ from the previous layer and produce the current $Z$ pre-activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, A):\n",
    "        \"\"\"Computes the forward pass of the layer.\n",
    "            \n",
    "        Args:\n",
    "            A (ndarray): An m by b matrix representing the activations from\n",
    "                the previous layer with a batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix, Z, representing the pre-activations\n",
    "                as the output from this linear layer.\n",
    "        \n",
    "        \"\"\"\n",
    "        # We need this input later when computing the backward path.\n",
    "        self.A = A\n",
    "\n",
    "        return np.transpose(self.W) @ self.A + self.W0\n",
    "\n",
    "Linear.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will compute the gradients by hand using back-propogation. We take the gradient of the loss with respect to the pre-activations of the layer $\\partial \\mathrm{Loss} / \\partial Z$ and compute the gradient of the loss with respect to the activations of the previous layer $\\partial \\mathrm{Loss} / \\partial A$. \n",
    "\n",
    "In addition, we save gradients of the loss with respect to the weights ($\\partial \\mathrm{Loss} / \\partial W$ and $\\partial \\mathrm{Loss} / \\partial W_0$) for the stochastic gradient descent update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdZ):\n",
    "        \"\"\"Computes the backward pass for the layer. Also records gradients\n",
    "            of the loss with respect to weights for later stochastic \n",
    "            gradient descent updates.\n",
    "        \n",
    "        Args:\n",
    "            dLdZ (ndarray): An n by b matrix representing the gradient of\n",
    "                the loss with respect to the current layer's \n",
    "                pre-activations for a batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            ndarray: An m by b matrix, dLdA, representing the gradient of \n",
    "                the loss with respect to the previous layer's activations.\n",
    "        \n",
    "        \"\"\"\n",
    "        # We store these gradients for use later in the sgd_step\n",
    "        self.dLdW = self.A @ np.transpose(dLdZ)\n",
    "        self.dLdW0 = np.sum(dLdZ, axis=1, keepdims=True)\n",
    "\n",
    "        return self.W @ dLdZ\n",
    "\n",
    "Linear.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear layer has some trainable parameters to update. Using the specified learning rate $\\eta$, we can re-assign our variable values using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, eta):\n",
    "        \"\"\"Updates the layer's variables using stochastic gradient\n",
    "            descent.\n",
    "            \n",
    "        Args:\n",
    "            eta (float): The learning rate to use for the stochastic\n",
    "                gradient descent update step.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.W -= eta * self.dLdW\n",
    "        self.W0 -= eta * self.dLdW0\n",
    "        \n",
    "Linear.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit Activation Layer\n",
    "\n",
    "This layer applies the relu activation function to each of the inputs element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"Applies relu activation function to all inputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have no variables to consider, we just need to construct the forward and backward passes. With the forward pass we compute the activation $A$ using the previous layer's pre-activation $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Compute the forward pass output for the layer. \n",
    "            \n",
    "        Args:\n",
    "            Z (ndarray): An m by b matrix representing the pre-activations\n",
    "                from the previous layer for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix, A, representing the activations from\n",
    "                the current layer for a batch of size b. (Note: n and m \n",
    "                are equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        # We need this activation when computing the backward step later\n",
    "        self.A = np.maximum(0.0, Z)\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "ReLU.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backward pass, we use the gradient of the loss with respect to the layer's activations $\\partial \\mathrm{Loss} / \\partial A$ to compute the gradient of the loss with respect to the previous layer's pre-activations $\\partial \\mathrm{Loss} / \\partial Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the backward pass for the layer.\n",
    "        \n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of\n",
    "                the loss with respect to the current layer's activations\n",
    "                for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An m by b matrix, dLdZ, representing the gradient of\n",
    "                the loss with respect to the previous layer's activations\n",
    "                for a batch of size b. (Note: n and m are equal.)\n",
    "        \n",
    "        \"\"\"\n",
    "        return np.sign(self.A) * dLdA\n",
    "    \n",
    "ReLU.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent Activation Layer\n",
    "\n",
    "This layer applies the hyperbolic tangent activation function to each input element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    \"\"\"Applies hyperbolic tangent activation function to all inputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no variables in this layer, thus we only need to worry about forward and backward passes. The forward pass takes the previous layer's pre-activation $Z$ and produces the activation $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the forward pass activation for the layer.\n",
    "        \n",
    "        Args:\n",
    "            Z (ndarray): An m by b matrix representing the previous layer's\n",
    "                pre-activation for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix, A, representing the layer's \n",
    "                activation for a batch of size b. (Note: m and n are\n",
    "                equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        # We need this activation when computing the backward step later\n",
    "        self.A = np.tanh(Z)\n",
    "\n",
    "        return self.A\n",
    "\n",
    "Tanh.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward pass uses the gradient of the loss with respect to the layer's activation $\\partial \\mathrm{Loss} / \\partial A$ to compute the gradient of the loss with respect to the previous layer's pre-activation $\\partial \\mathrm{Loss} / \\partial Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the backward pass for the layer.\n",
    "        \n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of\n",
    "                the loss with respect to this layer's activation for a \n",
    "                batch of size b.\n",
    "                \n",
    "        Returns:\n",
    "            ndarray: An m by b matrix, dLdZ, representing the gradient of \n",
    "                the loss with respect to the previous layer's \n",
    "                pre-activation for a batch of size b. (Note: n and m are \n",
    "                equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        return (1.0 - self.A ** 2.0) * dLdA\n",
    "\n",
    "Tanh.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Layer\n",
    "\n",
    "This layer applies the softmax activation function to the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Layer):\n",
    "    \"\"\"Applies the softmax activation function to layer inputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the other activation layers, there are no variables to consider. The forward pass takes the previous layer's pre-activations $Z$ and computes the current layer's activation $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the forward pass activations for the layer.\n",
    "            \n",
    "        Args:\n",
    "            Z (ndarray): An m by b matrix representing the previous layer's\n",
    "                pre-activation for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix, A, representing the current layer's\n",
    "                activation for a batch of size b. (Note: m and n are\n",
    "                equal.)\n",
    "                \n",
    "        \"\"\"\n",
    "        # We need this activation when computing the backward step later\n",
    "        self.A = np.exp(Z) / np.sum(np.exp(Z), axis=0, keepdims=True)\n",
    "        \n",
    "        return self.A\n",
    "\n",
    "SoftMax.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backward pass, we use the gradient of the loss with respect to the current layer's activations $\\partial \\mathrm{Loss} / \\partial A$ to compute the gradient of the loss with respect to the previous layer's pre-activations $\\partial \\mathrm{Loss} / \\partial Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the backward pass for the layer.\n",
    "            \n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of\n",
    "                the loss with respect to the current layer's activation.\n",
    "                \n",
    "        Returns:\n",
    "            ndarray: An m by b tensor, dLdZ, representing the gradient of \n",
    "                the loss with respect to the previous layer's \n",
    "                pre-activation. (Note: n and m are equal.)\n",
    "            \n",
    "        \"\"\"\n",
    "        n = dLdA.shape[0]\n",
    "        \n",
    "        # This is just a way to compute dLdZ by using the provided dLdA\n",
    "        # and softmax's dAdZ tensor. Or you can assume dLdZ is passed in.\n",
    "        return np.einsum('ikj,kj->ij', np.einsum('jk,jk,ji->ijk', self.A, 1.0 - self.A, np.eye(n)) + np.einsum('jk,ik,ji->ijk', -self.A, self.A, 1.0 - np.eye(n)), dLdA)\n",
    "\n",
    "SoftMax.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log-Likelihood Multi-Class Loss Layer\n",
    "\n",
    "This layer computes the loss of the output of the network compared with the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLM(Layer):\n",
    "    \"\"\"Computes the negative log-likelihood multi-class loss for neural\n",
    "        network outputs and expected outputs.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the activation layers, there are no variables to consider. The forward pass takes the neural network's final activations $A$ and the expected outputs\n",
    "$Y$ and computes the loss scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, A, Y):\n",
    "        \"\"\"Computes the loss of predictions vs expected outputs.\n",
    "            \n",
    "        Args:\n",
    "            A (ndarray): An n by b matrix representing the neural network's\n",
    "                outputs for a batch of size b.\n",
    "            Y (ndarray): An n by b matrix representing the expected outputs\n",
    "                from the neural network for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            float: A scalar, L, which represents the loss of the neural\n",
    "                network for a batch of size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        # We will need both of these later to compute the backward pass.\n",
    "        self.A = A\n",
    "        self.Y = Y\n",
    "\n",
    "        return -np.sum(self.Y * np.log(self.A))\n",
    "\n",
    "NLLM.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward pass computes the gradient of the loss with respect to the neural network's final activations $\\partial \\mathrm{Loss} / \\partial A$. Note, this is not immediately computing $\\partial \\mathrm{Loss} / \\partial Z$ by assuming softmax activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self):\n",
    "        \"\"\"Computes the backward step for the loss.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: An n by b tensor, dLdA, representing the gradient of\n",
    "                the loss with respect to the neural network's outputs.\n",
    "                \n",
    "        \"\"\"\n",
    "        return -self.Y / self.A\n",
    "\n",
    "NLLM.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now we have all the components to construct our neural network, but we need a model to connect them together.\n",
    "\n",
    "### Sequential Model\n",
    "\n",
    "The sequential model simply connects all the layer linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"A standard neural network model with linearly stacked layers.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to construct the model is to provide a list of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, layers, loss):\n",
    "        \"\"\"Initialize the layers and the loss for the network.\n",
    "        \n",
    "        Args:\n",
    "            layers (list of Layer): A list of layers in sequential order\n",
    "                to construct the model from.\n",
    "            loss (Layer): A layer used to construct the objective for\n",
    "                stochastic gradient descent.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "\n",
    "Sequential.__init__ = __init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions with the network, we use the `forward` method. This passes the data through every layer and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, X):\n",
    "        \"\"\"Predicts the output for a training input batch.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): A d by b matrix of points to predict with \n",
    "                dimension d and batch size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: A c by b matrix representing the predicted outputs \n",
    "                with c features of the neural network for a batch size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "            \n",
    "        return X\n",
    "\n",
    "Sequential.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we will use stochastic gradient descent. Before we define the stochastic gradient descent training loop, we have to back-propogate the error throughout the layers of the network. To do this, we use the `backward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, dL):\n",
    "        \"\"\"Computes the gradients of the loss with respect to each weight\n",
    "        in the neural network to prepare for stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            dL (ndarray): An n by b tensor representing the gradient of the\n",
    "                loss with respect to the output of the neural network.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers[::-1]:\n",
    "            dL = layer.backward(dL)\n",
    "\n",
    "Sequential.backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the error is propogated through all the layers, each layer can update their weight matrices. For a single step, this is achieved through the `sgd_step` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd_step(self, eta):\n",
    "        \"\"\"Runs a single update step on the weight matrices throughout the\n",
    "        neural network using stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            eta (float): A learning rate for stochastic gradient descent.\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.sgd_step(eta)\n",
    "\n",
    "Sequential.sgd_step = sgd_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop over the data applying many stochastic gradient descent update steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sgd(self, X_train, Y_train, epochs, eta):\n",
    "        \"\"\"Trains the neural network by running stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): A d by n NumPy array representing n input\n",
    "                training points each with d features.\n",
    "            Y_train (ndarray): A c by n NumPy array representing n output\n",
    "                training points each with c features.\n",
    "            epochs (int): Number of iterations to run stochastic gradient\n",
    "                descent.\n",
    "            eta (float): A learning rate for stochastic gradient descent.\n",
    "        \n",
    "        \"\"\"\n",
    "        _, n = X.shape\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            t = np.random.randint(n)\n",
    "            \n",
    "            Xt = X[:, t:t + 1]\n",
    "            Yt = Y[:, t:t + 1]\n",
    "            \n",
    "            loss = self.loss.forward(self.forward(Xt), Yt)\n",
    "            self.backward(self.loss.backward())            \n",
    "            self.sgd_step(eta)\n",
    "            \n",
    "            if epoch % 250 == 1:\n",
    "                \n",
    "                acc = np.mean(np.argmax(self.forward(X_train), axis=0) == np.argmax(Y_train, axis=0))\n",
    "                print('Iteration =', epoch, '\\tAcc =', acc, '\\tLoss =', loss, flush=True)\n",
    "\n",
    "Sequential.sgd = sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Our model is complete! Let's train it on some data and see how will it can classify. We will use the standard 'hard' data set used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[-0.23390341,  1.18151883, -2.46493986,  1.55322202,  1.27621763,\n",
    "                2.39710997, -1.34403040, -0.46903436, -0.64673502, -1.44029872,\n",
    "               -1.37537243,  1.05994811, -0.93311512,  1.02735575, -0.84138778,\n",
    "               -2.22585412, -0.42591102,  1.03561105,  0.91125595, -2.26550369],\n",
    "              [-0.92254932, -1.10309630, -2.41956036, -1.15509002, -1.04805327,\n",
    "                0.08717325,  0.81847250, -0.75171045,  0.60664705,  0.80410947,\n",
    "               -0.11600488,  1.03747218, -0.67210575,  0.99944446, -0.65559838,\n",
    "               -0.40744784, -0.58367642,  1.05972780, -0.95991874, -1.41720255]])\n",
    "\n",
    "Y = np.array([[0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
    "              [1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.scatter(X[0,:], X[1,:], c=Y[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct a neural network we think might be able to classify these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Linear(2, 10), ReLU(), \n",
    "                    Linear(10, 10), ReLU(), \n",
    "                    Linear(10, 2), SoftMax()], NLLM())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training the model on the data for a few thousand iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.3 \tLoss = 0.7688498024176027\n",
      "Iteration = 251 \tAcc = 0.7 \tLoss = 0.7227086479860636\n",
      "Iteration = 501 \tAcc = 0.95 \tLoss = 0.22725126343606974\n",
      "Iteration = 751 \tAcc = 0.95 \tLoss = 0.49308214409011153\n",
      "Iteration = 1001 \tAcc = 0.95 \tLoss = 0.5985567349213566\n",
      "Iteration = 1251 \tAcc = 0.9 \tLoss = 0.0021923854937036706\n",
      "Iteration = 1501 \tAcc = 0.95 \tLoss = 0.13244327706362838\n",
      "Iteration = 1751 \tAcc = 0.95 \tLoss = 0.4779544679693506\n",
      "Iteration = 2001 \tAcc = 0.95 \tLoss = 0.032823807140995614\n",
      "Iteration = 2251 \tAcc = 0.95 \tLoss = 0.3889254506293234\n",
      "Iteration = 2501 \tAcc = 0.95 \tLoss = 0.009707154883209093\n",
      "Iteration = 2751 \tAcc = 0.95 \tLoss = 0.4017469114977696\n",
      "Iteration = 3001 \tAcc = 0.95 \tLoss = 0.021375131893223345\n",
      "Iteration = 3251 \tAcc = 0.95 \tLoss = 0.15429971662514563\n",
      "Iteration = 3501 \tAcc = 0.95 \tLoss = 0.0004997254136731498\n",
      "Iteration = 3751 \tAcc = 0.95 \tLoss = 0.005664656562346892\n",
      "Iteration = 4001 \tAcc = 0.95 \tLoss = 0.0011771863322346087\n",
      "Iteration = 4251 \tAcc = 0.95 \tLoss = 4.364046896866222e-05\n",
      "Iteration = 4501 \tAcc = 0.95 \tLoss = 0.07953717982766546\n",
      "Iteration = 4751 \tAcc = 0.95 \tLoss = 0.06224195064176642\n",
      "Iteration = 5001 \tAcc = 0.95 \tLoss = 0.06691430199299463\n",
      "Iteration = 5251 \tAcc = 0.95 \tLoss = 0.0030435823502046725\n",
      "Iteration = 5501 \tAcc = 0.95 \tLoss = 0.04540191109166638\n",
      "Iteration = 5751 \tAcc = 0.95 \tLoss = 0.0036836745085306057\n",
      "Iteration = 6001 \tAcc = 0.95 \tLoss = 0.2513697237787769\n",
      "Iteration = 6251 \tAcc = 0.95 \tLoss = 0.038022349868315125\n",
      "Iteration = 6501 \tAcc = 0.95 \tLoss = 0.3383627257541268\n",
      "Iteration = 6751 \tAcc = 0.95 \tLoss = 0.002618570157087833\n",
      "Iteration = 7001 \tAcc = 0.95 \tLoss = 0.004746937193001718\n",
      "Iteration = 7251 \tAcc = 0.95 \tLoss = 0.0029499324639184546\n",
      "Iteration = 7501 \tAcc = 0.95 \tLoss = 0.042559294618270745\n",
      "Iteration = 7751 \tAcc = 0.95 \tLoss = 0.05111043947155221\n",
      "Iteration = 8001 \tAcc = 0.95 \tLoss = 0.0020689896421173955\n",
      "Iteration = 8251 \tAcc = 0.95 \tLoss = 0.04090343209465911\n",
      "Iteration = 8501 \tAcc = 0.95 \tLoss = 5.9703272710790804e-06\n",
      "Iteration = 8751 \tAcc = 0.95 \tLoss = 0.18866323888815678\n",
      "Iteration = 9001 \tAcc = 0.95 \tLoss = 0.007860143324484476\n",
      "Iteration = 9251 \tAcc = 0.95 \tLoss = 0.0016450531618167955\n",
      "Iteration = 9501 \tAcc = 0.95 \tLoss = 3.1135774707915272e-06\n",
      "Iteration = 9751 \tAcc = 0.95 \tLoss = 0.02043009771454764\n",
      "Iteration = 10001 \tAcc = 0.95 \tLoss = 0.0014517492409194964\n",
      "Iteration = 10251 \tAcc = 0.95 \tLoss = 0.4906682360116405\n",
      "Iteration = 10501 \tAcc = 0.95 \tLoss = 0.0018266767030803617\n",
      "Iteration = 10751 \tAcc = 0.95 \tLoss = 2.3769368234278888e-06\n",
      "Iteration = 11001 \tAcc = 0.95 \tLoss = 0.0012707969405275313\n",
      "Iteration = 11251 \tAcc = 0.95 \tLoss = 0.02413398232103042\n",
      "Iteration = 11501 \tAcc = 0.95 \tLoss = 0.22253764103801502\n",
      "Iteration = 11751 \tAcc = 0.95 \tLoss = 1.0177681567490869e-05\n",
      "Iteration = 12001 \tAcc = 0.95 \tLoss = 9.686484511750723e-06\n",
      "Iteration = 12251 \tAcc = 0.95 \tLoss = 0.0631089646038952\n",
      "Iteration = 12501 \tAcc = 0.95 \tLoss = 0.040058921261700714\n",
      "Iteration = 12751 \tAcc = 0.95 \tLoss = 0.006807491235536605\n",
      "Iteration = 13001 \tAcc = 0.95 \tLoss = 0.004000107421862503\n",
      "Iteration = 13251 \tAcc = 0.95 \tLoss = 0.03442303224668209\n",
      "Iteration = 13501 \tAcc = 0.95 \tLoss = 2.560115350534451e-05\n",
      "Iteration = 13751 \tAcc = 0.95 \tLoss = 0.0010451273191665821\n",
      "Iteration = 14001 \tAcc = 0.95 \tLoss = 0.03672152130416933\n",
      "Iteration = 14251 \tAcc = 0.95 \tLoss = 2.6515660655591726e-05\n",
      "Iteration = 14501 \tAcc = 0.95 \tLoss = 0.0009815661420981695\n",
      "Iteration = 14751 \tAcc = 0.95 \tLoss = 0.000926302118918151\n",
      "Iteration = 15001 \tAcc = 0.95 \tLoss = 1.3295688907899655e-06\n",
      "Iteration = 15251 \tAcc = 0.95 \tLoss = 1.2878705124734424e-05\n",
      "Iteration = 15501 \tAcc = 0.95 \tLoss = 1.4583683140455206\n",
      "Iteration = 15751 \tAcc = 0.95 \tLoss = 0.35971737591387226\n",
      "Iteration = 16001 \tAcc = 0.95 \tLoss = 7.863902957481012e-05\n",
      "Iteration = 16251 \tAcc = 0.95 \tLoss = 1.5451033673372345\n",
      "Iteration = 16501 \tAcc = 0.95 \tLoss = 0.34866555152123724\n",
      "Iteration = 16751 \tAcc = 0.95 \tLoss = 0.002076893726053427\n",
      "Iteration = 17001 \tAcc = 0.95 \tLoss = 0.005184330209214002\n",
      "Iteration = 17251 \tAcc = 0.95 \tLoss = 0.06979135186399069\n",
      "Iteration = 17501 \tAcc = 0.95 \tLoss = 0.04553880551870368\n",
      "Iteration = 17751 \tAcc = 0.95 \tLoss = 2.772566867706444e-05\n",
      "Iteration = 18001 \tAcc = 0.95 \tLoss = 0.0300442486023629\n",
      "Iteration = 18251 \tAcc = 0.95 \tLoss = 1.7863437360281933\n",
      "Iteration = 18501 \tAcc = 0.95 \tLoss = 0.0005158129047193997\n",
      "Iteration = 18751 \tAcc = 0.95 \tLoss = 0.00046334437946537836\n",
      "Iteration = 19001 \tAcc = 0.95 \tLoss = 1.9034734329167153e-05\n",
      "Iteration = 19251 \tAcc = 0.95 \tLoss = 0.20409512455291878\n",
      "Iteration = 19501 \tAcc = 0.95 \tLoss = 0.27359366968926807\n",
      "Iteration = 19751 \tAcc = 0.95 \tLoss = 1.7467991470937478\n",
      "Iteration = 20001 \tAcc = 0.95 \tLoss = 1.5262126666940603\n",
      "Iteration = 20251 \tAcc = 0.95 \tLoss = 7.610235908830465e-06\n",
      "Iteration = 20501 \tAcc = 0.95 \tLoss = 0.25660026626146093\n",
      "Iteration = 20751 \tAcc = 0.95 \tLoss = 0.29609061096081324\n",
      "Iteration = 21001 \tAcc = 0.95 \tLoss = 3.011308280937278e-07\n",
      "Iteration = 21251 \tAcc = 0.95 \tLoss = 0.11278390851860648\n",
      "Iteration = 21501 \tAcc = 0.95 \tLoss = 1.0194271589994793e-08\n",
      "Iteration = 21751 \tAcc = 0.95 \tLoss = 0.0016353378278021323\n",
      "Iteration = 22001 \tAcc = 0.95 \tLoss = 0.27094362089290586\n",
      "Iteration = 22251 \tAcc = 0.95 \tLoss = 8.397791814836942e-08\n",
      "Iteration = 22501 \tAcc = 0.95 \tLoss = 2.289562071741883e-06\n",
      "Iteration = 22751 \tAcc = 0.95 \tLoss = 0.02408649598958164\n",
      "Iteration = 23001 \tAcc = 0.95 \tLoss = 0.05059917516870051\n",
      "Iteration = 23251 \tAcc = 0.95 \tLoss = 0.00021304911502061514\n",
      "Iteration = 23501 \tAcc = 0.95 \tLoss = 0.3434578848423758\n",
      "Iteration = 23751 \tAcc = 0.95 \tLoss = 9.851322571782218e-07\n",
      "Iteration = 24001 \tAcc = 0.95 \tLoss = 0.0028355460886336785\n",
      "Iteration = 24251 \tAcc = 0.95 \tLoss = 7.319098625548819e-07\n",
      "Iteration = 24501 \tAcc = 0.95 \tLoss = 0.012730845702658764\n",
      "Iteration = 24751 \tAcc = 0.95 \tLoss = 3.696998263149125e-11\n",
      "Iteration = 25001 \tAcc = 0.95 \tLoss = 0.018045733761706984\n",
      "Iteration = 25251 \tAcc = 0.95 \tLoss = 3.324317116800186e-07\n",
      "Iteration = 25501 \tAcc = 0.95 \tLoss = 0.15883072648864188\n",
      "Iteration = 25751 \tAcc = 0.95 \tLoss = 2.755535759521329e-07\n",
      "Iteration = 26001 \tAcc = 0.95 \tLoss = 0.02422001371201925\n",
      "Iteration = 26251 \tAcc = 0.95 \tLoss = 0.06843698597019066\n",
      "Iteration = 26501 \tAcc = 0.95 \tLoss = 0.00040813464210901504\n",
      "Iteration = 26751 \tAcc = 0.95 \tLoss = 4.6989856451456384e-11\n",
      "Iteration = 27001 \tAcc = 0.95 \tLoss = 0.011885787016009312\n",
      "Iteration = 27251 \tAcc = 0.95 \tLoss = 0.0022624564383133326\n",
      "Iteration = 27501 \tAcc = 0.95 \tLoss = 0.03871208470515409\n",
      "Iteration = 27751 \tAcc = 0.95 \tLoss = 0.07165704960752493\n",
      "Iteration = 28001 \tAcc = 0.95 \tLoss = 0.20512311983311693\n",
      "Iteration = 28251 \tAcc = 0.95 \tLoss = 0.008775779350626157\n",
      "Iteration = 28501 \tAcc = 0.95 \tLoss = 0.020171063279999096\n",
      "Iteration = 28751 \tAcc = 0.95 \tLoss = 0.00012548982172004761\n",
      "Iteration = 29001 \tAcc = 0.95 \tLoss = 0.00011279803374752333\n",
      "Iteration = 29251 \tAcc = 0.95 \tLoss = 3.554847224264926e-08\n",
      "Iteration = 29501 \tAcc = 0.95 \tLoss = 0.00010973939117057855\n",
      "Iteration = 29751 \tAcc = 0.95 \tLoss = 7.916536727612641e-05\n",
      "Iteration = 30001 \tAcc = 0.95 \tLoss = 0.129474451480101\n",
      "Iteration = 30251 \tAcc = 0.95 \tLoss = 0.22309645008104587\n",
      "Iteration = 30501 \tAcc = 0.95 \tLoss = 0.03499845675487965\n",
      "Iteration = 30751 \tAcc = 0.95 \tLoss = 4.2277292777734897e-13\n",
      "Iteration = 31001 \tAcc = 0.95 \tLoss = 8.987253521890278e-05\n",
      "Iteration = 31251 \tAcc = 0.95 \tLoss = 8.48725676503236e-05\n",
      "Iteration = 31501 \tAcc = 1.0 \tLoss = 7.680185118286007e-05\n",
      "Iteration = 31751 \tAcc = 0.95 \tLoss = 1.099120794378911e-14\n",
      "Iteration = 32001 \tAcc = 0.95 \tLoss = 0.0003711664235737421\n",
      "Iteration = 32251 \tAcc = 0.95 \tLoss = 0.025781058605941876\n",
      "Iteration = 32501 \tAcc = 1.0 \tLoss = 0.008805908371043895\n",
      "Iteration = 32751 \tAcc = 0.95 \tLoss = 0.021167841091615083\n",
      "Iteration = 33001 \tAcc = 0.95 \tLoss = 3.820879566548408e-05\n",
      "Iteration = 33251 \tAcc = 0.95 \tLoss = 0.04348602341593221\n",
      "Iteration = 33501 \tAcc = 0.95 \tLoss = 5.7262833635379126e-05\n",
      "Iteration = 33751 \tAcc = 1.0 \tLoss = 0.17954068621905406\n",
      "Iteration = 34001 \tAcc = 0.95 \tLoss = 1.635919525893975e-05\n",
      "Iteration = 34251 \tAcc = 1.0 \tLoss = 0.7138930368709285\n",
      "Iteration = 34501 \tAcc = 0.95 \tLoss = 5.2175805794117806e-05\n",
      "Iteration = 34751 \tAcc = 0.95 \tLoss = 1.1102230246251565e-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 35001 \tAcc = 1.0 \tLoss = 0.00012573864556920494\n",
      "Iteration = 35251 \tAcc = 0.95 \tLoss = 0.007489049294427614\n",
      "Iteration = 35501 \tAcc = 1.0 \tLoss = 0.006452116039589023\n",
      "Iteration = 35751 \tAcc = 0.95 \tLoss = 1.4696022176973995e-12\n",
      "Iteration = 36001 \tAcc = 0.95 \tLoss = 0.004309764138967085\n",
      "Iteration = 36251 \tAcc = 0.95 \tLoss = 0.07422340997921632\n",
      "Iteration = 36501 \tAcc = 0.95 \tLoss = 3.675616603420684e-05\n",
      "Iteration = 36751 \tAcc = 0.95 \tLoss = 0.003959660241477145\n",
      "Iteration = 37001 \tAcc = 0.95 \tLoss = 3.369167001182116e-05\n",
      "Iteration = 37251 \tAcc = 1.0 \tLoss = 2.218824203249962e-05\n",
      "Iteration = 37501 \tAcc = 1.0 \tLoss = 6.92441033009123e-05\n",
      "Iteration = 37751 \tAcc = 0.95 \tLoss = 2.2204460492503136e-16\n",
      "Iteration = 38001 \tAcc = 0.95 \tLoss = 3.1240331974530763e-05\n",
      "Iteration = 38251 \tAcc = 0.95 \tLoss = 0.0062699540182643955\n",
      "Iteration = 38501 \tAcc = 0.95 \tLoss = 2.8478063897961193e-05\n",
      "Iteration = 38751 \tAcc = 0.95 \tLoss = 1.7319479184152593e-14\n",
      "Iteration = 39001 \tAcc = 0.95 \tLoss = 1.032507412901401e-14\n",
      "Iteration = 39251 \tAcc = 0.95 \tLoss = 1.9480964464122813e-05\n",
      "Iteration = 39501 \tAcc = 0.95 \tLoss = 0.0302651097199497\n",
      "Iteration = 39751 \tAcc = 1.0 \tLoss = 5.4334120331162676e-05\n",
      "Iteration = 40001 \tAcc = 1.0 \tLoss = 1.8762769116165322e-14\n",
      "Iteration = 40251 \tAcc = 1.0 \tLoss = 0.005080685573418457\n",
      "Iteration = 40501 \tAcc = 1.0 \tLoss = 1.049749176479334e-11\n",
      "Iteration = 40751 \tAcc = 0.95 \tLoss = 0.003298225627066609\n",
      "Iteration = 41001 \tAcc = 0.95 \tLoss = 0.05239292967536053\n",
      "Iteration = 41251 \tAcc = 1.0 \tLoss = 2.2514887906986842e-05\n",
      "Iteration = 41501 \tAcc = 1.0 \tLoss = 0.03487098225397598\n",
      "Iteration = 41751 \tAcc = 0.95 \tLoss = 0.0011004560980755879\n",
      "Iteration = 42001 \tAcc = 1.0 \tLoss = 0.0071764568175314335\n",
      "Iteration = 42251 \tAcc = 1.0 \tLoss = 0.029827328518031583\n",
      "Iteration = 42501 \tAcc = 1.0 \tLoss = 0.0052484260028656\n",
      "Iteration = 42751 \tAcc = 1.0 \tLoss = 0.023667532719790867\n",
      "Iteration = 43001 \tAcc = 0.95 \tLoss = 7.126536457304906e-06\n",
      "Iteration = 43251 \tAcc = 0.95 \tLoss = 2.461595238791793e-06\n",
      "Iteration = 43501 \tAcc = 1.0 \tLoss = 0.0018918424752012958\n",
      "Iteration = 43751 \tAcc = 0.95 \tLoss = 0.0004582935758817921\n",
      "Iteration = 44001 \tAcc = 1.0 \tLoss = 1.5258353497671842e-05\n",
      "Iteration = 44251 \tAcc = 1.0 \tLoss = 1.519673276108069e-12\n",
      "Iteration = 44501 \tAcc = 1.0 \tLoss = 2.2980336113259026e-06\n",
      "Iteration = 44751 \tAcc = 0.95 \tLoss = 0.02913392156324271\n",
      "Iteration = 45001 \tAcc = 0.95 \tLoss = 2.572984303164312e-06\n",
      "Iteration = 45251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 45501 \tAcc = 1.0 \tLoss = 0.008494059964804002\n",
      "Iteration = 45751 \tAcc = 1.0 \tLoss = 1.1102230246251565e-16\n",
      "Iteration = 46001 \tAcc = 0.95 \tLoss = 8.76407090885079e-06\n",
      "Iteration = 46251 \tAcc = 1.0 \tLoss = 1.2104643487058228e-05\n",
      "Iteration = 46501 \tAcc = 1.0 \tLoss = 0.002066010713873953\n",
      "Iteration = 46751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 47001 \tAcc = 1.0 \tLoss = 1.1122838454744605e-05\n",
      "Iteration = 47251 \tAcc = 0.95 \tLoss = 2.4342126972519806e-06\n",
      "Iteration = 47501 \tAcc = 1.0 \tLoss = 1.7185626130158698e-06\n",
      "Iteration = 47751 \tAcc = 1.0 \tLoss = 0.5945911118428889\n",
      "Iteration = 48001 \tAcc = 1.0 \tLoss = 0.019104924989953156\n",
      "Iteration = 48251 \tAcc = 0.95 \tLoss = 0.017465130396399006\n",
      "Iteration = 48501 \tAcc = 0.95 \tLoss = 9.055681396887725e-06\n",
      "Iteration = 48751 \tAcc = 1.0 \tLoss = 5.21481717087414e-06\n",
      "Iteration = 49001 \tAcc = 1.0 \tLoss = 6.267482094981705e-06\n",
      "Iteration = 49251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 49501 \tAcc = 1.0 \tLoss = 0.0021878661713199497\n",
      "Iteration = 49751 \tAcc = 1.0 \tLoss = 0.004164936408488762\n",
      "Iteration = 50001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 50251 \tAcc = 1.0 \tLoss = 2.3066635387613182e-06\n",
      "Iteration = 50501 \tAcc = 1.0 \tLoss = 0.0013977372809434042\n",
      "Iteration = 50751 \tAcc = 1.0 \tLoss = 1.322525549865536e-06\n",
      "Iteration = 51001 \tAcc = 1.0 \tLoss = 0.0156963216265238\n",
      "Iteration = 51251 \tAcc = 1.0 \tLoss = 0.015576194702603569\n",
      "Iteration = 51501 \tAcc = 0.95 \tLoss = 0.013311950499992418\n",
      "Iteration = 51751 \tAcc = 1.0 \tLoss = 0.0042889013622001045\n",
      "Iteration = 52001 \tAcc = 0.95 \tLoss = 0.015296427191926277\n",
      "Iteration = 52251 \tAcc = 1.0 \tLoss = 0.0010692778742169427\n",
      "Iteration = 52501 \tAcc = 1.0 \tLoss = 0.4354455157879251\n",
      "Iteration = 52751 \tAcc = 1.0 \tLoss = 0.004009636833251477\n",
      "Iteration = 53001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 53251 \tAcc = 1.0 \tLoss = 4.601051106176421e-06\n",
      "Iteration = 53501 \tAcc = 1.0 \tLoss = 0.01428198145489155\n",
      "Iteration = 53751 \tAcc = 0.95 \tLoss = 4.650687412480235e-06\n",
      "Iteration = 54001 \tAcc = 1.0 \tLoss = 5.588150890199124e-07\n",
      "Iteration = 54251 \tAcc = 1.0 \tLoss = 0.002530168042238192\n",
      "Iteration = 54501 \tAcc = 0.95 \tLoss = 0.29169380032694825\n",
      "Iteration = 54751 \tAcc = 0.95 \tLoss = 7.354536040127734e-07\n",
      "Iteration = 55001 \tAcc = 1.0 \tLoss = 0.552464861253257\n",
      "Iteration = 55251 \tAcc = 1.0 \tLoss = 1.384145061705963e-06\n",
      "Iteration = 55501 \tAcc = 1.0 \tLoss = 0.34437890166646895\n",
      "Iteration = 55751 \tAcc = 1.0 \tLoss = 0.004964625231218954\n",
      "Iteration = 56001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 56251 \tAcc = 1.0 \tLoss = 8.595260681189182e-07\n",
      "Iteration = 56501 \tAcc = 1.0 \tLoss = 1.0214051826551492e-14\n",
      "Iteration = 56751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 57001 \tAcc = 1.0 \tLoss = 1.1128833391214145e-06\n",
      "Iteration = 57251 \tAcc = 1.0 \tLoss = 0.4488842538639786\n",
      "Iteration = 57501 \tAcc = 1.0 \tLoss = 0.0024518933327626574\n",
      "Iteration = 57751 \tAcc = 1.0 \tLoss = 2.830556594480447e-07\n",
      "Iteration = 58001 \tAcc = 1.0 \tLoss = 0.4749505598700817\n",
      "Iteration = 58251 \tAcc = 0.95 \tLoss = 1.204930108198228e-06\n",
      "Iteration = 58501 \tAcc = 1.0 \tLoss = 0.011456440490510807\n",
      "Iteration = 58751 \tAcc = 1.0 \tLoss = 1.2110542443557979e-06\n",
      "Iteration = 59001 \tAcc = 0.95 \tLoss = 1.1522027907159488e-06\n",
      "Iteration = 59251 \tAcc = 1.0 \tLoss = 0.0011224596640718235\n",
      "Iteration = 59501 \tAcc = 0.95 \tLoss = 1.6088556356247548e-06\n",
      "Iteration = 59751 \tAcc = 1.0 \tLoss = 2.903166927970462e-06\n",
      "Iteration = 60001 \tAcc = 1.0 \tLoss = 0.010440237868011952\n",
      "Iteration = 60251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 60501 \tAcc = 1.0 \tLoss = 0.0008658145588909882\n",
      "Iteration = 60751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 61001 \tAcc = 0.95 \tLoss = 1.6024065648731165e-06\n",
      "Iteration = 61251 \tAcc = 0.95 \tLoss = 0.002023793144994289\n",
      "Iteration = 61501 \tAcc = 1.0 \tLoss = 0.009714603029318824\n",
      "Iteration = 61751 \tAcc = 1.0 \tLoss = 2.919815174595003e-06\n",
      "Iteration = 62001 \tAcc = 1.0 \tLoss = 0.01064432376310858\n",
      "Iteration = 62251 \tAcc = 1.0 \tLoss = 0.0011325899600702631\n",
      "Iteration = 62501 \tAcc = 0.95 \tLoss = 0.005310281608621941\n",
      "Iteration = 62751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 63001 \tAcc = 1.0 \tLoss = 6.550315845288445e-15\n",
      "Iteration = 63251 \tAcc = 1.0 \tLoss = 0.0040071560903891055\n",
      "Iteration = 63501 \tAcc = 1.0 \tLoss = 0.42079699438213847\n",
      "Iteration = 63751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 64001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 64251 \tAcc = 0.95 \tLoss = 0.001334608190536629\n",
      "Iteration = 64501 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 64751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 65001 \tAcc = 1.0 \tLoss = 0.010913680547764499\n",
      "Iteration = 65251 \tAcc = 1.0 \tLoss = 0.015385830197826127\n",
      "Iteration = 65501 \tAcc = 1.0 \tLoss = 1.3067017561047624e-06\n",
      "Iteration = 65751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 66001 \tAcc = 1.0 \tLoss = 0.0008960127764593233\n",
      "Iteration = 66251 \tAcc = 1.0 \tLoss = 1.065814103640156e-14\n",
      "Iteration = 66501 \tAcc = 1.0 \tLoss = 0.5978984922316214\n",
      "Iteration = 66751 \tAcc = 1.0 \tLoss = 0.29230447835665946\n",
      "Iteration = 67001 \tAcc = 0.95 \tLoss = 0.12246780933012198\n",
      "Iteration = 67251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 67501 \tAcc = 1.0 \tLoss = 0.17463274256038758\n",
      "Iteration = 67751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 68001 \tAcc = 1.0 \tLoss = 2.492129358824517e-06\n",
      "Iteration = 68251 \tAcc = 1.0 \tLoss = 2.7866597918091817e-14\n",
      "Iteration = 68501 \tAcc = 1.0 \tLoss = 1.9458238137611198e-07\n",
      "Iteration = 68751 \tAcc = 1.0 \tLoss = 2.912960157692227e-07\n",
      "Iteration = 69001 \tAcc = 1.0 \tLoss = 0.0009230809982729635\n",
      "Iteration = 69251 \tAcc = 1.0 \tLoss = 0.0004464562965793789\n",
      "Iteration = 69501 \tAcc = 1.0 \tLoss = 0.009279197106395208\n",
      "Iteration = 69751 \tAcc = 1.0 \tLoss = 0.18654929818541324\n",
      "Iteration = 70001 \tAcc = 1.0 \tLoss = 0.0009892757618311853\n",
      "Iteration = 70251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 70501 \tAcc = 1.0 \tLoss = 2.3849043432791766e-06\n",
      "Iteration = 70751 \tAcc = 1.0 \tLoss = 1.386391138072708e-07\n",
      "Iteration = 71001 \tAcc = 1.0 \tLoss = 0.003636144434346897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 71251 \tAcc = 1.0 \tLoss = 1.0012078412389748e-07\n",
      "Iteration = 71501 \tAcc = 1.0 \tLoss = 3.7960557616140933e-06\n",
      "Iteration = 71751 \tAcc = 1.0 \tLoss = 0.01141014349120305\n",
      "Iteration = 72001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 72251 \tAcc = 1.0 \tLoss = 0.000694511402376017\n",
      "Iteration = 72501 \tAcc = 1.0 \tLoss = 5.5439426587472295e-08\n",
      "Iteration = 72751 \tAcc = 1.0 \tLoss = 0.24908564165246921\n",
      "Iteration = 73001 \tAcc = 1.0 \tLoss = 2.2093438190040858e-14\n",
      "Iteration = 73251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 73501 \tAcc = 1.0 \tLoss = 0.11211618933329084\n",
      "Iteration = 73751 \tAcc = 1.0 \tLoss = 0.000832242226835554\n",
      "Iteration = 74001 \tAcc = 1.0 \tLoss = 0.008666427610414577\n",
      "Iteration = 74251 \tAcc = 1.0 \tLoss = 1.4172235301488469e-07\n",
      "Iteration = 74501 \tAcc = 1.0 \tLoss = 0.24787757854994757\n",
      "Iteration = 74751 \tAcc = 1.0 \tLoss = 7.965299459717195e-08\n",
      "Iteration = 75001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 75251 \tAcc = 1.0 \tLoss = 0.0003491558697197592\n",
      "Iteration = 75501 \tAcc = 1.0 \tLoss = 0.00655744085392495\n",
      "Iteration = 75751 \tAcc = 1.0 \tLoss = 5.311652830773367e-06\n",
      "Iteration = 76001 \tAcc = 1.0 \tLoss = 0.11538222809430373\n",
      "Iteration = 76251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 76501 \tAcc = 1.0 \tLoss = 0.0007042584932954128\n",
      "Iteration = 76751 \tAcc = 1.0 \tLoss = 6.138178065476957e-08\n",
      "Iteration = 77001 \tAcc = 1.0 \tLoss = 0.007958996134423127\n",
      "Iteration = 77251 \tAcc = 1.0 \tLoss = 1.1689385487485589e-05\n",
      "Iteration = 77501 \tAcc = 1.0 \tLoss = 0.0008791032278404811\n",
      "Iteration = 77751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 78001 \tAcc = 1.0 \tLoss = 5.857557233171938e-08\n",
      "Iteration = 78251 \tAcc = 1.0 \tLoss = 0.000286958500486122\n",
      "Iteration = 78501 \tAcc = 1.0 \tLoss = 5.453994439195165e-06\n",
      "Iteration = 78751 \tAcc = 1.0 \tLoss = 5.233512404947432e-08\n",
      "Iteration = 79001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 79251 \tAcc = 1.0 \tLoss = 0.12707152260433593\n",
      "Iteration = 79501 \tAcc = 1.0 \tLoss = 0.16006468313368225\n",
      "Iteration = 79751 \tAcc = 1.0 \tLoss = 3.4013739959078944e-08\n",
      "Iteration = 80001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 80251 \tAcc = 1.0 \tLoss = 1.713112612506123e-05\n",
      "Iteration = 80501 \tAcc = 1.0 \tLoss = 0.00035342062855458323\n",
      "Iteration = 80751 \tAcc = 1.0 \tLoss = 5.08120816120281e-08\n",
      "Iteration = 81001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 81251 \tAcc = 1.0 \tLoss = 2.1263067603108078e-05\n",
      "Iteration = 81501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 81751 \tAcc = 1.0 \tLoss = 0.0002489757779863814\n",
      "Iteration = 82001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 82251 \tAcc = 1.0 \tLoss = 1.1006937200089063e-08\n",
      "Iteration = 82501 \tAcc = 1.0 \tLoss = 0.0002681254097457175\n",
      "Iteration = 82751 \tAcc = 1.0 \tLoss = 8.864154782835245e-09\n",
      "Iteration = 83001 \tAcc = 1.0 \tLoss = 0.008058660629767692\n",
      "Iteration = 83251 \tAcc = 1.0 \tLoss = 1.2989609388114415e-14\n",
      "Iteration = 83501 \tAcc = 1.0 \tLoss = 0.0009316383001860916\n",
      "Iteration = 83751 \tAcc = 1.0 \tLoss = 0.0008929398541651585\n",
      "Iteration = 84001 \tAcc = 1.0 \tLoss = 1.5096672603696171e-05\n",
      "Iteration = 84251 \tAcc = 1.0 \tLoss = 0.0001810929476500184\n",
      "Iteration = 84501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 84751 \tAcc = 1.0 \tLoss = 0.002144642726669244\n",
      "Iteration = 85001 \tAcc = 1.0 \tLoss = 0.05014210389163569\n",
      "Iteration = 85251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 85501 \tAcc = 1.0 \tLoss = 2.796338964185285e-05\n",
      "Iteration = 85751 \tAcc = 1.0 \tLoss = 1.1324274851176661e-14\n",
      "Iteration = 86001 \tAcc = 1.0 \tLoss = 1.3436017055947693e-08\n",
      "Iteration = 86251 \tAcc = 1.0 \tLoss = 1.7783677355643315e-08\n",
      "Iteration = 86501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 86751 \tAcc = 1.0 \tLoss = 1.8619744923269037e-05\n",
      "Iteration = 87001 \tAcc = 1.0 \tLoss = 1.120143606370987e-08\n",
      "Iteration = 87251 \tAcc = 1.0 \tLoss = 0.0875591648932727\n",
      "Iteration = 87501 \tAcc = 1.0 \tLoss = 5.210355408274107e-05\n",
      "Iteration = 87751 \tAcc = 1.0 \tLoss = 0.003852474484776912\n",
      "Iteration = 88001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 88251 \tAcc = 1.0 \tLoss = 2.5275335810658468e-08\n",
      "Iteration = 88501 \tAcc = 1.0 \tLoss = 0.0026672398121228252\n",
      "Iteration = 88751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 89001 \tAcc = 1.0 \tLoss = 0.0018268762480565463\n",
      "Iteration = 89251 \tAcc = 1.0 \tLoss = 0.0006623802533737342\n",
      "Iteration = 89501 \tAcc = 1.0 \tLoss = 0.005304676767099161\n",
      "Iteration = 89751 \tAcc = 1.0 \tLoss = 2.544891955584013e-05\n",
      "Iteration = 90001 \tAcc = 1.0 \tLoss = 6.883382752675994e-15\n",
      "Iteration = 90251 \tAcc = 1.0 \tLoss = 0.0015310894480088841\n",
      "Iteration = 90501 \tAcc = 1.0 \tLoss = 6.994405055138511e-15\n",
      "Iteration = 90751 \tAcc = 1.0 \tLoss = 2.2645558787165918e-09\n",
      "Iteration = 91001 \tAcc = 1.0 \tLoss = 0.0041395960850940106\n",
      "Iteration = 91251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 91501 \tAcc = 1.0 \tLoss = 0.10387426551619716\n",
      "Iteration = 91751 \tAcc = 1.0 \tLoss = 8.640155855751673e-05\n",
      "Iteration = 92001 \tAcc = 1.0 \tLoss = 0.0004791772634660266\n",
      "Iteration = 92251 \tAcc = 1.0 \tLoss = 1.537514006183103e-09\n",
      "Iteration = 92501 \tAcc = 1.0 \tLoss = 3.151682480158687e-05\n",
      "Iteration = 92751 \tAcc = 1.0 \tLoss = 0.005422187281358995\n",
      "Iteration = 93001 \tAcc = 1.0 \tLoss = 3.357582120956731e-05\n",
      "Iteration = 93251 \tAcc = 1.0 \tLoss = 0.00014594848548738694\n",
      "Iteration = 93501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 93751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 94001 \tAcc = 1.0 \tLoss = 4.147852290506912e-05\n",
      "Iteration = 94251 \tAcc = 1.0 \tLoss = 4.6629367034256685e-15\n",
      "Iteration = 94501 \tAcc = 1.0 \tLoss = 0.004578096073547178\n",
      "Iteration = 94751 \tAcc = 1.0 \tLoss = 0.0001321462638816553\n",
      "Iteration = 95001 \tAcc = 1.0 \tLoss = 2.4719307981391033e-05\n",
      "Iteration = 95251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 95501 \tAcc = 1.0 \tLoss = 3.6138870850097396e-09\n",
      "Iteration = 95751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 96001 \tAcc = 1.0 \tLoss = 0.0005620748990003512\n",
      "Iteration = 96251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 96501 \tAcc = 1.0 \tLoss = 6.948194410017081e-05\n",
      "Iteration = 96751 \tAcc = 1.0 \tLoss = 0.005476894066180912\n",
      "Iteration = 97001 \tAcc = 1.0 \tLoss = 7.084044986145364e-05\n",
      "Iteration = 97251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 97501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 97751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 98001 \tAcc = 1.0 \tLoss = 0.0023688388072371754\n",
      "Iteration = 98251 \tAcc = 1.0 \tLoss = 1.9349311061024654e-09\n",
      "Iteration = 98501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 98751 \tAcc = 1.0 \tLoss = 0.0026914014551353144\n",
      "Iteration = 99001 \tAcc = 1.0 \tLoss = 6.774921737026244e-10\n",
      "Iteration = 99251 \tAcc = 1.0 \tLoss = 0.00011867069478510074\n",
      "Iteration = 99501 \tAcc = 1.0 \tLoss = 0.00014512680969671281\n",
      "Iteration = 99751 \tAcc = 1.0 \tLoss = 0.0056863205036013645\n"
     ]
    }
   ],
   "source": [
    "model.sgd(X, Y, 100000, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like it has fitted the data 100\\%. Let's see what the decision boundary looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a grid of points to classify\n",
    "xx1, xx2 = np.meshgrid(np.arange(-3, 3, 0.005), np.arange(-3, 3, 0.005))\n",
    "\n",
    "# Flatten the grid to pass into model\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()].T\n",
    "\n",
    "# Predict classification at every point on the grid\n",
    "Z = model.forward(grid)[1,:].reshape(xx1.shape)\n",
    "\n",
    "# Plot the prediction regions.\n",
    "plt.imshow(Z, interpolation='bicubic', origin='lower', extent=[-3, 3, -3, 3], \n",
    "           cmap=ListedColormap(['#1f77b4', '#ff7f0e']), alpha=0.55, aspect='auto')\n",
    "\n",
    "# Plot the original points.\n",
    "_ = plt.scatter(X[0,:], X[1,:], c=Y[1,:], cmap=ListedColormap(['#1f77b4', '#ff7f0e']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
