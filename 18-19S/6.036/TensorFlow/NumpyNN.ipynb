{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_for_hw7 import *\n",
    "import numpy as np\n",
    "import modules_disp as disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"A simple, fully-connected linear layer.\"\"\"\n",
    "\n",
    "    def __init__(self, m, n):\n",
    "        \"\"\"Initializes the layer based on input and output dimensions. \n",
    "\n",
    "        Note: Kernel is initialized using normal distribution with mean 0 and \n",
    "        variance 1 / m. All biases are initialized to zero.\n",
    "\n",
    "        Args:\n",
    "            m (int): Number of inputs to the layer.\n",
    "            n (int): Number of outputs from the layer.\n",
    "\n",
    "        \"\"\"\n",
    "        self.m, self.n = m, n\n",
    "\n",
    "        self.W0 = np.zeros((n, 1))\n",
    "        self.W = np.random.normal(0, np.sqrt(1 / m), (m, n))\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"Computes the forward pass through the linear network for a batch.\n",
    "\n",
    "        Args:\n",
    "            A (ndarray): An m by b matrix representing the m activations from the\n",
    "                previous layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the result of passing the \n",
    "                activations through the network layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "\n",
    "        return self.W.T @ self.A + self.W0\n",
    "\n",
    "    def backward(self, dLdZ):\n",
    "        \"\"\"Uses the gradient of loss with respect to outputs of the layer for a \n",
    "        batch to update the sum of gradients of the loss with respect to the \n",
    "        weights for the entire batch. Also returns the gradient of the loss with \n",
    "        respect to the inputs to the layer for a batch.\n",
    "\n",
    "        Args:\n",
    "            dLdZ (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the layer outputs for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An m by b matrix representing the gradient of the loss with \n",
    "                respect to the inputs to the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.dLdW = self.A @ dLdZ.T  # Implicit sum over all b\n",
    "        self.dLdW0 = np.sum(dLdZ, axis=1, keepdims=True)\n",
    "\n",
    "        return self.W @ dLdZ\n",
    "\n",
    "    def sgd_step(self, lrate):\n",
    "        \"\"\"Performs a single step of gradient descent to update the weights for a \n",
    "        single batch of points.\n",
    "\n",
    "        Args:\n",
    "            lrate (float): A learning rate to scale the gradient for the update.\n",
    "\n",
    "        \"\"\"\n",
    "        self.W = self.W - lrate * self.dLdW\n",
    "        self.W0 = self.W0 - lrate * self.dLdW0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_forward: OK\n",
      "linear_backward: OK\n",
      "linear_sgd_step_W: OK\n",
      "linear_sgd_step_W0: OK\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# data\n",
    "X, Y = super_simple_separable()\n",
    "\n",
    "# module\n",
    "linear_1 = Linear(2, 3)\n",
    "\n",
    "#hyperparameters\n",
    "lrate = 0.005\n",
    "\n",
    "# test case\n",
    "# forward\n",
    "z_1 = linear_1.forward(X)\n",
    "exp_z_1 =  np.array([[10.41750064, 6.91122168, 20.73366505, 22.8912344],\n",
    "                     [7.16872235, 3.48998746, 10.46996239, 9.9982611],\n",
    "                     [-2.07105455, 0.69413716, 2.08241149, 4.84966811]])\n",
    "unit_test(\"linear_forward\", exp_z_1, z_1)\n",
    "\n",
    "# backward\n",
    "dL_dz1 = np.array([[1.69467553e-09, -1.33530535e-06, 0.00000000e+00, -0.00000000e+00],\n",
    "                                     [-5.24547376e-07, 5.82459519e-04, -3.84805202e-10, 1.47943038e-09],\n",
    "                                     [-3.47063705e-02, 2.55611604e-01, -1.83538094e-02, 1.11838432e-04]])\n",
    "exp_dLdX = np.array([[-2.40194628e-02, 1.77064845e-01, -1.27021626e-02, 7.74006953e-05],\n",
    "                                    [2.39827939e-02, -1.75870737e-01, 1.26832126e-02, -7.72828555e-05]])\n",
    "dLdX = linear_1.backward(dL_dz1)\n",
    "unit_test(\"linear_backward\", exp_dLdX, dLdX)\n",
    "\n",
    "# sgd step\n",
    "linear_1.sgd_step(lrate)\n",
    "exp_linear_1_W = np.array([[1.2473734,  0.28294514,  0.68940437],\n",
    "                           [1.58455079, 1.32055711, -0.69218045]]),\n",
    "unit_test(\"linear_sgd_step_W\",  exp_linear_1_W,  linear_1.W)\n",
    "\n",
    "exp_linear_1_W0 = np.array([[6.66805339e-09],\n",
    "                            [-2.90968033e-06],\n",
    "                            [-1.01331631e-03]]),\n",
    "unit_test(\"linear_sgd_step_W0\", exp_linear_1_W0, linear_1.W0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    \"\"\"Hyperbolic tangent activation layer.\"\"\"\n",
    "\n",
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the output of the hyperbolic tangent activation layer.\n",
    "\n",
    "        Args:\n",
    "            Z (ndarray): An n by b matrix representing the input pre-activations\n",
    "                of the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the output of the layer after\n",
    "                using the hyperbolic tangent activation on all inputs for a batch\n",
    "                of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = np.tanh(Z)\n",
    "\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return (1 - self.A ** 2) * dLdA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"Rectified linear unit layer.\"\"\"\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the output of the rectified linear unit layer.\n",
    "        \n",
    "        Args:\n",
    "            Z (ndarray): An n by b matrix representing the input pre-activations\n",
    "                of the layer for a batch of size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the output of the layer after\n",
    "                using the rectified linear activation on all inputs for a batch\n",
    "                of size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.A = np.maximum(0, Z)\n",
    "        \n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return dLdA * (self.A != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    \"\"\"Softmax activation layer.\"\"\"\n",
    "\n",
    "    def forward(self, Z):\n",
    "        \"\"\"Computes the softmax activation given the inputs from the previous\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            Z (ndarray): An n by b matrix representing the inputs to the softmax\n",
    "                layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix of outputs from softmax for a batch of \n",
    "                size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.A = np.exp(Z) / np.sum(np.exp(Z), axis=0, keepdims=True)\n",
    "        \n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradient of the loss with respect to the inputs to the\n",
    "        layer using the gradient of the loss with respect to the outputs of the\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the loss\n",
    "                with respect to the outputs for the layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of the loss with\n",
    "                respect to the inputs of the layer for a batch of size b.\n",
    "                \n",
    "        \"\"\"\n",
    "        n, _ = dLdA.shape\n",
    "        \n",
    "        # Don't mind this nonsense... Trust me, it works. It just splits the\n",
    "        # gradient calculation into two parts rather than using the simplification\n",
    "        # A - Y that was used previously. Actually, this method is more universal:\n",
    "        # it applies even when Y is not one-hot encoded!\n",
    "        dAdZ = np.einsum('ji,jk->ijk', self.A * (1 - self.A), np.eye(n)) \\\n",
    "                + np.einsum('ki,ji,jk->ijk', -self.A, self.A, np.ones((n, n)) - np.eye(n))\n",
    "        \n",
    "        return np.einsum('jik,kj->ij', dAdZ, dLdA)\n",
    "\n",
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        \"\"\"Computes the index of maximum value given the softmax outputs from a\n",
    "        layer for a single batch.\n",
    "\n",
    "        Args:\n",
    "            Ypred (ndarray): An n by b matrix representing the softmax outputs of a\n",
    "                layer for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: A 1 by b row vectors representing the indices of maximum value\n",
    "                for each output from a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.argmax(Ypred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([[1, 0],\n",
    "              [0, 0],\n",
    "              [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[5, 4],\n",
    "              [7, 9],\n",
    "              [3, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAdZ0 = np.array([[A[0,0]*(1-A[0,0]),    -A[0,0]*A[1,0],    -A[0,0]*A[2,0]],\n",
    "                  [   -A[1,0]*A[0,0], A[1,0]*(1-A[1,0]),    -A[1,0]*A[2,0]],\n",
    "                  [   -A[2,0]*A[0,0],    -A[2,0]*A[1,0], A[2,0]*(1-A[2,0])]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-20, -35, -15],\n",
       "       [-35, -42, -21],\n",
       "       [-15, -21,  -6]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAdZ0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAdZ = np.einsum('ji,jk->ijk', A * (1 - A), np.eye(3)) \\\n",
    "       + np.einsum('ki,ji,jk->ijk', -A, A, np.ones((3, 3)) - np.eye(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-20., -35., -15.],\n",
       "        [-35., -42., -21.],\n",
       "        [-15., -21.,  -6.]],\n",
       "\n",
       "       [[-12., -36.,  -8.],\n",
       "        [-36., -72., -18.],\n",
       "        [ -8., -18.,  -2.]]])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAdZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdA = -Y / A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2,  0. ],\n",
       "       [ 0. ,  0. ],\n",
       "       [ 0. , -0.5]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdZ0 = dAdZ0 @ dLdA[:, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.],\n",
       "       [7.],\n",
       "       [3.]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdZ0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdZ = np.einsum('jik,kj->ij', dAdZ, dLdA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 4.],\n",
       "       [7., 9.],\n",
       "       [3., 1.]])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLL(Module):\n",
    "    \"\"\"Negative log-likelihood loss layer.\"\"\"\n",
    "\n",
    "    def forward(self, Ypred, Y):\n",
    "        \"\"\"Computes the loss given the predicted and actual results.\n",
    "\n",
    "        Args:\n",
    "            Ypred (ndarray): An n by b matrix representing the predicted results\n",
    "                from the network for a batch of size b.\n",
    "            Y (ndarray): An n by b matrix representing the actual expected results\n",
    "                for a batch of size b.\n",
    "\n",
    "        Returns:\n",
    "            float: A scalar representing the total loss for each of the outputs\n",
    "                in a batch of size b.\n",
    "\n",
    "        \"\"\"\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "\n",
    "        return -np.sum(self.Y * np.log(self.Ypred))\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"Computes the gradient of the loss with respect to predicted targets for\n",
    "        a single batch.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: An n by b matrix representing the gradient of loss with\n",
    "                respect to predicted targets for a batch of size b.\n",
    "                \n",
    "        \"\"\"\n",
    "        return -self.Y / self.Ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_1.W: OK\n",
      "linear_1.W0: OK\n",
      "linear_2.W: OK\n",
      "linear_2.W0: OK\n",
      "z_1: OK\n",
      "a_1: OK\n",
      "z_2: OK\n",
      "a_2: OK\n",
      "loss: OK\n",
      "dloss: FAILED\n",
      "expected: [[ 0.74498961 -0.52860334  0.4027417  -0.60278945]\n",
      " [-0.74498961  0.52860334 -0.4027417   0.60278945]]\n",
      "but was: [[ 0.         -2.12135574  0.         -2.51755652]\n",
      " [-3.92140888  0.         -1.67431747  0.        ]]\n",
      "dL_dz2: OK\n",
      "dL_da1: OK\n",
      "dL_dz1: OK\n",
      "dL_dX: OK\n",
      "updated_linear_1.W: OK\n",
      "updated_linear_1.W0: OK\n",
      "updated_linear_2.W: OK\n",
      "updated_linear_2.W0: OK\n"
     ]
    }
   ],
   "source": [
    "# TEST 1: sgd_test for Tanh activation and SoftMax output\n",
    "np.random.seed(0)\n",
    "sgd_test(Sequential([Linear(2,3), Tanh(), Linear(3,2), SoftMax()], NLL()), test_1_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"A standard neural network model with linear stacked layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, modules, loss):\n",
    "        \"\"\"Initialize the modules and the loss for the network.\n",
    "        \n",
    "        Args:\n",
    "            modules (list of Module): A list of modules to make up the linear\n",
    "                neural network.\n",
    "            loss (Module): A final module to use to compute the loss of the\n",
    "                neural network.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "\n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):\n",
    "        \"\"\"Trains the neural network by running stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): A d by n matrix representing n training data points\n",
    "                each with d dimensions.\n",
    "            Y (ndarray): A 1 by n matrix representing n training labels.\n",
    "            iters (int): The number of iterations to run stochastic graident\n",
    "                descent.\n",
    "            lrate (float): The step size for stochastic gradient descent.\n",
    "        \n",
    "        \"\"\"\n",
    "        d, n = X.shape\n",
    "        \n",
    "        for it in range(iters):\n",
    "            \n",
    "            t = np.random.randint(n)\n",
    "            \n",
    "            Xt = X[:, t:t + 1]\n",
    "            Yt = Y[:, t:t + 1]\n",
    "            \n",
    "            loss = self.loss.forward(self.forward(Xt), Yt)\n",
    "            self.backward(self.loss.backward())      \n",
    "            \n",
    "            self.print_accuracy(it, X, Y, loss)\n",
    "            \n",
    "            self.sgd_step(lrate)\n",
    "\n",
    "    def forward(self, Xt):\n",
    "        \"\"\"Predicts the output for a training input batch.\n",
    "        \n",
    "        Args:\n",
    "            Xt (ndarray): A d by b matrix of points to predict\n",
    "                with dimension d and batch size b.\n",
    "        \n",
    "        Returns:\n",
    "            ndarray: A 1 by b matrix representing the predicted\n",
    "                outputs of the neural network for a batch size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        for m in self.modules:\n",
    "            Xt = m.forward(Xt)\n",
    "            \n",
    "        return Xt\n",
    "\n",
    "    def backward(self, dLdA):\n",
    "        \"\"\"Computes the gradients of the loss with respect to each weight\n",
    "        in the neural network to prepare for stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            dLdA (ndarray): An n by b matrix representing the gradient of the\n",
    "                loss with respect to the outputs of the neural network for a\n",
    "                batch of size b.\n",
    "        \n",
    "        \"\"\"\n",
    "        for m in self.modules[::-1]:\n",
    "            dLdA = m.backward(dLdA)\n",
    "\n",
    "    def sgd_step(self, lrate):\n",
    "        \"\"\"Runs a single update step on the weight matrices throughout the\n",
    "        neural network using stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            lrate (float): Learning rate for the update step.\n",
    "        \n",
    "        \"\"\"\n",
    "        for m in self.modules:\n",
    "            m.sgd_step(lrate)\n",
    "\n",
    "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
    "        \"\"\"Displays current prediction statistics.\n",
    "        \n",
    "        Args:\n",
    "            it (int): Current iteration.\n",
    "            X (ndarray): A d by n matrix of n points to evaluate, each with\n",
    "                d dimensions.\n",
    "            Y (ndarray): A 1 by n vector of n labels.\n",
    "            cur_loss (float): Current loss.\n",
    "            every (int): Frequency to output statistics.\n",
    "        \n",
    "        \"\"\"\n",
    "        if it % every == 1:\n",
    "            \n",
    "            cf = self.modules[-1].class_fun\n",
    "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "            \n",
    "            print('Iteration =', it, '\\tAcc =', acc, '\\tLoss =', cur_loss, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.35 \tLoss = 1.1909562362636668\n",
      "Iteration = 251 \tAcc = 0.9 \tLoss = 0.04743846432527709\n",
      "Iteration = 501 \tAcc = 0.9 \tLoss = 0.06128737046652348\n",
      "Iteration = 751 \tAcc = 0.9 \tLoss = 0.11068909178162331\n",
      "Iteration = 1001 \tAcc = 0.9 \tLoss = 0.8212849360712676\n",
      "Iteration = 1251 \tAcc = 0.9 \tLoss = 0.17631650481143035\n",
      "Iteration = 1501 \tAcc = 0.95 \tLoss = 1.8830886295052687\n",
      "Iteration = 1751 \tAcc = 0.95 \tLoss = 0.3284693537790125\n",
      "Iteration = 2001 \tAcc = 0.95 \tLoss = 0.008358742246127895\n",
      "Iteration = 2251 \tAcc = 0.95 \tLoss = 0.0005570391881270971\n",
      "Iteration = 2501 \tAcc = 0.95 \tLoss = 0.05127695010422399\n",
      "Iteration = 2751 \tAcc = 0.95 \tLoss = 0.00025954056558243445\n",
      "Iteration = 3001 \tAcc = 0.95 \tLoss = 0.0009724307199789167\n",
      "Iteration = 3251 \tAcc = 0.95 \tLoss = 0.01546586532483277\n",
      "Iteration = 3501 \tAcc = 0.95 \tLoss = 0.0004974997983775112\n",
      "Iteration = 3751 \tAcc = 0.95 \tLoss = 0.21582853262485893\n",
      "Iteration = 4001 \tAcc = 0.95 \tLoss = 0.4443563522565856\n",
      "Iteration = 4251 \tAcc = 0.95 \tLoss = 0.28253738715155846\n",
      "Iteration = 4501 \tAcc = 0.95 \tLoss = 9.457819008684443e-05\n",
      "Iteration = 4751 \tAcc = 0.95 \tLoss = 1.7288172794253838e-05\n",
      "Iteration = 5001 \tAcc = 0.95 \tLoss = 0.0012222574944608297\n",
      "Iteration = 5251 \tAcc = 0.95 \tLoss = 0.0012894553515395912\n",
      "Iteration = 5501 \tAcc = 0.95 \tLoss = 0.00034631238665785237\n",
      "Iteration = 5751 \tAcc = 0.95 \tLoss = 0.45312585350972534\n",
      "Iteration = 6001 \tAcc = 0.95 \tLoss = 0.0002625674720698635\n",
      "Iteration = 6251 \tAcc = 0.95 \tLoss = 0.0007895574669443558\n",
      "Iteration = 6501 \tAcc = 0.95 \tLoss = 0.0006426585075501762\n",
      "Iteration = 6751 \tAcc = 0.95 \tLoss = 0.0052786307597427\n",
      "Iteration = 7001 \tAcc = 0.95 \tLoss = 0.25112571615659574\n",
      "Iteration = 7251 \tAcc = 0.95 \tLoss = 0.0005353736377157561\n",
      "Iteration = 7501 \tAcc = 0.95 \tLoss = 2.157863755425874e-08\n",
      "Iteration = 7751 \tAcc = 0.95 \tLoss = 0.3209635146611814\n",
      "Iteration = 8001 \tAcc = 0.95 \tLoss = 1.1217991843881994e-05\n",
      "Iteration = 8251 \tAcc = 0.95 \tLoss = 0.22373169214012253\n",
      "Iteration = 8501 \tAcc = 0.95 \tLoss = 7.284687324359388e-09\n",
      "Iteration = 8751 \tAcc = 0.95 \tLoss = 7.675267338153437e-06\n",
      "Iteration = 9001 \tAcc = 0.95 \tLoss = 5.699569538551159e-07\n",
      "Iteration = 9251 \tAcc = 0.95 \tLoss = 5.765231701118063e-05\n",
      "Iteration = 9501 \tAcc = 0.95 \tLoss = 0.00030668331590967177\n",
      "Iteration = 9751 \tAcc = 0.95 \tLoss = 0.00033685246413129677\n",
      "Iteration = 10001 \tAcc = 0.95 \tLoss = 0.3111740715436909\n",
      "Iteration = 10251 \tAcc = 0.95 \tLoss = 0.0020438684755051317\n",
      "Iteration = 10501 \tAcc = 0.95 \tLoss = 0.0038059347007149266\n",
      "Iteration = 10751 \tAcc = 0.95 \tLoss = 0.2679592989029851\n",
      "Iteration = 11001 \tAcc = 0.95 \tLoss = 1.2120689339185122e-06\n",
      "Iteration = 11251 \tAcc = 0.95 \tLoss = 0.00012175085846732557\n",
      "Iteration = 11501 \tAcc = 0.95 \tLoss = 0.0001659722034571941\n",
      "Iteration = 11751 \tAcc = 0.95 \tLoss = 0.10746705466834365\n",
      "Iteration = 12001 \tAcc = 0.95 \tLoss = 1.8265711644028524\n",
      "Iteration = 12251 \tAcc = 0.95 \tLoss = 0.019951449929360283\n",
      "Iteration = 12501 \tAcc = 0.95 \tLoss = 5.89394401688047e-06\n",
      "Iteration = 12751 \tAcc = 0.95 \tLoss = 1.512346093457176\n",
      "Iteration = 13001 \tAcc = 0.95 \tLoss = 4.5826364041216605e-06\n",
      "Iteration = 13251 \tAcc = 0.95 \tLoss = 6.665633148963097e-05\n",
      "Iteration = 13501 \tAcc = 0.95 \tLoss = 0.06588241950786844\n",
      "Iteration = 13751 \tAcc = 0.95 \tLoss = 3.62011828533644e-06\n",
      "Iteration = 14001 \tAcc = 0.95 \tLoss = 6.952222377529156e-09\n",
      "Iteration = 14251 \tAcc = 0.95 \tLoss = 5.831665722895161e-09\n",
      "Iteration = 14501 \tAcc = 0.95 \tLoss = 0.011445672613561952\n",
      "Iteration = 14751 \tAcc = 0.95 \tLoss = 0.002622715393334943\n",
      "Iteration = 15001 \tAcc = 0.95 \tLoss = 0.0007578591191094033\n",
      "Iteration = 15251 \tAcc = 0.95 \tLoss = 2.9196281485108798e-05\n",
      "Iteration = 15501 \tAcc = 0.95 \tLoss = 2.303131440856666e-05\n",
      "Iteration = 15751 \tAcc = 0.95 \tLoss = 0.31210504383932264\n",
      "Iteration = 16001 \tAcc = 0.95 \tLoss = 0.343282544746022\n",
      "Iteration = 16251 \tAcc = 0.95 \tLoss = 1.830526127928575e-05\n",
      "Iteration = 16501 \tAcc = 0.95 \tLoss = 7.090994458283389e-13\n",
      "Iteration = 16751 \tAcc = 0.95 \tLoss = 3.402833570476684e-13\n",
      "Iteration = 17001 \tAcc = 0.95 \tLoss = 0.42054388222066535\n",
      "Iteration = 17251 \tAcc = 0.95 \tLoss = 1.142134439900836\n",
      "Iteration = 17501 \tAcc = 0.95 \tLoss = 0.38579031298233474\n",
      "Iteration = 17751 \tAcc = 0.95 \tLoss = 2.8647242784556405e-07\n",
      "Iteration = 18001 \tAcc = 0.95 \tLoss = 0.011609841284346684\n",
      "Iteration = 18251 \tAcc = 0.95 \tLoss = 0.0023867740367899803\n",
      "Iteration = 18501 \tAcc = 0.95 \tLoss = 6.482592240996408e-11\n",
      "Iteration = 18751 \tAcc = 0.95 \tLoss = 0.052090420177960506\n",
      "Iteration = 19001 \tAcc = 0.95 \tLoss = 0.49393882916109205\n",
      "Iteration = 19251 \tAcc = 0.95 \tLoss = 2.8593247810371482e-09\n",
      "Iteration = 19501 \tAcc = 0.95 \tLoss = 0.034874261869126406\n",
      "Iteration = 19751 \tAcc = 0.95 \tLoss = 1.1694012291757636\n",
      "Iteration = 20001 \tAcc = 0.95 \tLoss = 0.00025363364691794257\n",
      "Iteration = 20251 \tAcc = 0.95 \tLoss = 0.49473834535136507\n",
      "Iteration = 20501 \tAcc = 0.95 \tLoss = 0.0014851568021959185\n",
      "Iteration = 20751 \tAcc = 0.95 \tLoss = 0.006545018200759666\n",
      "Iteration = 21001 \tAcc = 0.95 \tLoss = 0.36077390984710306\n",
      "Iteration = 21251 \tAcc = 0.95 \tLoss = 0.1263953469216534\n",
      "Iteration = 21501 \tAcc = 0.95 \tLoss = 3.3585678688252743e-10\n",
      "Iteration = 21751 \tAcc = 0.95 \tLoss = 1.506223224165109e-08\n",
      "Iteration = 22001 \tAcc = 0.95 \tLoss = 0.0007140656836374358\n",
      "Iteration = 22251 \tAcc = 0.95 \tLoss = 0.06571278651931636\n",
      "Iteration = 22501 \tAcc = 0.95 \tLoss = 1.1197398564549342e-10\n",
      "Iteration = 22751 \tAcc = 0.95 \tLoss = 0.4240462533805517\n",
      "Iteration = 23001 \tAcc = 0.95 \tLoss = 6.233040880650847e-09\n",
      "Iteration = 23251 \tAcc = 0.95 \tLoss = 0.0003837568533415529\n",
      "Iteration = 23501 \tAcc = 0.95 \tLoss = 0.13261153820757152\n",
      "Iteration = 23751 \tAcc = 0.95 \tLoss = 0.004399672146737978\n",
      "Iteration = 24001 \tAcc = 0.95 \tLoss = 2.58044636090173e-07\n",
      "Iteration = 24251 \tAcc = 0.95 \tLoss = 0.10880159329413862\n",
      "Iteration = 24501 \tAcc = 0.95 \tLoss = 0.04620142431774842\n",
      "Iteration = 24751 \tAcc = 0.95 \tLoss = 1.6879313516716967e-09\n",
      "Iteration = 25001 \tAcc = 0.95 \tLoss = 0.07622834127349287\n",
      "Iteration = 25251 \tAcc = 0.95 \tLoss = 2.0403678746581944e-12\n",
      "Iteration = 25501 \tAcc = 0.95 \tLoss = 3.0102426322308497e-05\n",
      "Iteration = 25751 \tAcc = 0.95 \tLoss = 3.282468121046323e-05\n",
      "Iteration = 26001 \tAcc = 0.95 \tLoss = 4.434723948663108e-05\n",
      "Iteration = 26251 \tAcc = 0.95 \tLoss = 6.083248079335786e-08\n",
      "Iteration = 26501 \tAcc = 1.0 \tLoss = 2.9031452764976925e-05\n",
      "Iteration = 26751 \tAcc = 1.0 \tLoss = 0.005315786277159726\n",
      "Iteration = 27001 \tAcc = 0.95 \tLoss = 0.0023410747157133734\n",
      "Iteration = 27251 \tAcc = 0.95 \tLoss = 0.04041755100771831\n",
      "Iteration = 27501 \tAcc = 1.0 \tLoss = 1.8991382869018588e-08\n",
      "Iteration = 27751 \tAcc = 0.95 \tLoss = 8.149037000748981e-14\n",
      "Iteration = 28001 \tAcc = 0.95 \tLoss = 2.9221855806860165e-08\n",
      "Iteration = 28251 \tAcc = 0.95 \tLoss = 1.4910295220716963e-13\n",
      "Iteration = 28501 \tAcc = 1.0 \tLoss = 2.289590739250179e-11\n",
      "Iteration = 28751 \tAcc = 0.95 \tLoss = 8.460421997833079e-05\n",
      "Iteration = 29001 \tAcc = 0.95 \tLoss = 0.029284504789742344\n",
      "Iteration = 29251 \tAcc = 0.95 \tLoss = 0.037525705023947815\n",
      "Iteration = 29501 \tAcc = 0.95 \tLoss = 0.0024848086615457906\n",
      "Iteration = 29751 \tAcc = 0.95 \tLoss = 0.12909701964522913\n",
      "Iteration = 30001 \tAcc = 1.0 \tLoss = 0.11299569706587836\n",
      "Iteration = 30251 \tAcc = 0.95 \tLoss = 2.6659690172438253e-05\n",
      "Iteration = 30501 \tAcc = 0.95 \tLoss = 0.025545427099459676\n",
      "Iteration = 30751 \tAcc = 0.95 \tLoss = 6.285123432072333e-06\n",
      "Iteration = 31001 \tAcc = 0.95 \tLoss = 0.062480233172509114\n",
      "Iteration = 31251 \tAcc = 0.95 \tLoss = 1.4217762879558918e-05\n",
      "Iteration = 31501 \tAcc = 1.0 \tLoss = 3.767611563168864e-09\n",
      "Iteration = 31751 \tAcc = 0.95 \tLoss = 6.661338147750941e-16\n",
      "Iteration = 32001 \tAcc = 0.95 \tLoss = 1.776356839400252e-15\n",
      "Iteration = 32251 \tAcc = 0.95 \tLoss = 2.2204460492503154e-15\n",
      "Iteration = 32501 \tAcc = 0.95 \tLoss = 0.005003918193649968\n",
      "Iteration = 32751 \tAcc = 0.95 \tLoss = 1.1102230246251565e-16\n",
      "Iteration = 33001 \tAcc = 0.95 \tLoss = 4.523304074052138e-06\n",
      "Iteration = 33251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 33501 \tAcc = 1.0 \tLoss = 4.509671683105292e-06\n",
      "Iteration = 33751 \tAcc = 1.0 \tLoss = 1.9709628393255304e-09\n",
      "Iteration = 34001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 34251 \tAcc = 0.95 \tLoss = 6.709143049485588e-06\n",
      "Iteration = 34501 \tAcc = 0.95 \tLoss = 0.013210596555191028\n",
      "Iteration = 34751 \tAcc = 0.95 \tLoss = -0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 35001 \tAcc = 1.0 \tLoss = 0.5333440058470783\n",
      "Iteration = 35251 \tAcc = 0.95 \tLoss = 0.029655940467557446\n",
      "Iteration = 35501 \tAcc = 0.95 \tLoss = 3.458460319489584e-06\n",
      "Iteration = 35751 \tAcc = 0.95 \tLoss = 0.011302968337092306\n",
      "Iteration = 36001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 36251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 36501 \tAcc = 1.0 \tLoss = 0.5632988340400369\n",
      "Iteration = 36751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 37001 \tAcc = 1.0 \tLoss = 0.005320543643491508\n",
      "Iteration = 37251 \tAcc = 0.95 \tLoss = 0.012149591512626873\n",
      "Iteration = 37501 \tAcc = 0.95 \tLoss = 6.402327559047479e-10\n",
      "Iteration = 37751 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 38001 \tAcc = 0.95 \tLoss = 2.9385141587721965e-06\n",
      "Iteration = 38251 \tAcc = 1.0 \tLoss = 8.173947657072458e-06\n",
      "Iteration = 38501 \tAcc = 1.0 \tLoss = 6.046753371435964e-06\n",
      "Iteration = 38751 \tAcc = 1.0 \tLoss = 0.6813107436030115\n",
      "Iteration = 39001 \tAcc = 0.95 \tLoss = 0.0006819066319920023\n",
      "Iteration = 39251 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 39501 \tAcc = 0.95 \tLoss = 0.7376062787272041\n",
      "Iteration = 39751 \tAcc = 1.0 \tLoss = 0.0028863143569360416\n",
      "Iteration = 40001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 40251 \tAcc = 1.0 \tLoss = 1.8899326549979962e-10\n",
      "Iteration = 40501 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 40751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 41001 \tAcc = 0.95 \tLoss = 2.5535129566378632e-15\n",
      "Iteration = 41251 \tAcc = 1.0 \tLoss = 1.5132561871390782e-10\n",
      "Iteration = 41501 \tAcc = 1.0 \tLoss = 1.9984014443252837e-15\n",
      "Iteration = 41751 \tAcc = 1.0 \tLoss = 0.0060913560010113985\n",
      "Iteration = 42001 \tAcc = 1.0 \tLoss = 6.316325240817905e-11\n",
      "Iteration = 42251 \tAcc = 0.95 \tLoss = 0.9022448250439732\n",
      "Iteration = 42501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 42751 \tAcc = 1.0 \tLoss = 0.005853177222893843\n",
      "Iteration = 43001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 43251 \tAcc = 1.0 \tLoss = 0.0014666629833776036\n",
      "Iteration = 43501 \tAcc = 1.0 \tLoss = 0.00407803039428931\n",
      "Iteration = 43751 \tAcc = 1.0 \tLoss = 0.007493810824195484\n",
      "Iteration = 44001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 44251 \tAcc = 1.0 \tLoss = 3.821487570905024e-11\n",
      "Iteration = 44501 \tAcc = 0.95 \tLoss = 0.24542644470740466\n",
      "Iteration = 44751 \tAcc = 1.0 \tLoss = 3.704563944206518e-06\n",
      "Iteration = 45001 \tAcc = 1.0 \tLoss = 0.0008845294404191631\n",
      "Iteration = 45251 \tAcc = 1.0 \tLoss = 0.012332634246228123\n",
      "Iteration = 45501 \tAcc = 1.0 \tLoss = 3.4323146202855e-06\n",
      "Iteration = 45751 \tAcc = 1.0 \tLoss = 0.0008555099825313783\n",
      "Iteration = 46001 \tAcc = 1.0 \tLoss = 6.751166292901252e-11\n",
      "Iteration = 46251 \tAcc = 0.95 \tLoss = 0.00047341140153020765\n",
      "Iteration = 46501 \tAcc = 1.0 \tLoss = 0.45799097801743177\n",
      "Iteration = 46751 \tAcc = 1.0 \tLoss = 1.8358996324848637e-06\n",
      "Iteration = 47001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 47251 \tAcc = 1.0 \tLoss = 0.0006894605396359782\n",
      "Iteration = 47501 \tAcc = 0.95 \tLoss = 0.0017673439176273545\n",
      "Iteration = 47751 \tAcc = 0.95 \tLoss = 0.20769892994273886\n",
      "Iteration = 48001 \tAcc = 1.0 \tLoss = 0.0024446786683557853\n",
      "Iteration = 48251 \tAcc = 1.0 \tLoss = 4.856126612058591e-11\n",
      "Iteration = 48501 \tAcc = 0.95 \tLoss = 3.978373186520923e-11\n",
      "Iteration = 48751 \tAcc = 0.95 \tLoss = 0.7408579546306312\n",
      "Iteration = 49001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 49251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 49501 \tAcc = 1.0 \tLoss = 0.27257847436773913\n",
      "Iteration = 49751 \tAcc = 1.0 \tLoss = 0.4756781529518477\n",
      "Iteration = 50001 \tAcc = 1.0 \tLoss = 4.61484184210538e-11\n",
      "Iteration = 50251 \tAcc = 0.95 \tLoss = 0.002938737731861248\n",
      "Iteration = 50501 \tAcc = 1.0 \tLoss = 4.363032157878844e-11\n",
      "Iteration = 50751 \tAcc = 1.0 \tLoss = 4.409372866928731e-11\n",
      "Iteration = 51001 \tAcc = 0.95 \tLoss = 1.383933816836206e-06\n",
      "Iteration = 51251 \tAcc = 1.0 \tLoss = 0.0017079399742981267\n",
      "Iteration = 51501 \tAcc = 1.0 \tLoss = 2.246836554297153e-06\n",
      "Iteration = 51751 \tAcc = 0.95 \tLoss = 2.3068214005927574e-11\n",
      "Iteration = 52001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 52251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 52501 \tAcc = 1.0 \tLoss = 0.001713298715451426\n",
      "Iteration = 52751 \tAcc = 1.0 \tLoss = 2.193514484503096e-06\n",
      "Iteration = 53001 \tAcc = 1.0 \tLoss = 2.031741441775415e-11\n",
      "Iteration = 53251 \tAcc = 0.95 \tLoss = 1.1092238239091457e-11\n",
      "Iteration = 53501 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 53751 \tAcc = 1.0 \tLoss = 1.9855006527988486e-11\n",
      "Iteration = 54001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 54251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 54501 \tAcc = 1.0 \tLoss = 1.0809464434716334e-11\n",
      "Iteration = 54751 \tAcc = 1.0 \tLoss = 7.797319828813531e-07\n",
      "Iteration = 55001 \tAcc = 1.0 \tLoss = 0.0003781098973873381\n",
      "Iteration = 55251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 55501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 55751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 56001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 56251 \tAcc = 1.0 \tLoss = 0.004063123617386431\n",
      "Iteration = 56501 \tAcc = 1.0 \tLoss = 6.020973910170558e-07\n",
      "Iteration = 56751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 57001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 57251 \tAcc = 1.0 \tLoss = 1.2017091728009185e-06\n",
      "Iteration = 57501 \tAcc = 1.0 \tLoss = 8.523959316200894e-12\n",
      "Iteration = 57751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 58001 \tAcc = 1.0 \tLoss = 0.2386957699488602\n",
      "Iteration = 58251 \tAcc = 1.0 \tLoss = 0.005458501625884388\n",
      "Iteration = 58501 \tAcc = 1.0 \tLoss = 1.4051870778174408e-11\n",
      "Iteration = 58751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 59001 \tAcc = 1.0 \tLoss = 0.005683934503610345\n",
      "Iteration = 59251 \tAcc = 1.0 \tLoss = 0.005095032789598803\n",
      "Iteration = 59501 \tAcc = 1.0 \tLoss = 7.008393865273322e-12\n",
      "Iteration = 59751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 60001 \tAcc = 1.0 \tLoss = 0.0032790609427286368\n",
      "Iteration = 60251 \tAcc = 1.0 \tLoss = 3.070899233468717e-07\n",
      "Iteration = 60501 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 60751 \tAcc = 1.0 \tLoss = 0.0018477516695334997\n",
      "Iteration = 61001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 61251 \tAcc = 1.0 \tLoss = 0.004852527070253456\n",
      "Iteration = 61501 \tAcc = 1.0 \tLoss = 1.0127010341472107e-11\n",
      "Iteration = 61751 \tAcc = 1.0 \tLoss = 0.0080676943111047\n",
      "Iteration = 62001 \tAcc = 1.0 \tLoss = 1.0269674000137893e-11\n",
      "Iteration = 62251 \tAcc = 1.0 \tLoss = 0.0007845424520619525\n",
      "Iteration = 62501 \tAcc = 1.0 \tLoss = 1.4195533637562933e-11\n",
      "Iteration = 62751 \tAcc = 1.0 \tLoss = 9.379830245892088e-12\n",
      "Iteration = 63001 \tAcc = 1.0 \tLoss = 1.3094858530934534e-11\n",
      "Iteration = 63251 \tAcc = 1.0 \tLoss = 4.488187599359729e-12\n",
      "Iteration = 63501 \tAcc = 0.95 \tLoss = 0.0010904503360404472\n",
      "Iteration = 63751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 64001 \tAcc = 1.0 \tLoss = 0.12571264638616744\n",
      "Iteration = 64251 \tAcc = 0.95 \tLoss = 0.004487645868668841\n",
      "Iteration = 64501 \tAcc = 1.0 \tLoss = 0.003970992980229049\n",
      "Iteration = 64751 \tAcc = 1.0 \tLoss = 0.00024335032471062943\n",
      "Iteration = 65001 \tAcc = 0.95 \tLoss = -0.0\n",
      "Iteration = 65251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 65501 \tAcc = 1.0 \tLoss = 5.671006722409337e-07\n",
      "Iteration = 65751 \tAcc = 1.0 \tLoss = 0.004744219112301831\n",
      "Iteration = 66001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 66251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 66501 \tAcc = 1.0 \tLoss = 0.017859230011280553\n",
      "Iteration = 66751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 67001 \tAcc = 1.0 \tLoss = 1.301048772310503e-07\n",
      "Iteration = 67251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 67501 \tAcc = 1.0 \tLoss = 1.0328959909653487e-11\n",
      "Iteration = 67751 \tAcc = 1.0 \tLoss = 0.008832237974912433\n",
      "Iteration = 68001 \tAcc = 1.0 \tLoss = 0.00033623806526312443\n",
      "Iteration = 68251 \tAcc = 1.0 \tLoss = 0.0066386685152256195\n",
      "Iteration = 68501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 68751 \tAcc = 1.0 \tLoss = 0.0012517025011552305\n",
      "Iteration = 69001 \tAcc = 1.0 \tLoss = 0.00036676070470051034\n",
      "Iteration = 69251 \tAcc = 1.0 \tLoss = 4.932609876119273e-12\n",
      "Iteration = 69501 \tAcc = 1.0 \tLoss = 0.0033998670731023766\n",
      "Iteration = 69751 \tAcc = 1.0 \tLoss = 0.22299211617633427\n",
      "Iteration = 70001 \tAcc = 1.0 \tLoss = 0.012880004178853358\n",
      "Iteration = 70251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 70501 \tAcc = 1.0 \tLoss = 0.2139818574077019\n",
      "Iteration = 70751 \tAcc = 1.0 \tLoss = 0.0003625895039519641\n",
      "Iteration = 71001 \tAcc = 1.0 \tLoss = 4.295119815376567e-12\n",
      "Iteration = 71251 \tAcc = 1.0 \tLoss = 0.003596377818217753\n",
      "Iteration = 71501 \tAcc = 1.0 \tLoss = 2.24220642053548e-12\n",
      "Iteration = 71751 \tAcc = 1.0 \tLoss = 7.652339199774161e-08\n",
      "Iteration = 72001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 72251 \tAcc = 1.0 \tLoss = 2.2071631959840897e-07\n",
      "Iteration = 72501 \tAcc = 1.0 \tLoss = 9.922418423943673e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 72751 \tAcc = 1.0 \tLoss = 0.006152884990281412\n",
      "Iteration = 73001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 73251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 73501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 73751 \tAcc = 1.0 \tLoss = 1.7740253710501112e-12\n",
      "Iteration = 74001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 74251 \tAcc = 1.0 \tLoss = 0.004393228856904789\n",
      "Iteration = 74501 \tAcc = 1.0 \tLoss = 0.007052093081549037\n",
      "Iteration = 74751 \tAcc = 1.0 \tLoss = 1.5978352879675743e-08\n",
      "Iteration = 75001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 75251 \tAcc = 1.0 \tLoss = 0.0028660474778253995\n",
      "Iteration = 75501 \tAcc = 1.0 \tLoss = 0.18750852637083068\n",
      "Iteration = 75751 \tAcc = 1.0 \tLoss = 0.009036826347249342\n",
      "Iteration = 76001 \tAcc = 1.0 \tLoss = 0.004233354937851132\n",
      "Iteration = 76251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 76501 \tAcc = 1.0 \tLoss = 2.666533660548256e-12\n",
      "Iteration = 76751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 77001 \tAcc = 1.0 \tLoss = 0.001224113239326284\n",
      "Iteration = 77251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 77501 \tAcc = 1.0 \tLoss = 2.2809532040949856e-12\n",
      "Iteration = 77751 \tAcc = 1.0 \tLoss = 1.2563283746666164e-12\n",
      "Iteration = 78001 \tAcc = 1.0 \tLoss = 3.5734490842189465e-05\n",
      "Iteration = 78251 \tAcc = 1.0 \tLoss = 3.4728886433359828e-12\n",
      "Iteration = 78501 \tAcc = 1.0 \tLoss = 2.247962941080104e-08\n",
      "Iteration = 78751 \tAcc = 1.0 \tLoss = 2.4112211851433332e-08\n",
      "Iteration = 79001 \tAcc = 1.0 \tLoss = 1.3220525539553382e-08\n",
      "Iteration = 79251 \tAcc = 1.0 \tLoss = 6.689114275271393e-08\n",
      "Iteration = 79501 \tAcc = 1.0 \tLoss = 0.11946493534091064\n",
      "Iteration = 79751 \tAcc = 1.0 \tLoss = 0.009596935491120008\n",
      "Iteration = 80001 \tAcc = 1.0 \tLoss = 1.0458300891974443e-12\n",
      "Iteration = 80251 \tAcc = 1.0 \tLoss = 9.673373213563668e-13\n",
      "Iteration = 80501 \tAcc = 1.0 \tLoss = 0.2484549153439332\n",
      "Iteration = 80751 \tAcc = 1.0 \tLoss = 0.00408699894796021\n",
      "Iteration = 81001 \tAcc = 1.0 \tLoss = 3.1143976286833387e-12\n",
      "Iteration = 81251 \tAcc = 1.0 \tLoss = 0.0022784132129591147\n",
      "Iteration = 81501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 81751 \tAcc = 1.0 \tLoss = 1.6768808563952424e-12\n",
      "Iteration = 82001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 82251 \tAcc = 1.0 \tLoss = 0.00316793424718667\n",
      "Iteration = 82501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 82751 \tAcc = 1.0 \tLoss = 1.5305534617494121e-12\n",
      "Iteration = 83001 \tAcc = 1.0 \tLoss = 4.161401676360257e-09\n",
      "Iteration = 83251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 83501 \tAcc = 1.0 \tLoss = 0.0003453735967953569\n",
      "Iteration = 83751 \tAcc = 1.0 \tLoss = 0.0033020427980783957\n",
      "Iteration = 84001 \tAcc = 1.0 \tLoss = 0.19082498956136495\n",
      "Iteration = 84251 \tAcc = 1.0 \tLoss = 2.592814851712952e-12\n",
      "Iteration = 84501 \tAcc = 1.0 \tLoss = 0.0035910868582631726\n",
      "Iteration = 84751 \tAcc = 1.0 \tLoss = 0.004053566620550111\n",
      "Iteration = 85001 \tAcc = 1.0 \tLoss = 0.00010462707565122876\n",
      "Iteration = 85251 \tAcc = 1.0 \tLoss = 1.292743689874368e-12\n",
      "Iteration = 85501 \tAcc = 1.0 \tLoss = 0.00011149240208026459\n",
      "Iteration = 85751 \tAcc = 1.0 \tLoss = 1.1897149931890254e-12\n",
      "Iteration = 86001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 86251 \tAcc = 1.0 \tLoss = 0.10876716567406265\n",
      "Iteration = 86501 \tAcc = 1.0 \tLoss = 8.733683540935705e-05\n",
      "Iteration = 86751 \tAcc = 1.0 \tLoss = 0.002346533210437837\n",
      "Iteration = 87001 \tAcc = 1.0 \tLoss = 5.0396246192165424e-09\n",
      "Iteration = 87251 \tAcc = 1.0 \tLoss = 3.39682138550005e-09\n",
      "Iteration = 87501 \tAcc = 1.0 \tLoss = 0.003756475537787065\n",
      "Iteration = 87751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 88001 \tAcc = 1.0 \tLoss = 0.00012135688764428698\n",
      "Iteration = 88251 \tAcc = 1.0 \tLoss = 0.1149858594841137\n",
      "Iteration = 88501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 88751 \tAcc = 1.0 \tLoss = 0.0008410466094502291\n",
      "Iteration = 89001 \tAcc = 1.0 \tLoss = 0.0025023417940697886\n",
      "Iteration = 89251 \tAcc = 1.0 \tLoss = 4.60337995795406e-05\n",
      "Iteration = 89501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 89751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 90001 \tAcc = 1.0 \tLoss = 1.0973043590903541e-09\n",
      "Iteration = 90251 \tAcc = 1.0 \tLoss = 6.437175377977022e-05\n",
      "Iteration = 90501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 90751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 91001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 91251 \tAcc = 1.0 \tLoss = 1.382055057732832e-08\n",
      "Iteration = 91501 \tAcc = 1.0 \tLoss = 0.19294922668589362\n",
      "Iteration = 91751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 92001 \tAcc = 1.0 \tLoss = 5.2455480214297396e-05\n",
      "Iteration = 92251 \tAcc = 1.0 \tLoss = 0.003461552948060903\n",
      "Iteration = 92501 \tAcc = 1.0 \tLoss = 1.420735641254386e-09\n",
      "Iteration = 92751 \tAcc = 1.0 \tLoss = 1.3952172750474076e-12\n",
      "Iteration = 93001 \tAcc = 1.0 \tLoss = 1.2152309343825317e-08\n",
      "Iteration = 93251 \tAcc = 1.0 \tLoss = 3.547663494005171e-05\n",
      "Iteration = 93501 \tAcc = 1.0 \tLoss = 0.0016080823661821118\n",
      "Iteration = 93751 \tAcc = 1.0 \tLoss = 0.004199806946527582\n",
      "Iteration = 94001 \tAcc = 1.0 \tLoss = 0.09381565208761285\n",
      "Iteration = 94251 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 94501 \tAcc = 1.0 \tLoss = 6.972752544779781e-05\n",
      "Iteration = 94751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 95001 \tAcc = 1.0 \tLoss = 0.03561318636754566\n",
      "Iteration = 95251 \tAcc = 1.0 \tLoss = 0.004978768234733686\n",
      "Iteration = 95501 \tAcc = 1.0 \tLoss = 0.052528864487938934\n",
      "Iteration = 95751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 96001 \tAcc = 1.0 \tLoss = 0.0008204320179202716\n",
      "Iteration = 96251 \tAcc = 1.0 \tLoss = 4.2356283388022595e-05\n",
      "Iteration = 96501 \tAcc = 1.0 \tLoss = 8.469879500326083e-09\n",
      "Iteration = 96751 \tAcc = 1.0 \tLoss = 9.85322934355312e-13\n",
      "Iteration = 97001 \tAcc = 1.0 \tLoss = 9.268141809575102e-13\n",
      "Iteration = 97251 \tAcc = 1.0 \tLoss = 0.0307634471039966\n",
      "Iteration = 97501 \tAcc = 1.0 \tLoss = 0.003401293040948401\n",
      "Iteration = 97751 \tAcc = 1.0 \tLoss = 0.06429487851215138\n",
      "Iteration = 98001 \tAcc = 1.0 \tLoss = 4.60520510614621e-13\n",
      "Iteration = 98251 \tAcc = 1.0 \tLoss = 8.752998326148565e-13\n",
      "Iteration = 98501 \tAcc = 1.0 \tLoss = 8.37219182870181e-13\n",
      "Iteration = 98751 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 99001 \tAcc = 1.0 \tLoss = 0.07358559357845412\n",
      "Iteration = 99251 \tAcc = 1.0 \tLoss = 4.363176486777817e-13\n",
      "Iteration = 99501 \tAcc = 1.0 \tLoss = 2.2459811788169439e-13\n",
      "Iteration = 99751 \tAcc = 1.0 \tLoss = 0.0008208873542518031\n",
      "-3.46493986 -3.41956036 3.39710997 2.0597278\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Sequential at 0x7f5539702400>"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = hard()\n",
    "nn = Sequential([Linear(2, 10), ReLU(), \n",
    "                 Linear(10, 10), ReLU(), \n",
    "                 Linear(10, 2), SoftMax()], NLL())\n",
    "disp.classify(X, Y, nn, it=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 4: try calling these methods that train with a simple dataset\n",
    "def nn_tanh_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), Tanh(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
    "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
    "\n",
    "\n",
    "def nn_relu_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=2, lrate=0.005)\n",
    "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
    "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
    "\n",
    "\n",
    "def nn_pred_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    Ypred = nn.forward(X)\n",
    "    return nn.modules[-1].class_fun(Ypred).tolist(), [nn.loss.forward(Ypred, Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\\n  [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\\n  [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\\n [[0.544808855557535, -0.08366117689965663],\\n  [-0.06331837550937104, 0.24078409926389266],\\n  [0.08677202043839037, 0.8360167748667923],\\n  [-0.0037249480614718, 0.0037249480614718]]]\\n'"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_tanh_test()\n",
    "\n",
    "# Expected output:\n",
    "'''\n",
    "[[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
    "  [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
    "  [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n",
    " [[0.544808855557535, -0.08366117689965663],\n",
    "  [-0.06331837550937104, 0.24078409926389266],\n",
    "  [0.08677202043839037, 0.8360167748667923],\n",
    "  [-0.0037249480614718, 0.0037249480614718]]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.5 \tLoss = 0.1491149875248131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\\n  [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\\n  [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\\n [[0.501769700845158, -0.040622022187279644],\\n  [-0.09260786974986723, 0.27007359350438886],\\n  [0.08364438851530624, 0.8391444067898763],\\n  [-0.004252310922204504, 0.004252310922204505]]]\\n'"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_relu_test()\n",
    "\n",
    "# Expected output:\n",
    "'''\n",
    "[[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
    "  [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
    "  [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n",
    " [[0.501769700845158, -0.040622022187279644],\n",
    "  [-0.09260786974986723, 0.27007359350438886],\n",
    "  [0.08364438851530624, 0.8391444067898763],\n",
    "  [-0.004252310922204504, 0.004252310922204505]]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n([0, 0, 0, 0], [8.56575061835767])\\n'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pred_test()\n",
    "\n",
    "# Expected output:\n",
    "'''\n",
    "([0, 0, 0, 0], [8.56575061835767])\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
