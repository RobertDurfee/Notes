# 14.32 Reading 1

## 1) Economic Questions and Data

### 1.3) Data: Sources and Types

- **Cross-Sectional**: Multiple individuals, single time period.
- **Time Series**: Single individual, multiple time periods.
- **Panel**: Multiple individuals, multiple time periods.

## 2) Review of Probability

### 2.2) Expected Values, Mean, and Variance

#### Bernoulli Distribution

- **Mean**: $\mu = p$
- **Variance**" $\sigma^2 = p (1  - p)$

#### Linear Relations with Random Variables

If $X$ and $Y$ are related linearly as

$Y = a + b X$

Then the following are true:
- **Mean**: $\mu_y = a + b \mu_x$
- **Variance**: $\sigma_Y^2 = b^2 \sigma_x^2$

#### Other Measures of the Shape of a Distribution

- **Skewness**: This measures the difference of symmetry between the tails of
a distribution. 

$$E[(Y - \mu)^3]/\sigma^3$$

- **Kurosis**: This measures the thickness of the tails of a distribution.
Or, the likelihood of outliers.

$$E[(Y - \mu)^4] / \sigma^4$$

- **Moments**: The $r$th moment is given by 

$$E[X^r]$$.

### 2.3) Two Random Variables

#### Joint Distributions

- **Marginal Distribution**: Another name for probability distribution. From
joint probability, this is given by 

$$\mathrm{Pr}(Y = y) = \sum_{i=1}^{\ell} \mathrm{Pr}(X = x_i, Y = y)$$

#### Conditional Distributions

- **Conditional Distribution**: From joint probability, given by

$$ \mathrm{Pr}(Y = y \mid X = x) = \frac{\mathrm{Pr}(X = x, Y = y)}{\mathrm{Pr}(X = x)} $$

- **Conditional Expectation**: From conditional distribution,

$$ E[Y \mid X = x] = \sum_{i = 1}^{k} y_i \mathrm{Pr}(Y = y_i \mid X = x) $$

- **Law of Iterated Expectations**:

$$ E[Y] = \sum_{i = 1}^{\ell} E[Y \mid X = x_i] \mathrm{Pr}(X = x_i) $$
$$ E[Y] = E[E[Y \mid X]] $$

- **Conditional Variance**:

$$ \mathrm{var}(Y \mid X = x) = \sum_{i = 1}^{k} (y_i - E[Y \mid X = x])^2 \mathrm{Pr}(Y = y_i \mid X = x) $$

#### Independence

$$ \mathrm{Pr}(Y = y \mid X = x) = \mathrm{Pr}(Y = y) $$
$$ \mathrm{Pr}(X = x, Y = y) = \mathrm{Pr}(X = x) \mathrm{Pr}(Y = y) $$

#### Covariance and Correlation

- **Covariance**:

$$ \mathrm{cov}(X, Y) = \sigma_{XY} = E[(X - \mu_X)(Y - \mu_Y)] $$

- **Correlation**:

$$ \mathrm{corr}(X, Y) = \frac{\mathrm{cov}(X, Y)}{\sqrt{\mathrm{var}(X) \mathrm{var}(Y)}} = \frac{\sigma_{XY}}{\sigma_X \sigma_Y} $$

- **Conditional Mean**: If $E[Y \mid X] = \mu_Y$, then $\mathrm{cov}(Y, X) = 0$ 
and $\mathrm{corr}(Y, X) = 0$.

#### The Mean and Variance of Sums of Random Variables

- **Mean of Sum**: 

$$ E[X + Y] = E[X] + E[Y] = \mu_X + \mu_Y $$

- **Variance of Sum**: If the two variables are not independent,

$$ \mathrm{var}(X + Y) = \mathrm{var}(X) + \mathrm{var}(Y) + 2\mathrm{cov}(X, Y) $$

If the two variables *are* independent,

$$ \mathrm{var}(X + Y) = \mathrm{var}(X) + \mathrm{var}(Y) $$
